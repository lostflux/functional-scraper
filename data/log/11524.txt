Toggle Navigation News Events TNW Conference 2024 June 20 & 21, 2024 TNW Vision: 2024 All events Spaces Programs Newsletters Partner with us Jobs Contact News news news news Latest Deep tech Sustainability Ecosystems Data and security Fintech and ecommerce Future of work More Startups and technology Investors and funding Government and policy Corporates and innovation Gadgets & apps Early bird Business passes are 90% SOLD OUT üéüÔ∏è Buy now before they are gone ‚Üí This article was published on November 5, 2019 Artificial Intelligence Remember that scary AI text-generator that was too dangerous to release? It‚Äôs out now OpenAI today published the final model in its staged release for GPT-2, the spooky text generator the AI community‚Äôs been talking about all year.
GPT-2 uses machine learning to generate novel text based on a limited input. Basically, you can type a few sentences about anything you like and the AI will spit out some ‚Äòrelated‚Äô text. Unlike most ‚Äòtext generators‚Äô it doesn‚Äôt output pre-written strings. GPT-2 makes up text that didn‚Äôt previously exist‚Äì at least according to OpenAI‚Äôs research paper.
If death, in some obscure and distant hour, Strikes me still as I slept, if I yet dream: Is that my peace with an eternity spent? [‚Ä¶] But I fear it will be no peace or rest Until the stars give me the full glow of their light To see all my cares and woes in an instant.
Shit.
pic.twitter.com/QRoi1C3rjj ‚Äî Scott B. Weingart (@scott_bot) August 20, 2019 The non-profit made headlines in February when it disclosed that it would not release the full-sized models for GPT-2 to the general public all at once. Instead, the company opted to release it in four parts over eight months.
Get your ticket NOW for TNW Conference - Super Earlybird is 90% sold out! Unleash innovation, connect with thousands of tech lovers and shape the future on June 20-21, 2024.
An OpenAI blog post from February explains: Due to our concerns about malicious applications of the technology, we are not releasing the trained model. As an experiment in responsible disclosure, we are instead releasing a much smaller model for researchers to experiment with, as well as a technical paper.
The full model contains 1.5 billion parameters. The more parameters a model is trained with, the ‚Äòsmarter‚Äô it appears to be ‚Äì just like humans, practice makes perfect.
Initially OpenAI released a model with 124 million parameters subsequently followed by releases with 355 and 774 million. Each iteration showed a significant improvement in capability over previous iterations. We checked out the 774M model and were blown away.
 You can try it yourself at this link where developer Adam King has translated the model into a UI.
Along with the new model 1.5B model weights, OpenAI also released its GPT-2 detection models in an effort to preemptively combat misuse. Unfortunately, according to OpenAI, the detector isn‚Äôt as good as the generator. In a blog post today the company said: We conducted in-house detection research and developed a detection model that has detection rates of ~95% for detecting 1.5B GPT-2-generated, Specifically, we based a sequence classifier on RoBERTaBASE (125 million parameters) and RoBERTaLARGE (355 million parameters) and fine-tuned it to classify the outputs from the 1.5B GPT-2 model versus WebText, the dataset we used to train the GPT-2 model.
We believe this is not high enough accuracy for standalone detection and needs to be paired with metadata-based approaches, human judgment, and public education to be more effective. We are releasing this model to aid the study of research into the detection of synthetic text, although this does let adversaries with access better evade detection.
We‚Äôll get into the adversarial (and positive) use cases for GPT-2‚Äôs full release once we‚Äôve had the chance to experiment with the complete model. In the meantime, you can download the model here on Github, check out the model card here , and read OpenAI‚Äôs blog post here.
Read next: A beginner‚Äôs guide to the AI apocalypse: Misaligned objectives Story by Tristan Greene Editor, Neural by TNW Tristan is a futurist covering human-centric artificial intelligence advances, quantum computing, STEM, physics, and space stuff. Pronouns: (show all) Tristan is a futurist covering human-centric artificial intelligence advances, quantum computing, STEM, physics, and space stuff. Pronouns: He/him Get the TNW newsletter Get the most important tech news in your inbox each week.
Also tagged with Artificial intelligence OpenAI Story by Tristan Greene Popular articles 1 UK won‚Äôt regulate AI anytime soon, minister says 2 World-first CRISPR gene-editing therapy approved in UK 3 AI is transforming the English dictionary 4 This new EV supercharger on wheels recharges in just 6 minutes 5 VP of UK‚Äôs top generative AI firm resigns over ‚Äòfair use‚Äô controversy Related Articles deep tech UN creates AI advisory body to ‚Äòmaximise‚Äô benefits for humankind deep tech DeepMind says its new AI system is the world‚Äôs most accurate 10-day weather forecaster Join TNW All Access Watch videos of our inspiring talks for free ‚Üí deep tech How this Berlin startup deploys AI to make preventative healthcare accessible deep tech Social media has new moderation problems. This AI startup has a solution The heart of tech More TNW Media Events Programs Spaces Newsletters Jobs in tech About TNW Partner with us Jobs Terms & Conditions Cookie Statement Privacy Statement Editorial Policy Masthead Copyright ¬© 2006‚Äî2023, The Next Web B.V. Made with <3 in Amsterdam.
