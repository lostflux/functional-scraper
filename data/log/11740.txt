Artificial Intelligence View All AI, ML and Deep Learning Auto ML Data Labelling Synthetic Data Conversational AI NLP Text-to-Speech Security View All Data Security and Privacy Network Security and Privacy Software Security Computer Hardware Security Cloud and Data Storage Security Data Infrastructure View All Data Science Data Management Data Storage and Cloud Big Data and Analytics Data Networks Automation View All Industrial Automation Business Process Automation Development Automation Robotic Process Automation Test Automation Enterprise Analytics View All Business Intelligence Disaster Recovery Business Continuity Statistical Analysis Predictive Analysis More Data Decision Makers Virtual Communication Team Collaboration UCaaS Virtual Reality Collaboration Virtual Employee Experience Programming & Development Product Development Application Development Test Management Development Languages Researchers apply developmental psychology to AI model that predicts object relationships Share on Facebook Share on X Share on LinkedIn 3D Rendering, Robots speaking no evil, hearing no evil, seeing no evil Are you ready to bring more awareness to your brand? Consider becoming a sponsor for The AI Impact Tour. Learn more about the opportunities here.
Humans have no trouble recognizing objects and reasoning about their behaviors — it’s at the core of their cognitive development. Even as children, they group segments into objects based on motion and use concepts of object permanence, solidity, and continuity to explain what has happened and imagine what would happen in other scenarios. Inspired by this, a team of researchers hailing from the MIT-IBM Watson AI Lab, MIT’s Computer Science and Artificial Intelligence Laboratory, Alphabet’s DeepMind, and Harvard University sought to simplify the problem of visual recognition by introducing a benchmark — CoLlision Events for Video REpresentation and Reasoning ( CLEVRER ) — that draws on inspirations from developmental psychology.
CLEVRER contains over 20,000 5-second videos of colliding objects (three shapes of two materials and eight colors) generated by a physics engine and more than 300,000 questions and answers, all focusing on four elements of logical reasoning: descriptive (e.g., “what color”), explanatory (“what’s responsible for”), predictive (“what will happen next”), and counterfactual (“what if”). It comes with ground-truth motion traces and event histories for each object in the videos, and with functional programs representing underlying logic that pair with each question.
The researchers analyzed CLEVRER to identify the elements necessary to excel not only at the descriptive questions, which state-of-the-art visual reasoning AI models can do, but at the explanatory, predictive, and counterfactual questions as well. They found three elements — recognition of the objects and events in the videos, modeling the dynamics and causal relations between the objects and events, and understanding of the symbolic logic behind the questions — to be the most important, and they developed a model — Neuro-Symbolic Dynamic Reasoning (NS-DR) — that explicitly joined them together via a representation.
VB Event The AI Impact Tour Connect with the enterprise AI community at VentureBeat’s AI Impact Tour coming to a city near you! NS-DR is actually four models in one: a video frame parser, a neural dynamics predictor, a question parser, and a program executor. Given an input video, the video frame parser detects objects in the scene and extracts both their traces and attributes (i.e. position, color, shape, material). These form an abstract representation of the video, which is sent to the neural dynamics predictor to anticipate the motions and collisions of the objects. The question parser receives the input question to obtain a functional program representing its logic. Then the symbolic program executor runs the program on the dynamic scene and outputs an answer.
The team reports that their model achieved 88.1% accuracy when the question parser was trained under 1,000 programs, outperforming other baseline models. On explanatory, predictive, and counterfactual questions, it managed a “more significant” gain.
“NS-DR [incorporates a] dynamics planner into the visual reasoning task, which directly enables predictions of unobserved motion and events, and enables the model for the predictive and counterfactual tasks,” noted the researchers. “This suggests that dynamics planning has great potential for language-grounded visual reasoning tasks, and NS-DR takes a preliminary step toward this direction. Second, symbolic representation provides a powerful common ground for vision, language, dynamics, and causality. By design, it empowers the model to explicitly capture the compositionality behind the video’s causal structure and the question logic.” The researchers concede that even though the amount of data required for training is relatively minimal, it’s hard to come by in real-world applications. Additionally, NS-DR’s performance decreased on tasks that required long-term dynamics prediction, such as the counterfactual questions, which they say suggests the need for a better dynamics model capable of generating more stable and accurate trajectories.
VentureBeat's mission is to be a digital town square for technical decision-makers to gain knowledge about transformative enterprise technology and transact.
Discover our Briefings.
The AI Impact Tour Join us for an evening full of networking and insights at VentureBeat's AI Impact Tour, coming to San Francisco, New York, and Los Angeles! VentureBeat Homepage Follow us on Facebook Follow us on X Follow us on LinkedIn Follow us on RSS Press Releases Contact Us Advertise Share a News Tip Contribute to DataDecisionMakers Careers Privacy Policy Terms of Service Do Not Sell My Personal Information © 2023 VentureBeat.
 All rights reserved.
