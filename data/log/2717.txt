Artificial Intelligence View All AI, ML and Deep Learning Auto ML Data Labelling Synthetic Data Conversational AI NLP Text-to-Speech Security View All Data Security and Privacy Network Security and Privacy Software Security Computer Hardware Security Cloud and Data Storage Security Data Infrastructure View All Data Science Data Management Data Storage and Cloud Big Data and Analytics Data Networks Automation View All Industrial Automation Business Process Automation Development Automation Robotic Process Automation Test Automation Enterprise Analytics View All Business Intelligence Disaster Recovery Business Continuity Statistical Analysis Predictive Analysis More Data Decision Makers Virtual Communication Team Collaboration UCaaS Virtual Reality Collaboration Virtual Employee Experience Programming & Development Product Development Application Development Test Management Development Languages Removing bias in AI: Wells Fargo shares open toolkit for explainability Share on Facebook Share on X Share on LinkedIn #VBTransform of @AnnaGriffinNow @jeggers @manuaero @may_habib @mmarshall @nickfrosst @parasnis @PhilipDawson @sharongoldman @stevewoodwho @uljansharka @Venturebeat Are you ready to bring more awareness to your brand? Consider becoming a sponsor for The AI Impact Tour. Learn more about the opportunities here.
Explainability is not a technology issue — it is a human issue.
Therefore, it is incumbent on humans to be able to explain and understand how AI models come to the inferences that they do, said Madhu Narasimhan, EVP and head of innovation at Wells Fargo.
“That’s a key part of why explainable AI becomes so important,” she emphasized to the audience during a fireside chat at today’s VentureBeat Transform 2023 event.
Narasimhan explained to the crowd and moderator Jana Eggers, cofounder and CEO of synaptic intelligence platform Nara Logics , that Wells Fargo did a “tremendous amount” of post hoc testing on its Fargo virtual assistant to understand why the model was interpreting language the way that it was.
VB Event The AI Impact Tour Connect with the enterprise AI community at VentureBeat’s AI Impact Tour coming to a city near you! In building out models, the company concurrently builds out explainability and has an independent group of data scientists who separately validate them.
>> Follow all our VentureBeat Transform 2023 coverage << “We use that as part of our testing to make sure that when a customer starts using the virtual assistant, it’s behaving exactly the way they expect it to,” said Narasimhan. “Because virtual assistants are so common, no other experience will be acceptable.” Behaving the way a human would Essentially, said Narasimhan, the goal is to have models that behave the way a human would, “because that’s the whole premise of AI.
” One of the key challenges is that humans are biased, and it is critical to ensure that models are not biased. “We have to protect and manage the bias in the data,” she said.
As part of its model development process, Wells Fargo looks at all data elements for bias, both at the attribute level and the dataset level, she explained.
Eggers, for her part, noted that while removing bias is important, outright cleaning of data is not.
“I always tell people, ‘Don’t clean your data,’ because lots of data is dirty and messy,” she said. “And that’s just life, and we have to have models that adjust to that.” If a machine can tell people what it’s seeing in data, they can then go in and tell it to stop seeing a certain bias, she pointed out.
“It’s not that I want to take data out,” said Eggers. “It’s that I want to tune and adjust, just like with a human where we want to bring awareness: ‘Hey, you have some bias.’” Working together toward explainability Ultimately, it is important to understand what generative AI can do, as well as its limits, said Narasimhan. The leading economic force will be building more and more complex models, so explainability will continue to be required to support unexpected inferences.
To help support this across the board, Wells Fargo’s data scientists have created a Python interpretable machine learning (PiML) open access toolkit that the company has shared with other financial institutions.
“That is what I’m excited about: Being able to develop a tool that’s available in an open access manner that allows everyone to look at how you can inherently explain models,” said Narasimhan. “The more you can explain how we get to the explainability of a model, I think it’s better for us all around.” VentureBeat's mission is to be a digital town square for technical decision-makers to gain knowledge about transformative enterprise technology and transact.
Discover our Briefings.
The AI Impact Tour Join us for an evening full of networking and insights at VentureBeat's AI Impact Tour, coming to San Francisco, New York, and Los Angeles! VentureBeat Homepage Follow us on Facebook Follow us on X Follow us on LinkedIn Follow us on RSS Press Releases Contact Us Advertise Share a News Tip Contribute to DataDecisionMakers Careers Privacy Policy Terms of Service Do Not Sell My Personal Information © 2023 VentureBeat.
 All rights reserved.
