Open Navigation Menu To revist this article, visit My Profile, then View saved stories.
Close Alert Backchannel Business Culture Gear Ideas Science Security Merch To revist this article, visit My Profile, then View saved stories.
Close Alert Search Backchannel Business Culture Gear Ideas Science Security Merch Podcasts Video Artificial Intelligence Climate Games Newsletters Magazine Events Wired Insider Jobs Coupons Khari Johnson Business Chatbots Got Big‚Äîand Their Ethical Red Flags Got Bigger ILLUSTRATION: JAMES MARSHALL Save this story Save Save this story Save In the weeks following the release of OpenAI‚Äôs viral chatbot ChatGPT late last year, Google AI chief Jeff Dean expressed concern that deploying a conversational search engine too quickly might pose a reputational risk for Alphabet. But last week Google announced its own chatbot, Bard , which in its first demo made a factual error about the James Webb Space Telescope.
Also last week, Microsoft integrated ChatGPT-based technology into Bing search results.
 Sarah Bird, Microsoft‚Äôs head of responsible AI, acknowledged that the bot could still ‚Äúhallucinate‚Äù untrue information but said the technology had been made more reliable. In the days that followed, Bing claimed that running was invented in the 1700s and tried to convince one user that the year is 2022.
Alex Hanna sees a familiar pattern in these events‚Äîfinancial incentives to rapidly commercialize AI outweighing concerns about safety or ethics. There isn‚Äôt much money in responsibility or safety, but there‚Äôs plenty in overhyping the technology, says Hanna, who previously worked on Google‚Äôs Ethical AI team and is now head of research at nonprofit Distributed AI Research.
The race to make large language models‚ÄîAI systems trained on massive amounts of data from the web to work with text‚Äîand the movement to make ethics a core part of the AI design process began around the same time. In 2018, Google launched the language model BERT, and before long Meta, Microsoft, and Nvidia had released similar projects based on the AI that is now part of Google search results.
 Also in 2018, Google adopted AI ethics principles said to limit future projects. Since then, researchers have warned that large language models carry heightened ethical risks and can spew or even intensify toxic, hateful speech.
 These models are also predisposed to making things up.
As startups and tech giants have attempted to build competitors to ChatGPT, some in the industry wonder whether the bot has shifted perceptions for when it‚Äôs acceptable or ethical to deploy AI powerful enough to generate realistic text and images.
OpenAI‚Äôs process for releasing models has changed in the past few years. Executives said the text generator GPT-2 was released in stages over months in 2019 due to fear of misuse and its impact on society (that strategy was criticized by some as a publicity stunt ). In 2020, the training process for its more powerful successor, GPT-3, was well documented in public, but less than two months later OpenAI began commercializing the technology through an API for developers. By November 2022, the ChatGPT release process included no technical paper or research publication, only a blog post, a demo, and soon a subscription plan.
Business What Sam Altman‚Äôs Firing Means for the Future of OpenAI Steven Levy Business Sam Altman‚Äôs Sudden Exit Sends Shockwaves Through OpenAI and Beyond Will Knight Gear Humanity‚Äôs Most Obnoxious Vehicle Gets an Electric (and Nearly Silent) Makeover Boone Ashworth Security The Startup That Transformed the Hack-for-Hire Industry Andy Greenberg Irene Solaiman, policy director at open source AI startup Hugging Face, believes outside pressure can help hold AI systems like ChatGPT to account. She is working with people in academia and industry to create ways for nonexperts to perform tests on text and image generators to evaluate bias and other problems. If outsiders can probe AI systems, companies will no longer have an excuse to avoid testing for things like skewed outputs or climate impacts, says Solaiman, who previously worked at OpenAI on reducing the system‚Äôs toxicity.
Each evaluation is a window into an AI model, Solaiman says, not a perfect readout of how it will always perform. But she hopes to make it possible to identify and stop harms that AI can cause because alarming cases have already arisen, including players of the game AI Dungeon using GPT-3 to generate text describing sex scenes involving children.
 ‚ÄúThat‚Äôs an extreme case of what we can‚Äôt afford to let happen,‚Äù Solaiman says.
Solaiman‚Äôs latest research at Hugging Face found that major tech companies have taken an increasingly closed approach to the generative models they released from 2018 to 2022. That trend accelerated with Alphabet‚Äôs AI teams at Google and DeepMind, and more widely across companies working on AI after the staged release of GPT-2. Companies that guard their breakthroughs as trade secrets can also make the forefront of AI less accessible for marginalized researchers with few resources, Solaiman says.
As more money gets shoveled into large language models, closed releases are reversing the trend seen throughout the history of the field of natural language processing. Researchers have traditionally shared details about training data sets, parameter weights, and code to promote reproducibility of results.
‚ÄúWe have increasingly little knowledge about what database systems were trained on or how they were evaluated, especially for the most powerful systems being released as products,‚Äù says Alex Tamkin, a Stanford University PhD student whose work focuses on large language models.
He credits people in the field of AI ethics with raising public consciousness about why it‚Äôs dangerous to move fast and break things when technology is deployed to billions of people. Without that work in recent years, things could be a lot worse.
In fall 2020, Tamkin co-led a symposium with OpenAI‚Äôs policy director, Miles Brundage, about the societal impact of large language models. The interdisciplinary group emphasized the need for industry leaders to set ethical standards and take steps like running bias evaluations before deployment and avoiding certain use cases.
Business What Sam Altman‚Äôs Firing Means for the Future of OpenAI Steven Levy Business Sam Altman‚Äôs Sudden Exit Sends Shockwaves Through OpenAI and Beyond Will Knight Gear Humanity‚Äôs Most Obnoxious Vehicle Gets an Electric (and Nearly Silent) Makeover Boone Ashworth Security The Startup That Transformed the Hack-for-Hire Industry Andy Greenberg Tamkin believes external AI auditing services need to grow alongside the companies building on AI because internal evaluations tend to fall short. He believes participatory methods of evaluation that include community members and other stakeholders have great potential to increase democratic participation in the creation of AI models.
Merve Hickok, who is a research director at an AI ethics and policy center at the University of Michigan, says trying to get companies to put aside or puncture AI hype, regulate themselves, and adopt ethics principles isn‚Äôt enough. Protecting human rights means moving past conversations about what‚Äôs ethical and into conversations about what‚Äôs legal, she says.
Hickok and Hanna of DAIR are both watching the European Union finalize its AI Act this year to see how it treats models that generate text and imagery. Hickok said she‚Äôs especially interested in seeing how European lawmakers treat liability for harm involving models created by companies like Google, Microsoft, and OpenAI.
‚ÄúSome things need to be mandated because we have seen over and over again that if not mandated, these companies continue to break things and continue to push for profit over rights, and profit over communities,‚Äù Hickok says.
While policy gets hashed out in Brussels, the stakes remain high. A day after the Bard demo mistake, a drop in Alphabet‚Äôs stock price shaved about $100 billion in market cap. ‚ÄúIt‚Äôs the first time I‚Äôve seen this destruction of wealth because of a large language model error on that scale,‚Äù says Hanna. She is not optimistic this will convince the company to slow its rush to launch, however. ‚ÄúMy guess is that it‚Äôs not really going to be a cautionary tale.‚Äù Updated 2-16-2023, 12.15 pm EST: A previous version of this article misspelled Merve Hickok's name.
You Might Also Like ‚Ä¶ üì® Make the most of chatbots with our AI Unlocked newsletter Taylor Swift, Star Wars, Stranger Things , and Deadpool have one man in common Generative AI is playing a surprising role in Israel-Hamas disinformation The new era of social media looks as bad for privacy as the last one Johnny Cash‚Äôs Taylor Swift cover predicts the boring future of AI music Your internet browser does not belong to you üîå Charge right into summer with the best travel adapters , power banks , and USB hubs Senior Writer X Topics artificial intelligence machine learning algorithms deep learning Google ethics neural networks big data Microsoft ChatGPT Khari Johnson Will Knight Khari Johnson Will Knight Gregory Barber Khari Johnson Peter Guest Steven Levy Facebook X Pinterest YouTube Instagram Tiktok More From WIRED Subscribe Newsletters Mattresses Reviews FAQ Wired Staff Coupons Black Friday Editorial Standards Archive Contact Advertise Contact Us Customer Care Jobs Press Center RSS Accessibility Help Cond√© Nast Store Do Not Sell My Personal Info ¬© 2023 Cond√© Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights.
WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Cond√© Nast.
Ad Choices Select international site United States LargeChevron UK Italia Jap√≥n Czech Republic & Slovakia
