Artificial Intelligence View All AI, ML and Deep Learning Auto ML Data Labelling Synthetic Data Conversational AI NLP Text-to-Speech Security View All Data Security and Privacy Network Security and Privacy Software Security Computer Hardware Security Cloud and Data Storage Security Data Infrastructure View All Data Science Data Management Data Storage and Cloud Big Data and Analytics Data Networks Automation View All Industrial Automation Business Process Automation Development Automation Robotic Process Automation Test Automation Enterprise Analytics View All Business Intelligence Disaster Recovery Business Continuity Statistical Analysis Predictive Analysis More Data Decision Makers Virtual Communication Team Collaboration UCaaS Virtual Reality Collaboration Virtual Employee Experience Programming & Development Product Development Application Development Test Management Development Languages MLPerf introduces machine learning inference benchmark suite Share on Facebook Share on X Share on LinkedIn Are you ready to bring more awareness to your brand? Consider becoming a sponsor for The AI Impact Tour. Learn more about the opportunities here.
A major consortium of AI community stakeholders today introduced MLPerf Inference v0.5, the group’s first suite for measurement of AI system power efficiency and performance. Inference benchmarks are essential to understanding just how much time and power is required to deploy a neural network for common tasks like computer vision that predicts the contents of an image.
The suite consists of 5 benchmarks that include English-German machine translations with the WMT English-German data set, 2 object detection benchmarks with the COCO data set , and 2 image classification benchmarks with the ImageNet data set.
Submissions will be reviewed in September and MLPerf will share performance results in October, an organization spokesperson told VentureBeat in an email.
VB Event The AI Impact Tour Connect with the enterprise AI community at VentureBeat’s AI Impact Tour coming to a city near you! The inference standards were decided upon over the course of the past 11 months by partner organizations such as Arm, Facebook, Google, General Motors, Nvidia, and Toronto University, MLPerf said in a statement shared with VentureBeat.
MLPerf Inference Working Group cochair David Kanter told VentureBeat in a phone interview that benchmarks are important to using inference systems to definitively decide which solutions are worth the investment. They’re also important for engineers to understand what to optimize in the systems they’re making.
“If you’re a researcher or you’re an engineer designing these next generation systems, it’s important to know what are the workloads and metrics that matter, because ultimately at the end of the day, all the engineers working on this stuff are very smart and talented folks, but we’ve got to point them in the right direction and make sure they’re optimizing for the right things so that the solutions that come out — whether it’s 2 or 5 years from now — are designed for the workloads of today and tomorrow,” Kanter said.
Inference benchmarks introduced today follow the alpha release of the MLPerf Training benchmark in December 2018.
Results found Google tensor processing units and Nvidia graphic processing units among the top hardware for accelerating machine learning.
MLPerf is a group of 40 organizations like Alibaba, Baidu, Facebook, and Google that are working together to create a set of benchmarks — agreed-upon common sets of models and data sets — used to determine how quickly hardware can train an AI model or deploy from environments like cloud computing platforms, on devices like PCs and smartphones, and on multi-stream systems for autonomous vehicles.
The MLPerf reference code implementations will be incorporated into popular tools like Facebook”s PyTorch, Google’s TensorFlow, and the ONNX consortium for interoperability between the hardware and frameworks used for machine learning.
Plans for MLPerf to measure AI energy efficiency rates come shortly after researchers found that training deep learning algorithms like OpenAI’s GPT-2 and Google’s BERT and Transformer can have a carbon footprint 5 times that of a car.
Last week, supercomputer analysis group TOP500 released its Green500 ranking of energy-efficient hardware, which found that Shoubu system B was the most efficient AI accelerator with 17.6 gigaflops/watt. Shoubu is followed by Nvidia’s DGX SaturnV Volta and Summit’s AI Bridging Cloud Infrastructure being built at the University of Tokyo.
VentureBeat's mission is to be a digital town square for technical decision-makers to gain knowledge about transformative enterprise technology and transact.
Discover our Briefings.
The AI Impact Tour Join us for an evening full of networking and insights at VentureBeat's AI Impact Tour, coming to San Francisco, New York, and Los Angeles! VentureBeat Homepage Follow us on Facebook Follow us on X Follow us on LinkedIn Follow us on RSS Press Releases Contact Us Advertise Share a News Tip Contribute to DataDecisionMakers Careers Privacy Policy Terms of Service Do Not Sell My Personal Information © 2023 VentureBeat.
 All rights reserved.
