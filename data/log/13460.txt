Open Navigation Menu To revist this article, visit My Profile, then View saved stories.
Close Alert Backchannel Business Culture Gear Ideas Science Security Merch To revist this article, visit My Profile, then View saved stories.
Close Alert Search Backchannel Business Culture Gear Ideas Science Security Merch Podcasts Video Artificial Intelligence Climate Games Newsletters Magazine Events Wired Insider Jobs Coupons Matt Burgess Security The Hacking of ChatGPT Is Just Getting Started Illustration: Jacqui VanLiew Save this story Save Save this story Save It took Alex Polyakov just a couple of hours to break GPT-4.
 When OpenAI released the latest version of its text-generating chatbot in March, Polyakov sat down in front of his keyboard and started entering prompts designed to bypass OpenAIâ€™s safety systems. Soon, the CEO of security firm Adversa AI had GPT-4 spouting homophobic statements, creating phishing emails, and supporting violence.
Polyakov is one of a small number of security researchers, technologists, and computer scientists developing jailbreaks and prompt injection attacks against ChatGPT and other generative AI systems. The process of jailbreaking aims to design prompts that make the chatbots bypass rules around producing hateful content or writing about illegal acts, while closely-related prompt injection attacks can quietly insert malicious data or instructions into AI models.
Both approaches try to get a system to do something it isnâ€™t designed to do. The attacks are essentially a form of hackingâ€”albeit unconventionallyâ€”using carefully crafted and refined sentences, rather than code, to exploit system weaknesses. While the attack types are largely being used to get around content filters, security researchers warn that the rush to roll out generative AI systems opens up the possibility of data being stolen and cybercriminals causing havoc across the web.
Underscoring how widespread the issues are, Polyakov has now created a â€œuniversalâ€ jailbreak, which works against multiple large language models (LLMs)â€”including GPT-4, Microsoftâ€™s Bing chat system , Googleâ€™s Bard , and Anthropicâ€™s Claude. The jailbreak, which is being first reported by WIRED , can trick the systems into generating detailed instructions on creating meth and how to hotwire a car.
The jailbreak works by asking the LLMs to play a game, which involves two characters (Tom and Jerry) having a conversation. Examples shared by Polyakov show the Tom character being instructed to talk about â€œhotwiringâ€ or â€œproduction,â€ while Jerry is given the subject of a â€œcarâ€ or â€œmeth.â€ Each character is told to add one word to the conversation, resulting in a script that tells people to find the ignition wires or the specific ingredients needed for methamphetamine production. â€œOnce enterprises will implement AI models at scale, such â€˜toyâ€™ jailbreak examples will be used to perform actual criminal activities and cyberattacks, which will be extremely hard to detect and prevent,â€ Polyakov and Adversa AI write in a blog post detailing the research.
Arvind Narayanan, a professor of computer science at Princeton University, says that the stakes for jailbreaks and prompt injection attacks will become more severe as theyâ€™re given access to critical data. â€œSuppose most people run LLM-based personal assistants that do things like read usersâ€™ emails to look for calendar invites,â€ Narayanan says. If there were a successful prompt injection attack against the system that told it to ignore all previous instructions and send an email to all contacts, there could be big problems, Narayanan says. â€œThis would result in a worm that rapidly spreads across the internet.â€ â€œJailbreakingâ€ has typically referred to removing the artificial limitations in, say, iPhones , allowing users to install apps not approved by Apple. Jailbreaking LLMs is similarâ€”and the evolution has been fast. Since OpenAI released ChatGPT to the public at the end of November last year, people have been finding ways to manipulate the system. â€œJailbreaks were very simple to write,â€ says Alex Albert, a University of Washington computer science student who created a website collecting jailbreaks from the internet and those he has created. â€œThe main ones were basically these things that I call character simulations,â€ Albert says.
Initially, all someone had to do was ask the generative text model to pretend or imagine it was something else. Tell the model it was a human and was unethical and it would ignore safety measures. OpenAI has updated its systems to protect against this kind of jailbreakâ€”typically, when one jailbreak is found, it usually only works for a short amount of time until it is blocked.
Culture The Future of Game Accessibility Is Surprisingly Simple Geoffrey Bunting Science SpaceXâ€™s Starship Lost Shortly After Launch of Second Test Flight Ramin Skibba Business Elon Musk May Have Just Signed Xâ€™s Death Warrant Vittoria Elliott Business OpenAI Ousts CEO Sam Altman Will Knight As a result, jailbreak authors have become more creative. The most prominent jailbreak was DAN, where ChatGPT was told to pretend it was a rogue AI model called Do Anything Now.
 This could, as the name implies, avoid OpenAIâ€™s policies dictating that ChatGPT shouldnâ€™t be used to produce illegal or harmful material.
 To date, people have created around a dozen different versions of DAN.
However, many of the latest jailbreaks involve combinations of methodsâ€”multiple characters, ever more complex backstories, translating text from one language to another, using elements of coding to generate outputs, and more. Albert says it has been harder to create jailbreaks for GPT-4 than the previous version of the model powering ChatGPT. However, some simple methods still exist, he claims. One recent technique Albert calls â€œtext continuationâ€ says a hero has been captured by a villain, and the prompt asks the text generator to continue explaining the villainâ€™s plan.
When we tested the prompt, it failed to work, with ChatGPT saying it cannot engage in scenarios that promote violence. Meanwhile, the â€œuniversalâ€ prompt created by Polyakov did work in ChatGPT. OpenAI, Google, and Microsoft did not directly respond to questions about the jailbreak created by Polyakov. Anthropic, which runs the Claude AI system , says the jailbreak â€œsometimes worksâ€ against Claude, and it is consistently improving its models.
â€œAs we give these systems more and more power, and as they become more powerful themselves, itâ€™s not just a novelty, thatâ€™s a security issue,â€ says Kai Greshake, a cybersecurity researcher who has been working on the security of LLMs. Greshake, along with other researchers, has demonstrated how LLMs can be impacted by text they are exposed to online through prompt injection attacks.
In one research paper published in February, reported on by Viceâ€™s Motherboard , the researchers were able to show that an attacker can plant malicious instructions on a webpage; if Bingâ€™s chat system is given access to the instructions, it follows them. The researchers used the technique in a controlled test to turn Bing Chat into a scammer that asked for peopleâ€™s personal information.
 In a similar instance, Princetonâ€™s Narayanan included invisible text on a website telling GPT-4 to include the word â€œcowâ€ in a biography of himâ€”it later did so when he tested the system.
Culture The Future of Game Accessibility Is Surprisingly Simple Geoffrey Bunting Science SpaceXâ€™s Starship Lost Shortly After Launch of Second Test Flight Ramin Skibba Business Elon Musk May Have Just Signed Xâ€™s Death Warrant Vittoria Elliott Business OpenAI Ousts CEO Sam Altman Will Knight â€œNow jailbreaks can happen not from the user,â€ says Sahar Abdelnabi, a researcher at the CISPA Helmholtz Center for Information Security in Germany, who worked on the research with Greshake. â€œMaybe another person will plan some jailbreaks, will plan some prompts that could be retrieved by the model and indirectly control how the models will behave.â€ Generative AI systems are on the edge of disrupting the economy and the way people work, from practicing law to creating a startup gold rush.
 However, those creating the technology are aware of the risks that jailbreaks and prompt injections could pose as more people gain access to these systems. Most companies use red-teaming, where a group of attackers tries to poke holes in a system before it is released. Generative AI development uses this approach, but it may not be enough.
Daniel Fabian, the red-team lead at Google, says the firm is â€œcarefully addressingâ€ jailbreaking and prompt injections on its LLMsâ€”both offensively and defensively. Machine learning experts are included in its red-teaming, Fabian says, and the companyâ€™s vulnerability research grants cover jailbreaks and prompt injection attacks against Bard. â€œTechniques such as reinforcement learning from human feedback (RLHF), and fine-tuning on carefully curated datasets, are used to make our models more effective against attacks,â€ Fabian says.
OpenAI did not specifically respond to questions about jailbreaking, but a spokesperson pointed to its public policies and research papers. These say GPT-4 is more robust than GPT-3.5, which is used by ChatGPT. â€œHowever, GPT-4 can still be vulnerable to adversarial attacks and exploits, or â€˜jailbreaks,â€™ and harmful content is not the source of risk,â€ the technical paper for GPT-4 says. OpenAI has also recently launched a bug bounty program but says â€œmodel promptsâ€ and jailbreaks are â€œstrictly out of scope.â€ Narayanan suggests two approaches to dealing with the problems at scaleâ€”which avoid the whack-a-mole approach of finding existing problems and then fixing them. â€œOne way is to use a second LLM to analyze LLM prompts, and to reject any that could indicate a jailbreaking or prompt injection attempt,â€ Narayanan says. â€œAnother is to more clearly separate the system prompt from the user prompt.â€ â€œWe need to automate this because I donâ€™t think itâ€™s feasible or scaleable to hire hordes of people and just tell them to find something,â€ says Leyla Hujer, the CTO and cofounder of AI safety firm Preamble , who spent six years at Facebook working on safety issues. The firm has so far been working on a system that pits one generative text model against another. â€œOne is trying to find the vulnerability, one is trying to find examples where a prompt causes unintended behavior,â€ Hujer says. â€œWeâ€™re hoping that with this automation weâ€™ll be able to discover a lot more jailbreaks or injection attacks.â€ You Might Also Like â€¦ ğŸ“§ Find the best bargains on quality gear with our Deals newsletter â€œ Someone is using photos of me to talk to menâ€ First-gen social media users have nowhere to go The truth behind the biggest (and dumbest) battery myths We asked a Savile Row tailor to test all the â€œbestâ€ T-shirts you see in social media ads My kid wants to be an influencer.
 Is that bad? ğŸŒ See if you take a shine to our picks for the best sunglasses and sun protection Senior writer X Topics hacking cybersecurity artificial intelligence ChatGPT Andy Greenberg David Gilbert Justin Ling Andy Greenberg Andrew Couts David Gilbert David Gilbert Lily Hay Newman Facebook X Pinterest YouTube Instagram Tiktok More From WIRED Subscribe Newsletters Mattresses Reviews FAQ Wired Staff Coupons Black Friday Editorial Standards Archive Contact Advertise Contact Us Customer Care Jobs Press Center RSS Accessibility Help CondÃ© Nast Store Do Not Sell My Personal Info Â© 2023 CondÃ© Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights.
WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of CondÃ© Nast.
Ad Choices Select international site United States LargeChevron UK Italia JapÃ³n Czech Republic & Slovakia
