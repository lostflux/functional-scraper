Open Navigation Menu To revist this article, visit My Profile, then View saved stories.
Close Alert Backchannel Business Culture Gear Ideas Science Security Merch To revist this article, visit My Profile, then View saved stories.
Close Alert Search Backchannel Business Culture Gear Ideas Science Security Merch Podcasts Video Artificial Intelligence Climate Games Newsletters Magazine Events Wired Insider Jobs Coupons Max G. Levy Business Timnit Gebru Says Artificial Intelligence Needs to Slow Down Save this story Save Save this story Save Application Ethics Company Alphabet Google End User Big company Sector IT Source Data Text Technology Natural language processing Artificial intelligence researchers are facing a problem of accountability: How do you try to ensure decisions are responsible when the decision maker is not a responsible person, but rather an algorithm ? Right now, only a handful of people and organizations have the powerâ€”and resourcesâ€”to automate decision-making.
Organizations rely on AI to approve a loan or shape a defendantâ€™s sentence. But the foundations upon which these intelligent systems are built are susceptible to bias. Bias from the data, from the programmer, and from a powerful companyâ€™s bottom line can snowball into unintended consequences. This is the reality AI researcher Timnit Gebru cautioned against at a RE:WIRED talk on Tuesday.
â€œThere were companies purporting [to assess] someoneâ€™s likelihood of determining a crime again,â€ Gebru said. â€œThat was terrifying for me.â€ Gebru was a star engineer at Google who specialized in AI ethics. She co-led a team tasked with standing guard against algorithmic racism, sexism, and other bias. Gebru also cofounded the nonprofit Black in AI, which seeks to improve inclusion, visibility, and health of Black people in her field.
Last year, Google forced her out. But she hasnâ€™t given up her fight to prevent unintended damage from machine learning algorithms.
Tuesday, Gebru spoke with WIRED senior writer Tom Simonite about incentives in AI research, the role of worker protections, and the vision for her planned independent institute for AI ethics and accountability. Her central point: AI needs to slow down.
â€œWe havenâ€™t had the time to think about how it should even be built because weâ€™re always just putting out fires,â€ she said.
As an Ethiopian refugee attending public school in the Boston suburbs, Gebru was quick to pick up on Americaâ€™s racial dissonance. Lectures referred to racism in the past tense, but that didnâ€™t jibe with what she saw, Gebru told Simonite earlier this year. She has found a similar misalignment repeatedly in her tech career.
Gebruâ€™s professional career began in hardware. But she changed course when she saw barriers to diversity and began to suspect that most AI research had the potential to bring harm to already marginalized groups.
â€œThe confluence of that got me going in a different direction, which is to try to understand and try to limit the negative societal impacts of AI,â€ she said.
Culture The Future of Game Accessibility Is Surprisingly Simple Geoffrey Bunting Science SpaceXâ€™s Starship Lost Shortly After Launch of Second Test Flight Ramin Skibba Business Elon Musk May Have Just Signed Xâ€™s Death Warrant Vittoria Elliott Business OpenAI Ousts CEO Sam Altman Will Knight For two years, Gebru co-led Googleâ€™s Ethical AI team with computer scientist Margaret Mitchell. The team created tools to protect against AI mishaps for Googleâ€™s product teams. Over time, though, Gebru and Mitchell realized they were being left out of meetings and email threads.
In June 2020, the GPT-3 language model was released and displayed an ability to sometimes craft coherent prose. But Gebru's team worried about the excitement around it.
â€œWe havenâ€™t had the time to think about how it should even be built because weâ€™re always just putting out fires.â€ Timnit Gebru, AI researcher â€œLetâ€™s build larger and larger and larger language models,â€ said Gebru, recalling the popular sentiment. â€œWe had to be like, â€˜Letâ€™s please just stop and calm down for a second so that we can think about the pros and cons and maybe alternative ways of doing this.â€™â€ Her team helped write a paper about the ethical implications of language models, called â€œOn the Dangers of Stochastic Parrots: Can Language Models Be Too Big?â€ Others at Google were not happy. Gebru was asked to retract the paper or remove Google employeesâ€™ names. She countered with an ask for transparency: Who had requested such harsh action and why? Neither side budged. Gebru found out from one of her direct reports that she â€œhad resigned.â€ The experience at Google reinforced in her a belief that oversight of AIâ€™s ethics should not be left to a corporation or government.
â€œThe incentive structure is not such that you slow down, first of all, think about how you should approach research, how you should approach AI, when it should be built, when it should not be built,â€ said Gebru. â€œI want us to be able to do AI research in a way that we think it should be doneâ€”prioritizing the voices that we think are actually being harmed.â€ Since leaving Google, Gebru has been developing an independent research institute to show a new model for responsible and ethical AI research. The institute aims to answer similar questions as her Ethical AI team, without fraught incentives of private, federal, or academic researchâ€”and without ties to corporations or the Department of Defense.
â€œOur goal is not to make Google more money; itâ€™s not to help the Defense Department figure out how to kill more people more efficiently,â€ she said.
At Tuesdayâ€™s session, Gebru said the institute will be unveiled on December 2, the anniversary of her ousting from Google. â€œMaybe Iâ€™ll just start celebrating this every year,â€ she joked.
Slowing the pace of AI might cost companies money, she said. â€œEither put more resources to prioritize safety or [donâ€™t] deploy things,â€ she added. â€œAnd unless there is regulation that prioritizes that, itâ€™s going to be very difficult to have all these companies, out of their own goodwill, self-regulate.â€ Still, Gebru finds room for optimism. â€œThe conversation has really shifted, and some of the people in the Biden administration working on this stuff are the right people,â€ she said. â€œI have to be hopeful. I donâ€™t think we have other options.â€ Watch the RE:WIRED conference on WIRED.com.
ğŸ“© The latest on tech, science, and more: Get our newsletters ! Neal Stephenson finally takes on global warming A cosmic ray event pinpoints the Viking landing in Canada How to delete your Facebook account forever A look inside Apple's silicon playbook Want a better PC? Try building your own ğŸ‘ï¸ Explore AI like never before with our new database ğŸ® WIRED Games: Get the latest tips, reviews, and more ğŸƒğŸ½â€â™€ï¸ Want the best tools to get healthy? Check out our Gear teamâ€™s picks for the best fitness trackers , running gear (including shoes and socks ), and best headphones Contributor X Topics RE: WIRED artificial intelligence algorithms diversity Google machine learning ethics Gregory Barber Niamh Rowe Matt Burgess Khari Johnson Peter Guest Reece Rogers Matt Laslo Deidre Olsen Facebook X Pinterest YouTube Instagram Tiktok More From WIRED Subscribe Newsletters Mattresses Reviews FAQ Wired Staff Coupons Black Friday Editorial Standards Archive Contact Advertise Contact Us Customer Care Jobs Press Center RSS Accessibility Help CondÃ© Nast Store Do Not Sell My Personal Info Â© 2023 CondÃ© Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights.
WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of CondÃ© Nast.
Ad Choices Select international site United States LargeChevron UK Italia JapÃ³n Czech Republic & Slovakia
