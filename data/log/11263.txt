Open Navigation Menu To revist this article, visit My Profile, then View saved stories.
Close Alert Backchannel Business Culture Gear Ideas Science Security Merch To revist this article, visit My Profile, then View saved stories.
Close Alert Search Backchannel Business Culture Gear Ideas Science Security Merch Podcasts Video Artificial Intelligence Climate Games Newsletters Magazine Events Wired Insider Jobs Coupons Tom Simonite Business AI Has Started Cleaning Up Facebook, but Can It Finish? Hotlittlepotato Save this story Save Save this story Save Application Content moderation Safety Text analysis Company Facebook End User Big company Sector Social media Source Data Images Text Video Technology Natural language processing Machine vision Neural Network In the early hours of Aug. 25, 2017, a ragged insurgent group from Myanmar‚Äôs Rohingya Muslim minority attacked military outposts in the country‚Äôs northwest, killing 12 people. Security forces quickly retaliated with a campaign of village burning and mass killings that lasted weeks. As Rohingya died by the thousands, Myanmar‚Äôs military leaders took to Facebook.
A post from the commander-in-chief pledged to solve ‚Äúthe Bengali problem,‚Äù using a pejorative for Rohingya in Myanmar. Another general wrote to praise the ‚Äúbrilliant effort to restore regional peace,‚Äù observing that ‚Äúrace cannot be swallowed by the ground but only by another race.‚Äù A UN fact-finding report on the violence later cited the commander-in-chief‚Äôs post as suggestive of genocide, and noted the history of Facebook posts whipping up hate against Rohingya in Myanmar. The mission‚Äôs chair told journalists that the site had played a ‚Äúdetermining role‚Äù in the crisis.
In the US Capitol in April, Senator Jeff Flake asked Facebook CEO Mark Zuckerberg how his company might have avoided that role. The impassive then-33-year-old billionaire noted that he had hired more Burmese speakers. Then he expounded on a favorite topic‚Äîartificial intelligence. ‚ÄúOver the long term, building AI tools is going to be the scalable way to identify and root out most of this harmful content,‚Äù he said. During two days of congressional hearings, Zuckerberg mentioned AI more than 30 times. It would, he told lawmakers, fight fake news, prevent ads that discriminate on the grounds of race or gender, and hobble terrorist propaganda.
Facebook has faced a dizzying series of accusations and scandals over the past year. They include enabling Russian election interference and employment discrimination , in addition to being accessory to genocide in Myanmar. Monday, a Senate report said Russia's activities on Facebook properties were far greater than previously known, and suggested the the company misled Congress by downplaying the idea that Russian trolls used its product to suppress turnout during the 2016 presidential election.
Many of Facebook‚Äôs apologies exhibit a common theme: Artificial intelligence will help solve the problems incubating on the company's platform. Mike Schroepfer, the company‚Äôs chief technology officer, says the technology is the only way to prevent bad actors from taking advantage of the service. With 2.3 billion regular users, having everything reviewed by humans would be prohibitively expensive‚Äîand creepy. ‚ÄúI think most people would feel uncomfortable with that,‚Äù Schroepfer says, eliding the possibility users may find it creepy to have algorithms review their every post. ‚ÄúTo me AI is the best tool to implement the policy‚ÄîI actually don't know what the alternative is.‚Äù Culture The Future of Game Accessibility Is Surprisingly Simple Geoffrey Bunting Science SpaceX‚Äôs Starship Lost Shortly After Launch of Second Test Flight Ramin Skibba Business Elon Musk May Have Just Signed X‚Äôs Death Warrant Vittoria Elliott Business OpenAI Ousts CEO Sam Altman Will Knight Facebook CTO Mike Schroepfer PATRICIA DE MELO MOREIRA/AFP/Getty Images Counting on AI is a gamble. Algorithms have proved capable of helping to police Facebook, but they are far from a cure-all‚Äîand may never be. The company has had great success in detecting and blocking pornography and nudity. But training software to reliably decode text is much more difficult than categorizing images. To tamp down harassment, hate speech, and dangerous conspiracy theories across its vast platform, Facebook needs AI systems capable of understanding the shifting nuances of more than 100 different languages. Any shortfalls must be caught by Facebook‚Äôs roughly 15,000 human reviewers, but at the social network‚Äôs scale it‚Äôs unclear how manageable their workload will be. As events in Myanmar showed, gaps in the enforcement net that may look small from Menlo Park can feel dangerously large to people whose world is being shaped by Facebook.
Facebook‚Äôs push to automate its content moderation started on the initiative of an ad executive, not an expert in online discourse. Tanton Gibbs was hired as an engineering director in 2014 to work on ad technology, as he had previously at Microsoft and Google. After hearing about Facebook‚Äôs moderation challenges, he suggested a more algorithms-first approach. Facebook had adopted a tool called PhotoDNA developed by Microsoft and Dartmouth College to block known child exploitation images, but wasn‚Äôt deploying image-analysis software or AI more broadly. ‚ÄúThey were strictly using humans to review reports for things like pornography, hate speech, or graphic violence,‚Äù says Gibbs. ‚ÄúI saw we should automate that.‚Äù Facebook put Gibbs at the head of a new team, based in Seattle, known initially as CareML.
The new group quickly proved its worth. Gibbs and his engineers embraced a technology called deep learning , an approach to training algorithms with example data that had recently become much more powerful.
 Google showed the power of the technology when it developed software that learned to recognize cats.
 More quietly, Gibbs‚Äô group taught deep learning algorithms to recognize pornography and nude human beings. Initially that software reviewed images flagged by Facebook users. After a year and a half, Gibbs got permission to let his systems flag newly submitted content before anyone reported it. Facebook says 96 percent of adult and nude images are now automatically detected, and taken down, before anyone reports them.
That‚Äôs still a lot of nude flesh slipping past Facebook‚Äôs algorithms. The company says it took down 30.8 million images and videos of nudity or sexual activity in the third quarter of 2018; that means the algorithms didn‚Äôt catch 1.3 million such images. In fact, Facebook estimates that the percentage of views with nudity or sexual content nearly doubled over the 12 months ending in September, to about 9 in every 10,000 views. ‚ÄúMore nudity was posted on Facebook, and our systems did not catch all of it fast enough to prevent an increase in views,‚Äù Facebook said in its most recent community standards enforcement report. How much was posted and seen but not detected or reported is unknowable.
Still, the success of Gibbs‚Äô project in fighting pornography has become a favorite talking point of Facebook executives touting the potential of AI to clean up their service. It‚Äôs working proof of the idea that an algorithmic immune system can help shelter Facebook users from harmful content‚Äîand the company from the consequences of hosting it. Facebook says that just over half of hate speech removed from the platform in the most recent three months was flagged first by algorithms, more than double the proportion earlier in the year. Some 15 percent of posts removed for bullying are identified and taken down before anyone has reported them. In neither case, though, do the algorithms remove the post; the programs flag the posts to be reviewed by people.
Facebook‚Äôs challenge is getting its technology to work well enough that its roughly 15,000 human reviewers can reliably pick up the slack, in each of the more than 100 countries and languages the service is used. Getting its hate speech and bullying detectors close to the effectiveness and autonomy of its porn filters will be particularly difficult.
Culture The Future of Game Accessibility Is Surprisingly Simple Geoffrey Bunting Science SpaceX‚Äôs Starship Lost Shortly After Launch of Second Test Flight Ramin Skibba Business Elon Musk May Have Just Signed X‚Äôs Death Warrant Vittoria Elliott Business OpenAI Ousts CEO Sam Altman Will Knight Deep learning algorithms are pretty good at sorting images into categories‚Äîcat or car, porn or not porn. They‚Äôve also made computers better with language, enabling virtual assistants like Alexa and significant jumps in the accuracy of automatic translations.
 But they‚Äôre still a long way from understanding even relatively simple text in the way humans do.
To understand whether a post reading ‚ÄúI‚Äôm going to beat you‚Äù is a threat or a friendly joke, a human reviewer might effortlessly take into account whether it was paired with an image of a neighborhood basketball court, or the phrasing and tone of earlier messages. ‚ÄúHow a model could use context in that way is not understood,‚Äù says Ruihong Huang, a professor at Texas A&M University. She helped organize an academic workshop on using algorithms to fight online abuse this fall, at one of the world‚Äôs top conferences for language processing research. Attendance and the number of papers submitted roughly doubled compared with the event‚Äôs debut in 2017‚Äîand not because researchers smelled victory. ‚ÄúMany companies and people in academia are realizing this is an important task and problem, but the progress is not that satisfying so far,‚Äù says Huang. ‚ÄúThe current models are not that intelligent in short, that‚Äôs the problem.‚Äù Srinivas Narayanan, who leads engineering in Facebook‚Äôs Applied Machine Learning group, agrees. He‚Äôs proud of the work his team has done on systems that can scan for porn and hate speech at huge scale, but human-level accuracy and nuance remains a distant hope. ‚ÄúI think we‚Äôre still far away from being able to understand that deeply,‚Äù he says. ‚ÄúI think machines can eventually, but we just don‚Äôt know how.‚Äù Facebook has a large, multinational AI lab working on long term, fundamental research that may one day help solve that mystery. It also has journalists, lawmakers, civil society groups, and even the UN expecting improvements right now. Facebook‚Äôs AI team needs to develop tricks that can deliver meaningful progress before the next scandal hits.
The products of that push for practical new AI tools include a system called Rosetta announced this year that reads out text that is embedded in images and video, allowing it to be fed into hate speech detectors. (There‚Äôs evidence some online trolls are already testing ways to trick it.
) Another project used billions of hashtags from Instagram users to improve Facebook‚Äôs image recognition systems. The company has even used examples of bullying posts on Facebook to train a kind of AI-powered cyberbully, which generates text generator to push its moderation algorithms to get better. The company declined to provide WIRED a sample of its output.
Culture The Future of Game Accessibility Is Surprisingly Simple Geoffrey Bunting Science SpaceX‚Äôs Starship Lost Shortly After Launch of Second Test Flight Ramin Skibba Business Elon Musk May Have Just Signed X‚Äôs Death Warrant Vittoria Elliott Business OpenAI Ousts CEO Sam Altman Will Knight One big challenge for these projects is that today‚Äôs machine learning algorithms must be trained with narrow, specific data.
 This summer, Facebook changed how some of its human moderators work, in part to generate more useful training data on hate speech. Instead of using their knowledge of Facebook‚Äôs rules to decide whether to delete a post flagged for hate speech, workers answered a series of narrower questions. Did the post use a slur? Does it make reference to a protected category? Was that category attacked in this post? A reviewer could then scan through all the answers to make the final call. The responses are also useful feedstock for training algorithms to spot slurs or other things for themselves. ‚ÄúThat granular labeling gets us really exciting raw training data to build out classifiers,‚Äù says Aashin Gautam, who leads a team that develops content moderation processes. Facebook is exploring making this new model permanent, initially for hate speech, and then perhaps for other categories of prohibited content.
Elsewhere, Facebook is trying to sidestep the training data problem. One lesson from the tragic events in Myanmar is that the company needs to get better at putting humans and software in place to understand the language and culture of different markets, says Justin Osofsky, a vice president who runs global operations.
The conventional approach to training algorithms to decode text in multiple languages would be extremely expensive for Facebook. To detect birthday greetings or hate speech in English, you need thousands, preferably millions of examples. Each time you want to expand to a new language, you need a fresh set of data‚Äîa major challenge for a company of Facebook‚Äôs scale.
As a solution, Facebook is adapting systems built for common languages such as English or Spanish to work for less common languages, like Romanian or Malay. One approach involves using automated translation. Facebook has been able to suppress clickbait in languages including Hungarian and Greek in part by converting posts into English so they can be fed into clickbait detectors trained on US content. It also conjures up new training sets for less common languages by translating English ones. Another project involves creating multilingual systems primed on deep similarities between languages, meaning that once trained on a task in English, they can instantly do the same thing in Italian, too. ‚ÄúThese multilingual approaches have really helped accelerate our ability to apply AI to integrity problems across languages,‚Äù says Narayanan.
Culture The Future of Game Accessibility Is Surprisingly Simple Geoffrey Bunting Science SpaceX‚Äôs Starship Lost Shortly After Launch of Second Test Flight Ramin Skibba Business Elon Musk May Have Just Signed X‚Äôs Death Warrant Vittoria Elliott Business OpenAI Ousts CEO Sam Altman Will Knight The project also helps illustrate the scale of Facebook‚Äôs challenge. So far, its multilingual workarounds don‚Äôt work on languages for which the company has relatively small datasets, such as Burmese. The same challenge exists for Hausa, a West African language used in campaigns of anti-Muslim hate speech that local police told the BBC last month have led to more than a dozen murders. Facebook says it is expanding its relationship with Nigerian fact checking organizations and NGOs‚Äîas well as its use of machine learning to flag hate speech and violent images.
Invited to look ahead, Schroepfer, Facebook‚Äôs chief technology officer, concedes that preventing incidents like that from ever happening is impossible. ‚ÄúOne question I often ask myself is what other endeavors of equivalent complexity have a 100 percent safety record,‚Äù he says. ‚ÄúI can‚Äôt think of one. Aircraft, cars, space travel, law enforcement. Do you know any city that‚Äôs got a zero crime rate or is on the path to that?‚Äù All the same, he remains optimistic enough about Facebook‚Äôs path to imagine a day when its algorithms are so effective that bullying and hate speech virtually vanish. ‚ÄúMy hope is that it in two or three or five years there is so little of it on the site that it‚Äôs sort of ridiculous to argue that‚Äôs it having a big effect on the world,‚Äù Schroepfer says. A techie can dream.
Racing to understand Antarctica's most terrifying glacier Aston Martin's $3 million Valkyrie gets a V12 engine How the CIA trains spies to hide in plain sight Facebook's dirty tricks are nothing new for tech How to use Apple Watch's new heart rate features üëÄ Looking for the latest gadgets? Check out our picks , gift guides , and best deals all year round üì© Hungry for even more deep dives on your next favorite topic? Sign up for the Backchannel newsletter Senior Editor X Topics Facebook artificial intelligence Social Media machine learning Steven Levy Morgan Meaker Reece Rogers Steven Levy Jacopo Prisco Will Knight Facebook X Pinterest YouTube Instagram Tiktok More From WIRED Subscribe Newsletters Mattresses Reviews FAQ Wired Staff Coupons Black Friday Editorial Standards Archive Contact Advertise Contact Us Customer Care Jobs Press Center RSS Accessibility Help Cond√© Nast Store Do Not Sell My Personal Info ¬© 2023 Cond√© Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights.
WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Cond√© Nast.
Ad Choices Select international site United States LargeChevron UK Italia Jap√≥n Czech Republic & Slovakia
