Open Navigation Menu To revist this article, visit My Profile, then View saved stories.
Close Alert Backchannel Business Culture Gear Ideas Science Security Merch To revist this article, visit My Profile, then View saved stories.
Close Alert Search Backchannel Business Culture Gear Ideas Science Security Merch Podcasts Video Artificial Intelligence Climate Games Newsletters Magazine Events Wired Insider Jobs Coupons Will Knight Business Elon Musk Has Fired Twitterâ€™s â€˜Ethical AIâ€™ Team Photograph: Jeff Kravitz/Getty Images Save this story Save Save this story Save Application Content moderation Safety Ethics End User Big company Sector Social media Source Data Images Text Technology Machine learning Natural language processing Not long after Elon Musk announced plans to acquire Twitter last March, he mused about open sourcing â€œthe algorithmâ€ that determines how tweets are surfaced in user feeds so that it could be inspected for bias.
His fansâ€”as well as those who believe the social media platform harbors a left-wing biasâ€”were delighted.
But today, as part of an aggressive plan to trim costs that involves firing thousands of Twitter employees, Muskâ€™s management team cut a team of artificial intelligence researchers who were working toward making Twitterâ€™s algorithms more transparent and fair.
Rumman Chowdhury , director of the ML Ethics, Transparency, and Accountability (METAâ€”no, not that one ) team at Twitter, tweeted that she had been let go as part of mass layoffs implemented by new managementâ€”although it hardly seemed that she was relishing the idea of working under Musk.
X content This content can also be viewed on the site it originates from.
Chowdhury told WIRED earlier this week that the groupsâ€™ work was put on hold as a result of Muskâ€™s impending acquisition. â€œWe were told, in no uncertain terms, not to rock the boat,â€ she said. Chowdhury also said that her team had been doing some important new research on political bias that might have helped Twitter and other social networks from preventing particular viewpoints from being unfairly penalized.
Culture The Future of Game Accessibility Is Surprisingly Simple Geoffrey Bunting Science SpaceXâ€™s Starship Lost Shortly After Launch of Second Test Flight Ramin Skibba Business Elon Musk May Have Just Signed Xâ€™s Death Warrant Vittoria Elliott Business OpenAI Ousts CEO Sam Altman Will Knight Joan Deitchman , a senior manager at Twitterâ€™s META unit confirmed that the entire team had been fired. Kristian Lum, formerly a machine learning reacher on the team, said the â€œentire META team minus oneâ€ had been let go. Nobody from the team, or Twitter, could be reached for comment this morning.
X content This content can also be viewed on the site it originates from.
As more and more problems with AI have surfaced, including biases around race, gender, and age, many tech companies have installed â€œethical AIâ€ teams ostensibly dedicated to identifying and mitigating such issues.
Twitterâ€™s META unit was more progressive than most in publishing details of problems with the companyâ€™s AI systems, and in allowing outside researchers to probe its algorithms for new issues.
Last year, after Twitter users noticed that a photo-cropping algorithm seemed to favor white faces when choosing how to trim images, Twitter took the unusual decision to let its META unit publish details of the bias it uncovered. The group also launched one of the first ever â€œbias bountyâ€ contests, which let outside researchers test the algorithm for other problems. Last October, Chowdhuryâ€™s team also published details of unintentional political bias on Twitter, showing how right-leaning news sources were, in fact, promoted more than left-leaning ones.
Culture The Future of Game Accessibility Is Surprisingly Simple Geoffrey Bunting Science SpaceXâ€™s Starship Lost Shortly After Launch of Second Test Flight Ramin Skibba Business Elon Musk May Have Just Signed Xâ€™s Death Warrant Vittoria Elliott Business OpenAI Ousts CEO Sam Altman Will Knight Many outside researchers saw the layoffs as a blow, not just for Twitter but for efforts to improve AI. â€œWhat a tragedy,â€ Kate Starbird , an associate professor at the University of Washington who studies online disinformation, wrote on Twitter.
X content This content can also be viewed on the site it originates from.
â€œThe META team was one of the only good case studies of a tech company running an AI ethics group that interacts with the public and academia with substantial credibility,â€ says Ali Alkhatib , director of the Center for Applied Data Ethics at the University of San Francisco.
Alkhatib says Chowdhury is incredibly well thought of within the AI ethics community and her team did genuinely valuable work holding Big Tech to account. â€œThere arenâ€™t many corporate ethics teams worth taking seriously,â€ he says. â€œThis was one of the ones whose work I taught in classes.â€ Mark Riedl , a professor studying AI at Georgia Tech, says the algorithms that Twitter and other social media giants use have a huge impact on peopleâ€™s lives, and need to be studied. â€œWhether META had any impact inside Twitter is hard to discern from the outside, but the promise was there,â€ he says.
Riedl adds that letting outsiders probe Twitterâ€™s algorithms was an important step toward more transparency and understanding of issues around AI. â€œThey were becoming a watchdog that could help the rest of us understand how AI was affecting us,â€ he says. â€œThe researchers at META had outstanding credentials with long histories of studying AI for social good.â€ As for Muskâ€™s idea of open-sourcing the Twitter algorithm, the reality would be far more complicated.
 There are many different algorithms that affect the way information is surfaced, and itâ€™s challenging to understand them without the real time data they are being fed in terms of tweets, views, and likes.
The idea that there is one algorithm with explicit political leaning might oversimplify a system that can harbor more insidious biases and problems. Uncovering these is precisely the kind of work that Twitterâ€™s META group was doing. â€œThere arenâ€™t many groups that rigorously study their own algorithmsâ€™ biases and errors,â€ says Alkhatib at the University of San Francisco. â€œMETA did that.â€ And now, it doesnâ€™t.
You Might Also Like â€¦ ğŸ“¨ Make the most of chatbots with our AI Unlocked newsletter Taylor Swift, Star Wars, Stranger Things , and Deadpool have one man in common Generative AI is playing a surprising role in Israel-Hamas disinformation The new era of social media looks as bad for privacy as the last one Johnny Cashâ€™s Taylor Swift cover predicts the boring future of AI music Your internet browser does not belong to you ğŸ”Œ Charge right into summer with the best travel adapters , power banks , and USB hubs Senior Writer X Topics artificial intelligence content moderation twitter Elon Musk Social Media Will Knight Will Knight Will Bedingfield Khari Johnson Matt Burgess Will Knight Morgan Meaker Kari McMahon Facebook X Pinterest YouTube Instagram Tiktok More From WIRED Subscribe Newsletters Mattresses Reviews FAQ Wired Staff Coupons Black Friday Editorial Standards Archive Contact Advertise Contact Us Customer Care Jobs Press Center RSS Accessibility Help CondÃ© Nast Store Do Not Sell My Personal Info Â© 2023 CondÃ© Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights.
WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of CondÃ© Nast.
Ad Choices Select international site United States LargeChevron UK Italia JapÃ³n Czech Republic & Slovakia
