Open Navigation Menu To revist this article, visit My Profile, then View saved stories.
Close Alert Backchannel Business Culture Gear Ideas Science Security Merch To revist this article, visit My Profile, then View saved stories.
Close Alert Search Backchannel Business Culture Gear Ideas Science Security Merch Podcasts Video Artificial Intelligence Climate Games Newsletters Magazine Events Wired Insider Jobs Coupons Will Knight Business Some Glimpse AGI in ChatGPT. Others Call It a Mirage Photograph: Eugene Mymrin/Getty Images Save this story Save Save this story Save SÃ©bastien Bubeck, a machine learning researcher at Microsoft , woke up one night last September thinking about artificial intelligence â€”and unicorns.
Bubeck had recently gotten early access to GPT-4 , a powerful text generation algorithm from OpenAI and an upgrade to the machine learning model at the heart of the wildly popular chatbot ChatGPT.
 Bubeck was part of a team working to integrate the new AI system into Microsoftâ€™s Bing search engine. But he and his colleagues kept marveling at how different GPT-4 seemed from anything theyâ€™d seen before.
GPT-4, like its predecessors, had been fed massive amounts of text and code and trained to use the statistical patterns in that corpus to predict the words that should be generated in reply to a piece of text input. But to Bubeck, the systemâ€™s output seemed to do so much more than just make statistically plausible guesses.
That night, Bubeck got up, went to his computer, and asked GPT-4 to draw a unicorn using TikZ , a relatively obscure programming language for generating scientific diagrams. Bubeck was using a version of GPT-4 that only worked with text, not images. But the code the model presented him with, when fed into a TikZ rendering software, produced a crude yet distinctly unicorny image cobbled together from ovals, rectangles, and a triangle. To Bubeck, such a feat surely required some abstract grasp of the elements of such a creature. â€œSomething new is happening here,â€ he says. â€œMaybe for the first time we have something that we could call intelligence.â€ How intelligent AI is becomingâ€”and how much to trust the increasingly common feeling that a piece of software is intelligentâ€”has become a pressing, almost panic-inducing, question.
After OpenAI released ChatGPT , then powered by GPT-3, last November, it stunned the world with its ability to write poetry and prose on a vast array of subjects, solve coding problems, and synthesize knowledge from the web. But awe has been coupled with shock and concern about the potential for academic fraud , misinformation , and mass unemployment â€”and fears that companies like Microsoft are rushing to develop technology that could prove dangerous.
Understanding the potential or risks of AIâ€™s new abilities means having a clear grasp of what those abilities areâ€”and are not. But while thereâ€™s broad agreement that ChatGPT and similar systems give computers significant new skills, researchers are only just beginning to study these behaviors and determine whatâ€™s going on behind the prompt.
Business What Sam Altmanâ€™s Firing Means for the Future of OpenAI Steven Levy Business Sam Altmanâ€™s Sudden Exit Sends Shockwaves Through OpenAI and Beyond Will Knight Gear Humanityâ€™s Most Obnoxious Vehicle Gets an Electric (and Nearly Silent) Makeover Boone Ashworth Security The Startup That Transformed the Hack-for-Hire Industry Andy Greenberg While OpenAI has promoted GPT-4 by touting its performance on bar and med school exams, scientists who study aspects of human intelligence say its remarkable capabilities differ from our own in crucial ways. The modelsâ€™ tendency to make things up is well known, but the divergence goes deeper. And with millions of people using the technology every day and companies betting their future on it, this is a mystery of huge importance.
Bubeck and other AI researchers at Microsoft were inspired to wade into the debate by their experiences with GPT-4. A few weeks after the system was plugged into Bing and its new chat feature was launched, the company released a paper claiming that in early experiments, GPT-4 showed â€œsparks of artificial general intelligence.â€ The authors presented a scattering of examples in which the system performed tasks that appear to reflect more general intelligence, significantly beyond previous systems such as GPT-3. The examples show that unlike most previous AI programs, GPT-4 is not limited to a specific task but can turn its hand to all sorts of problemsâ€”a necessary quality of general intelligence.
The authors also suggest that these systems demonstrate an ability to reason, plan, learn from experience, and transfer concepts from one modality to another, such as from text to imagery. â€œGiven the breadth and depth of GPT-4â€™s capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system,â€ the paper states.
Bubeckâ€™s paper, written with 14 others, including Microsoftâ€™s chief scientific officer, was met with pushback from AI researchers and experts on social media. Use of the term AGI, a vague descriptor sometimes used to allude to the idea of super-intelligent or godlike machines, irked some researchers, who saw it as a symptom of the current hype.
The fact that Microsoft has invested more than $10 billion in OpenAI suggested to some researchers that the companyâ€™s AI experts had an incentive to hype GPT-4â€™s potential while downplaying its limitations.
 Others griped that the experiments are impossible to replicate because GPT-4 rarely responds in the same way when a prompt is repeated, and because OpenAI has not shared details of its design. Of course, people also asked why GPT-4 still makes ridiculous mistakes if it is really so smart.
Business What Sam Altmanâ€™s Firing Means for the Future of OpenAI Steven Levy Business Sam Altmanâ€™s Sudden Exit Sends Shockwaves Through OpenAI and Beyond Will Knight Gear Humanityâ€™s Most Obnoxious Vehicle Gets an Electric (and Nearly Silent) Makeover Boone Ashworth Security The Startup That Transformed the Hack-for-Hire Industry Andy Greenberg Talia Ringer , a professor at the University of Illinois at Urbana-Champaign, says Microsoftâ€™s paper â€œshows some interesting phenomena and then makes some really over-the-top claims.â€ Touting that systems are highly intelligent encourages users to trust them even when theyâ€™re deeply flawed, they say. Ringer also points out that while it may be tempting to borrow ideas from systems developed to measure human intelligence, many have proven unreliable and even rooted in racism.
Bubek admits that his study has its limits, including the reproducibility issue, and that GPT-4 also has big blind spots. He says use of the term AGI was meant to provoke debate. â€œIntelligence is by definition general,â€ he says. â€œWe wanted to get at the intelligence of the model and how broad it isâ€”that it covers many, many domains.â€ But for all of the examples cited in Bubeckâ€™s paper, there are many that show GPT-4 getting things blatantly wrongâ€”often on the very tasks Microsoftâ€™s team used to tout its success. For example, GPT-4â€™s ability to suggest a stable way to stack a challenging collection of objectsâ€” a book, four tennis balls, a nail, a wine glass, a wad of gum, and some uncooked spaghetti â€”seems to point to a grasp of the physical properties of the world that is second nature to humans, including infants.
 However, changing the items and the request can result in bizarre failures that suggest GPT-4â€™s grasp of physics is not complete or consistent.
Bubeck notes that GPT-4 lacks a working memory and is hopeless at planning ahead. â€œGPT-4 is not good at this, and maybe large language models in general will never be good at it,â€ he says, referring to the large-scale machine learning algorithms at the heart of systems like GPT-4. â€œIf you want to say that intelligence is planning, then GPT-4 is not intelligent.â€ One thing beyond debate is that the workings of GPT-4 and other powerful AI language models do not resemble the biology of brains or the processes of the human mind. The algorithms must be fed an absurd amount of training dataâ€”a significant portion of all the text on the internetâ€”far more than a human needs to learn language skills. The â€œexperienceâ€ that imbues GPT-4, and things built with it, with smarts is shoveled in wholesale rather than gained through interaction with the world and didactic dialog. And with no working memory, ChatGPT can maintain the thread of a conversation only by feeding itself the history of the conversation over again at each turn. Yet despite these differences, GPT-4 is clearly a leap forward, and scientists who research intelligence say its abilities need further interrogation.
A team of cognitive scientists, linguists, neuroscientists, and computer scientists from MIT, UCLA, and the University of Texas, Austin, posted a research paper in January that explores how the abilities of large language models differ from those of humans.
The group concluded that while large language models demonstrate impressive linguistic skillâ€”including the ability to coherently generate a complex essay on a given themeâ€”that is not the same as understanding language and how to use it in the world. That disconnect may be why language models have begun to imitate the kind of commonsense reasoning needed to stack objects or solve riddles. But the systems still make strange mistakes when it comes to understanding social relationships, how the physical world works, and how people think.
Business What Sam Altmanâ€™s Firing Means for the Future of OpenAI Steven Levy Business Sam Altmanâ€™s Sudden Exit Sends Shockwaves Through OpenAI and Beyond Will Knight Gear Humanityâ€™s Most Obnoxious Vehicle Gets an Electric (and Nearly Silent) Makeover Boone Ashworth Security The Startup That Transformed the Hack-for-Hire Industry Andy Greenberg The way these models use language, by predicting the words most likely to come after a given string, is very difference from how humans speak or write to convey concepts or intentions. The statistical approach can cause chatbots to follow and reflect back the language of usersâ€™ prompts to the point of absurdity.
When a chatbot tells someone to leave their spouse , for example, it only comes up with the answer that seems most plausible given the conversational thread. ChatGPT and similar bots will use the first person because they are trained on human writing. But they have no consistent sense of self and can change their claimed beliefs or experiences in an instant. OpenAI also uses feedback from humans to guide a model toward producing answers that people judge as more coherent and correct, which may make the model provide answers deemed more satisfying regardless of how accurate they are.
Josh Tenenbaum , a contributor to the January paper and a professor at MIT who studies human cognition and how to explore it using machines, says GPT-4 is remarkable but quite different from human intelligence in a number of ways. For instance, it lacks the kind of motivation that is crucial to the human mind. â€œIt doesnâ€™t care if itâ€™s turned off,â€ Tenenbaum says. And he says humans do not simply follow their programming but invent new goals for themselves based on their wants and needs.
Tenenbaum says some key engineering shifts happened between GPT-3 and GPT-4 and ChatGPT that made them more capable. For one, the model was trained on large amounts of computer code. He and others have argued that the human brain may use something akin to a computer program to handle some cognitive tasks, so perhaps GPT-4 learned some useful things from the patterns found in code. He also points to the feedback ChatGPT received from humans as a key factor.
But he says the resulting abilities arenâ€™t the same as the general intelligence that characterizes human intelligence. â€œIâ€™m interested in the cognitive capacities that led humans individually and collectively to where we are now, and thatâ€™s more than just an ability to perform a whole bunch of tasks,â€ he says. â€œWe make the tasksâ€”and we make the machines that solve them.â€ Business What Sam Altmanâ€™s Firing Means for the Future of OpenAI Steven Levy Business Sam Altmanâ€™s Sudden Exit Sends Shockwaves Through OpenAI and Beyond Will Knight Gear Humanityâ€™s Most Obnoxious Vehicle Gets an Electric (and Nearly Silent) Makeover Boone Ashworth Security The Startup That Transformed the Hack-for-Hire Industry Andy Greenberg Tenenbaum also says it isnâ€™t clear that future generations of GPT would gain these sorts of capabilities, unless some different techniques are employed. This might mean drawing from areas of AI research that go beyond machine learning. And he says itâ€™s important to think carefully about whether we want to engineer systems that way, as doing so could have unforeseen consequences.
Another author of the January paper, Kyle Mahowald , an assistant professor of linguistics at the University of Texas at Austin, says itâ€™s a mistake to base any judgements on single examples of GPT-4â€™s abilities. He says tools from cognitive psychology could be useful for gauging the intelligence of such models. But he adds that the challenge is complicated by the opacity of GPT-4. â€œIt matters what is in the training data, and we donâ€™t know. If GPT-4 succeeds on some commonsense reasoning tasks for which it was explicitly trained and fails on others for which it wasnâ€™t, itâ€™s hard to draw conclusions based on that.â€ Whether GPT-4 can be considered a step toward AGI, then, depends entirely on your perspective. Redefining the term altogether may provide the most satisfying answer. â€œThese days my viewpoint is that this is AGI, in that it is a kind of intelligence and it is generalâ€”but we have to be a little bit less, you know, hysterical about what AGI means,â€ says Noah Goodman , an associate professor of psychology, computer science, and linguistics at Stanford University.
Unfortunately, GPT-4 and ChatGPT are designed to resist such easy reframing. They are smart but offer little insight into how or why. Whatâ€™s more, the way humans use language relies on having a mental model of an intelligent entity on the other side of the conversation to interpret the words and ideas being expressed. We canâ€™t help but see flickers of intelligence in something that uses language so effortlessly. â€œIf the pattern of words is meaning-carrying, then humans are designed to interpret them as intentional, and accommodate that,â€ Goodman says.
The fact that AI is not like us, and yet seems so intelligent, is still something to marvel at. â€œWeâ€™re getting this tremendous amount of raw intelligence without it necessarily coming with an ego-viewpoint, goals, or a sense of coherent self,â€ Goodman says. â€œThat, to me, is just fascinating.â€ You Might Also Like â€¦ ğŸ“© Get the long view on tech with Steven Levy's Plaintext newsletter Watch this guy work, and youâ€™ll finally understand the TikTok era How Telegram became a terrifying weapon in the Israel-Hamas War Inside Elon Muskâ€™s first election crisis â€”a day after he â€œfreedâ€ the bird The ultra-efficient farm of the future is in the sky The best pickleball paddles for beginners and pros ğŸŒ² Our Gear team has branched out with a new guide to the best sleeping pads and fresh picks for the best coolers and binoculars Senior Writer X Topics artificial intelligence neural networks ChatGPT OpenAI Nelson C.J.
Peter Guest Andy Greenberg Steven Levy Will Knight Joel Khalili Kari McMahon David Gilbert Facebook X Pinterest YouTube Instagram Tiktok More From WIRED Subscribe Newsletters Mattresses Reviews FAQ Wired Staff Coupons Black Friday Editorial Standards Archive Contact Advertise Contact Us Customer Care Jobs Press Center RSS Accessibility Help CondÃ© Nast Store Do Not Sell My Personal Info Â© 2023 CondÃ© Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights.
WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of CondÃ© Nast.
Ad Choices Select international site United States LargeChevron UK Italia JapÃ³n Czech Republic & Slovakia
