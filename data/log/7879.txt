Open Navigation Menu To revist this article, visit My Profile, then View saved stories.
Close Alert Backchannel Business Culture Gear Ideas Science Security Merch To revist this article, visit My Profile, then View saved stories.
Close Alert Search Backchannel Business Culture Gear Ideas Science Security Merch Podcasts Video Artificial Intelligence Climate Games Newsletters Magazine Events Wired Insider Jobs Coupons WIRED Staff Science Google's Artificial Brain Learns to Find Cat Videos Save this story Save Save this story Save By Liat Clark, Wired UK When computer scientists at Google's mysterious X lab built a neural network of 16,000 computer processors with one billion connections and let it browse YouTube, it did what many web users might do -- it began to look for cats.
[partner id="wireduk"] The "brain" simulation was exposed to 10 million randomly selected YouTube video thumbnails over the course of three days and, after being presented with a list of 20,000 different items, it began to recognize pictures of cats using a "deep learning" algorithm. This was despite being fed no information on distinguishing features that might help identify one.
Picking up on the most commonly occurring images featured on YouTube, the system achieved 81.7 percent accuracy in detecting human faces, 76.7 percent accuracy when identifying human body parts and 74.8 percent accuracy when identifying cats.
"Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not," the team says in its paper, Building high-level features using large scale unsupervised learning , which it will present at the International Conference on Machine Learning in Edinburgh, 26 June-1 July.
"The network is sensitive to high-level concepts such as cat faces and human bodies. Starting with these learned features, we trained it to obtain 15.8 percent accuracy in recognizing 20,000 object categories, a leap of 70 percent relative improvement over the previous state-of-the-art [networks]." The findings -- which could be useful in the development of speech and image recognition software, including translation services -- are remarkably similar to the "grandmother cell" theory that says certain human neurons are programmed to identify objects considered significant. The "grandmother" neuron is a hypothetical neuron that activates every time it experiences a significant sound or sight. The concept would explain how we learn to discriminate between and identify objects and words. It is the process of learning through repetition.
"We never told it during the training, 'This is a cat,'" Jeff Dean, the Google fellow who led the study, told the New York Times.
 "It basically invented the concept of a cat." "The idea is that instead of having teams of researchers trying to find out how to find edges, you instead throw a ton of data at the algorithm and you let the data speak and have the software automatically learn from the data," added Andrew Ng, a computer scientist at Stanford University involved in the project. Ng has been developing algorithms for learning audio and visual data for several years at Stanford.
Science SpaceX’s Starship Lost Shortly After Launch of Second Test Flight Ramin Skibba Gear The Best Early Black Friday Deals We've Found Nena Farrell Gear The Best Black Friday Tech Deals Nena Farrell Business Sweden’s Tesla Blockade Is Spreading Morgan Meaker Since coming out to the public in 2011, the secretive Google X lab -- thought to be located in the California Bay Area -- has released research on the Internet of Things , a space elevator and autonomous driving.
Its latest venture, though not nearing the number of neurons in the human brain ( thought to be over 80 billion), is one of the world's most advanced brain simulators. In 2009, IBM developed a brain simulator that replicated one billion human brain neurons connected by ten trillion synapses.
However, Google's latest offering appears to be the first to identify objects without hints and additional information. The network continued to correctly identify these objects even when they were distorted or placed on backgrounds designed to disorientate.
"So far, most [previous] algorithms have only succeeded in learning low-level features such as 'edge' or 'blob' detectors," says the paper.
Ng remains skeptical and says he does not believe they are yet to hit on the perfect algorithm.
Nevertheless, Google considers it such an advance that the research has made the giant leap from the X lab to its main labs.
Image: peasap /Flickr Source: Wired.co.uk Topics artificial intelligence cats computer science Google neural networks Wired UK Ramin Skibba Emily Mullin Matt Simon Matt Simon Rhett Allain Rhett Allain Emily Mullin Emily Mullin Facebook X Pinterest YouTube Instagram Tiktok More From WIRED Subscribe Newsletters Mattresses Reviews FAQ Wired Staff Coupons Black Friday Editorial Standards Archive Contact Advertise Contact Us Customer Care Jobs Press Center RSS Accessibility Help Condé Nast Store Do Not Sell My Personal Info © 2023 Condé Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights.
WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast.
Ad Choices Select international site United States LargeChevron UK Italia Japón Czech Republic & Slovakia
