Artificial Intelligence View All AI, ML and Deep Learning Auto ML Data Labelling Synthetic Data Conversational AI NLP Text-to-Speech Security View All Data Security and Privacy Network Security and Privacy Software Security Computer Hardware Security Cloud and Data Storage Security Data Infrastructure View All Data Science Data Management Data Storage and Cloud Big Data and Analytics Data Networks Automation View All Industrial Automation Business Process Automation Development Automation Robotic Process Automation Test Automation Enterprise Analytics View All Business Intelligence Disaster Recovery Business Continuity Statistical Analysis Predictive Analysis More Data Decision Makers Virtual Communication Team Collaboration UCaaS Virtual Reality Collaboration Virtual Employee Experience Programming & Development Product Development Application Development Test Management Development Languages PyTorch 1.3 comes with speed gains from quantization and TPU support Share on Facebook Share on X Share on LinkedIn Are you ready to bring more awareness to your brand? Consider becoming a sponsor for The AI Impact Tour. Learn more about the opportunities here.
Facebook today released the latest version of its deep learning library PyTorch with quantization and Google Cloud TPU support for speedier training of machine learning models.
Tensor processing unit support begins with the ability to train a model with a single chip and will later be extended to Cloud Pods, Facebook CTO Mike Schroepfer said today.
Also new today are PyTorch Mobile for deployment of ML on edge devices starting with Android and iOS devices ; CryptTen, a tool for encrypted machine learning; and Captum, a tool for explainability of machine learning models.
The news is being announced at the PyTorch Developer Conference today at The Midway in San Francisco.
VB Event The AI Impact Tour Connect with the enterprise AI community at VentureBeat’s AI Impact Tour coming to a city near you! Available today, PyTorch 1.3 comes with the ability to quantize a model for inference on to either server or mobile devices. Quantization is a way to perform computation at reduced precision.
The latest version of PyTorch will support eager mode quantization at 8-bit integer with the eager mode Python API and will allow for post-training quantization in a variety of approaches like dynamic quantization and quantization-aware training.
This spring, Google’s TensorFlow Lite 1.0 also introduced quantization.
Version 1.3 also comes with named tensors, a way to write cleaner code, PyTorch project manager Joe Spisak told VentureBeat in a phone interview.
“This is really to allow you to write cleaner code,” he said. “I’m actually able to embed those into the code instead of having a comment that says, hey, this number is the height, this number is the width etc., so it makes for more I would say readable and cleaner code and more maintainable code.” Today’s releases join other advances in federated learning this year, such as TensorFlow Federated and TensorFlow Privacy , announced this March at the TensorFlow Dev Summit.
Facebook has been working on privacy-preserving models with Google’s DeepMind research scientist Andrew Trask, who this spring launched a Udacity course to teach developers how to use things like federated learning and PySyft, an open source project with a library for encrypted deep learning with extensions of PyTorch, TensorFlow, and Keras.
Facebook also introduced two new open source frameworks: Detectron2, a new version of the Detectron object detect system, as well as speech recognition extensions typically used for translation.
PyTorch 1.1 was released this spring at the F8 developer conference with support for TensorBoard.
Over the past two years, Facebook has moved away from using its predecessor Torch or Caffe2 in an effort to make PyTorch the main tool for deep learning, CTO Mike Schroepfer said at the start of the conference. This has been essential to keeping researchers and developers on the same page.
“This means it’s now the de facto tool for doing machine learning [and] deep learning at Facebook, not only for research that happens organically, but in production, so the vast majority of our models are now trained on PyTorch,” Schroepfer said onstage. “And this is true across multiple domains both in computer vision, NLP, speech, translation — all these systems are now using PyTorch.” Tesla, Microsoft, Uber, and other large companies have also adopted PyTorch, he said.
The PyTorch deep learning library has considerably grown in popularity among AI practitioners in the past year. Nearly 1,200 developers now contribute to the open source project, and an O’Reilly analysis released in June found that arXiv mentions of PyTorch went up 194% from January to June, and are now on par with TensorFlow mentions.
VentureBeat's mission is to be a digital town square for technical decision-makers to gain knowledge about transformative enterprise technology and transact.
Discover our Briefings.
The AI Impact Tour Join us for an evening full of networking and insights at VentureBeat's AI Impact Tour, coming to San Francisco, New York, and Los Angeles! VentureBeat Homepage Follow us on Facebook Follow us on X Follow us on LinkedIn Follow us on RSS Press Releases Contact Us Advertise Share a News Tip Contribute to DataDecisionMakers Careers Privacy Policy Terms of Service Do Not Sell My Personal Information © 2023 VentureBeat.
 All rights reserved.
