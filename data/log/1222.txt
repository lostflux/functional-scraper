Open Navigation Menu To revist this article, visit My Profile, then View saved stories.
Close Alert Backchannel Business Culture Gear Ideas Science Security Merch To revist this article, visit My Profile, then View saved stories.
Close Alert Search Backchannel Business Culture Gear Ideas Science Security Merch Podcasts Video Artificial Intelligence Climate Games Newsletters Magazine Events Wired Insider Jobs Coupons Zachary Fryer-Biggs Backchannel Inside the Pentagonâ€™s Plan to Win Over Silicon Valley's AI Experts Play/Pause Button Pause Elena Lacey; Getty Images Save this story Save Save this story Save The American military is desperately trying to get a leg up in the field of artificial intelligence, which top officials are convinced will deliver victory in future warfare. But internal Pentagon documents and interviews with senior officials make clear that the Defense Department is reeling from being spurned by a tech giant and struggling to develop a plan that might work in a new sort of battleâ€”for hearts and minds in Silicon Valley.
The battle began with an unexpected loss. In June, Google announced it was pulling out of a Pentagon programâ€”the much-discussed Project Maven â€”that used the tech giantâ€™s artificial intelligence software. Thousands of the companyâ€™s employees had signed a petition two months earlier calling for an end to its work on the project, an effort to create algorithms that could help intelligence analysts pick out military targets from video footage.
Inside the Pentagon, Googleâ€™s withdrawal brought a combination of frustration and distressâ€”even angerâ€”that has percolated ever since, according to five sources familiar with internal discussions on Maven, the militaryâ€™s first big effort to utilize AI in warfare.
This article was produced in partnership with the Center for Public Integrity , a nonprofit, nonpartisan news organization.
â€œWe have stumbled unprepared into a contest over the strategic narrative,â€ said an internal Pentagon memo circulated to roughly 50 defense officials on June 28. The memo depicted a department caught flat-footed and newly at risk of alienating experts critical to the militaryâ€™s artificial intelligence development plans.
â€œWe will not compete effectively against our adversaries if we do not win the â€˜hearts and mindsâ€™ of the key supporters,â€ it warned.
Maven was actually far from complete and cost only about $70 million in 2017, a molecule of water in the Pentagonâ€™s oceanic $600 billion budget that year. But Googleâ€™s announcement exemplified a larger public relations and scientific challenge the department is still wrestling with. It has responded so far by trying to create a new public image for its AI work and by seeking a review of the departmentâ€™s AI policy by an advisory board of top executives from tech companies.
The reason for the Pentagonâ€™s anxiety is clear: It wants a smooth path to use artificial intelligence in weaponry of the future, a desire already backed by the promise of several billion dollars to try to ensure such systems are trusted and accepted by military commanders, plus billions more in expenditures on the technologies themselves.
The exact role that AI will wind up playing in warfare remains unclear. Many weapons with AI will not involve decision-making by machine algorithms, but the potential for them to do so will exist. As a Pentagon strategy document said in August: â€œTechnologies underpinning unmanned systems would make it possible to develop and deploy autonomous systems that could independently select and attack targets with lethal force.â€ Business What Sam Altmanâ€™s Firing Means for the Future of OpenAI Steven Levy Business Sam Altmanâ€™s Sudden Exit Sends Shockwaves Through OpenAI and Beyond Will Knight Gear Humanityâ€™s Most Obnoxious Vehicle Gets an Electric (and Nearly Silent) Makeover Boone Ashworth Security The Startup That Transformed the Hack-for-Hire Industry Andy Greenberg Developing artificial intelligence, officials say, is unlike creating other military technologies. While the military can easily turn to big defense contractors for cutting-edge work on fighter jets and bombs, the heart of innovation in AI and machine learning resides among the non-defense tech giants of Silicon Valley. Without their help, officials worry, they could lose an escalating global arms race in which AI will play an increasingly important role, something top officials say they are unwilling to accept.
â€œIf you decide not to work on Maven, youâ€™re not actually having a discussion on if artificial intelligence or machine learning are going to be used for military operations,â€ Chris Lynch, a former tech entrepreneur who now runs the Pentagonâ€™s Defense Digital Service, said in an interview. AI is coming to warfare, he says, so the question is, which American technologists are going to engineer it? Lynch, who recruits technical experts to spend several years working on Pentagon problems before returning to the private sector, said that AI technology is too important, and that the agency will proceed even if it has to rely on lesser experts. But without the help of the industryâ€™s best minds, Lynch added, â€œweâ€™re going to pay somebody who is far less capable to go build a far less capable product that may put young men and women in dangerous positions, and there may be mistakes because of it.â€ Google isnâ€™t likely to shift gears soon. Less than a week after announcing that the company would not seek to renew the Maven contract in June, Google released a set of AI principles which specified that the company would not use AI for â€œweapons or other technologies whose principal purpose or implementation is to cause or directly facilitate injury to people.â€ Some defense officials have complained since then that Google was being unpatriotic , noting that the company was still pursuing work with the Chinese government, the top US competitor in artificial intelligence technology.
â€œI have a hard time with companies that are working very hard to engage in the market inside of China, and engaging in projects where intellectual property is shared with the Chinese, which is synonymous with sharing it with the Chinese military, and then don't want to work for the US military,â€ General Joe Dunford, chairman of the Joint Chiefs of Staff, commented while speaking at a conference in November.
Business What Sam Altmanâ€™s Firing Means for the Future of OpenAI Steven Levy Business Sam Altmanâ€™s Sudden Exit Sends Shockwaves Through OpenAI and Beyond Will Knight Gear Humanityâ€™s Most Obnoxious Vehicle Gets an Electric (and Nearly Silent) Makeover Boone Ashworth Security The Startup That Transformed the Hack-for-Hire Industry Andy Greenberg In December testimony before congress, Google CEO Sundar Pichai acknowledged that Google had experimented with a program involving China, Project Dragonfly , aimed at developing a model of what government-censored search results would look like in China. However, Pichai testified that Google currently â€œhas no plans to launch in China.â€ Project Mavenâ€™s aim was to simplify work for intelligence analysts by tagging object types in video footage from drones and other platforms, helping analysts gather information and narrow their focus on potential targets, according to sources familiar with the partly classified program. But the algorithms did not select the targets or order strikes, a longtime fear of those worried about the intersection of advanced computing and new forms of lethal violence.
Many at Google nonetheless saw the program in alarming terms.
â€œThey immediately heard drones and then they thought machine learning and automatic target recognition, and I think it escalated for them pretty quickly about enabling targeted killing, enabling targeted warfare,â€ said a former Google employee familiar with the internal discussions.
Google is just one of the tech giants that the Pentagon has sought to enlist in its effort to inject AI into modern warfare technology. Among the others: Microsoft and Amazon. After Googleâ€™s announcement in June more than a dozen large defense firms approached defense officials, offering to take over the work, according to current and former Pentagon officials.
But Silicon Valley activists also say the industry cannot easily ignore the ethical qualms of tech workers. â€œThereâ€™s a division between those who answer to shareholders, who want to get access to Defense Department contracts worth multimillions of dollars, and the rank and file who have to build the things and who feel morally complicit for things they donâ€™t agree with,â€ the former Google employee said.
In an effort to bridge this gulf and dampen hard-edged opposition from AI engineers, the Defense Department has so far undertaken two initiatives.
The first, formally begun in late June, was to create a Joint Artificial Intelligence Center meant to oversee and manage all of the militaryâ€™s AI efforts, with an initial focus on PR-friendly humanitarian missions. Itâ€™s set to be run by Lieutenant General Jack Shanahan, whose last major assignment was running Project Maven. In a politically shrewd decision, its first major initiative is to figure out a way to use AI to help organize the militaryâ€™s search and rescue response to natural disasters.
Business What Sam Altmanâ€™s Firing Means for the Future of OpenAI Steven Levy Business Sam Altmanâ€™s Sudden Exit Sends Shockwaves Through OpenAI and Beyond Will Knight Gear Humanityâ€™s Most Obnoxious Vehicle Gets an Electric (and Nearly Silent) Makeover Boone Ashworth Security The Startup That Transformed the Hack-for-Hire Industry Andy Greenberg â€œOur goal is to save lives,â€ Brendan McCord, one of the chief architects of the Pentagonâ€™s AI strategy, said while speaking at a technical conference in October. â€œOur militaryâ€™s fundamental role, its mission, is to keep the peace. It is to deter war and protect our country. It is to improve global stability, and itâ€™s to ultimately protect the set of values that came out of the Enlightenment.â€ The second initiative is to order a new review of AI ethics by an advisory panel of tech experts, the Defense Innovation Board, which includes former Google CEO Eric Schmidt and LinkedIn cofounder Reid Hoffman.
That review, designed to develop principles for the use of AI by the military, is being managed by Joshua Marcuse, a former adviser to the secretary of defense on innovation issues who is now executive director of the board. Set to take about nine months, the advisory panel will hold public meetings with AI experts, while an internal Pentagon group also considers questions. Then it will forward recommendations to secretary of defense James Mattis about the ways that AI should or should not be injected into weapons programs.
â€œThis has got to be about actually looking in the mirror and being willing to impose some constraints on what we will do, on what we wonâ€™t do, knowing what the boundaries are,â€ Marcuse said in an interview.
To make sure the debate is robust, Marcuse said that the board is seeking out critics of the militaryâ€™s role in AI.
â€œThey have a set of concerns, I think really valid and legitimate concerns, about how the Department of Defense is going to apply these technologies, because we have legal authority to invade peopleâ€™s privacy in certain circumstances, we have legal authority to commit violence, we have legal authority to wage war,â€ he said.
Resolving those concerns is critical, officials say, because of the difference in how Washington and Beijing manage AI talent. China can conscript experts to work on military problems, whereas the United States has to find a way to interest and attract outside experts.
â€œThey have to choose to work with us, so we need to offer them a meaningful, verifiable commitment that there are real opportunities to work with us where they can feel confident that theyâ€™re the good guys,â€ Marcuse said.
Business What Sam Altmanâ€™s Firing Means for the Future of OpenAI Steven Levy Business Sam Altmanâ€™s Sudden Exit Sends Shockwaves Through OpenAI and Beyond Will Knight Gear Humanityâ€™s Most Obnoxious Vehicle Gets an Electric (and Nearly Silent) Makeover Boone Ashworth Security The Startup That Transformed the Hack-for-Hire Industry Andy Greenberg Despite his willingness to discuss potential future constraints on AI usage, Marcuse said he didnâ€™t think the board would try to change the Pentagonâ€™s existing policy on autonomous weapons that depend on AI, which was put in place by the Obama administration in 2012.
That policy, which underwent a minor technical revision by the Trump administration in May 2017, doesnâ€™t prevent the military from using artificial intelligence in any of its weapons systems. It mandates that commanders have â€œappropriate levels of human judgmentâ€ over any AI-infused weapons systems, although the phrase isnâ€™t further defined and remains a source of confusion within the Pentagon, according to multiple officials there.
It does, however, require that before a computer could be programmed to initiate deadly action, the weapons system that contains it must undergo special review by three senior Pentagon officialsâ€”in advance of its purchase. To date that special review hasnâ€™t been undertaken.
In late 2016, during the waning days of the Obama administration, the Pentagon took a new look at the 2012 policy and decided in a classified report that no major change was needed, according to a former defense official familiar with the details. â€œThere was nothing that was held up, there was no one who thought, â€˜Oh we have to update the directives,â€™â€ the former official said.
The Trump administration nonetheless has internally discussed making it clearer to weapons engineers within the militaryâ€”who it fears have been reluctant to inject AI into their designsâ€”that the policy doesnâ€™t ban the use of autonomy in weapons systems. The contretemps in Silicon Valley over Project Maven at least temporarily halted that discussion, prompting the departmentâ€™s leaders to try first to win the support of the Defense Innovation Board.
But one way or another, the Pentagon intends to integrate more AI into its weaponry. â€œWeâ€™re not going to sit on the sidelines as a new technology revolutionizes the battlefield,â€ Marcuse said. â€œItâ€™s not fair to the American people, itâ€™s not fair to our service members who we send into harmâ€™s way, and itâ€™s not fair to our allies who depend on us.â€ The Center for Public Integrity is a nonprofit, nonpartisan, investigative newsroom in Washington, DC. More of its national security reporting can be found here.
Alexa grew up this year, mostly because we talked to it 8 sci-fi writers imagine the bold and new future of work The mad scramble for the world's most coveted meteorite Galileo, krypton, and how the true meter came to be Everything you want to know about the promise of 5G ğŸ‘€ Looking for the latest gadgets? Check out our picks , gift guides , and best deals all year round ğŸ“© Get even more of our inside scoops with our weekly Backchannel newsletter Samanth Subramanian Steven Levy Christopher Beam Virginia Heffernan Vauhini Vara Lexi Pandell Amit Katwala Gideon Lichfield Facebook X Pinterest YouTube Instagram Tiktok More From WIRED Subscribe Newsletters Mattresses Reviews FAQ Wired Staff Coupons Black Friday Editorial Standards Archive Contact Advertise Contact Us Customer Care Jobs Press Center RSS Accessibility Help CondÃ© Nast Store Do Not Sell My Personal Info Â© 2023 CondÃ© Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights.
WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of CondÃ© Nast.
Ad Choices Select international site United States LargeChevron UK Italia JapÃ³n Czech Republic & Slovakia
