Open Navigation Menu To revist this article, visit My Profile, then View saved stories.
Close Alert Backchannel Business Culture Gear Ideas Science Security Merch To revist this article, visit My Profile, then View saved stories.
Close Alert Search Backchannel Business Culture Gear Ideas Science Security Merch Podcasts Video Artificial Intelligence Climate Games Newsletters Magazine Events Wired Insider Jobs Coupons Sidney Fussell Business An Algorithm That ‚ÄòPredicts‚Äô Criminality Based on a Face Sparks a Furor A since-deleted press release claimed an algorithm could predict whether someone would become a criminal by analyzing their face.
Photograph: OCTAVIO ICONS/Alamy Save this story Save Save this story Save Application Ethics Face recognition Prediction End User Government Sector Public safety Technology Machine vision In early May, a press release from Harrisburg University claimed that two professors and a graduate student had developed a facial-recognition program that could predict whether someone would be a criminal. The release said the paper would be published in a collection by Springer Nature, a big academic publisher.
With ‚Äú80 percent accuracy and with no racial bias,‚Äù the paper, A Deep Neural Network Model to Predict Criminality Using Image Processing , claimed its algorithm could predict ‚Äúif someone is a criminal based solely on a picture of their face.‚Äù The press release has since been deleted from the university website.
Tuesday, more than 1,000 machine-learning researchers, sociologists, historians, and ethicists released a public letter condemning the paper, and Springer Nature confirmed on Twitter it will not publish the research.
But the researchers say the problem doesn't stop there. Signers of the letter, collectively calling themselves the Coalition for Critical Technology (CCT), said the paper‚Äôs claims ‚Äúare based on unsound scientific premises, research, and methods which ‚Ä¶ have [been] debunked over the years.‚Äù The letter argues it is impossible to predict criminality without racial bias, ‚Äúbecause the category of ‚Äòcriminality‚Äô itself is racially biased.‚Äù Advances in data science and machine learning have led to numerous algorithms in recent years that purport to predict crimes or criminality. But if the data used to build those algorithms is biased, the algorithms‚Äô predictions will also be biased. Because of the racially skewed nature of policing in the US, the letter argues, any predictive algorithm modeling criminality will only reproduce the biases already reflected in the criminal justice system.
Mapping these biases onto facial analysis recalls the abhorrent ‚Äúrace science‚Äù of prior centuries, which purported to use technology to identify differences between the races‚Äîin measurements such as head size or nose width‚Äîas proof of their innate intellect, virtue, or criminality.
Race science was debunked long ago, but papers that use machine learning to ‚Äúpredict‚Äù innate attributes or offer diagnoses are making a subtle, but alarming return.
In 2016 researchers from Shanghai Jiao Tong University claimed their algorithm could predict criminality using facial analysis. Engineers from Stanford and Google refuted the paper‚Äôs claims, calling the approach a new ‚Äúphysiognomy,‚Äù a debunked race science popular among eugenists, which infers personality attributes from the shape of someone‚Äôs head.
Business What Sam Altman‚Äôs Firing Means for the Future of OpenAI Steven Levy Business Sam Altman‚Äôs Sudden Exit Sends Shockwaves Through OpenAI and Beyond Will Knight Gear Humanity‚Äôs Most Obnoxious Vehicle Gets an Electric (and Nearly Silent) Makeover Boone Ashworth Security The Startup That Transformed the Hack-for-Hire Industry Andy Greenberg In 2017 a pair of Stanford researchers claimed their artificial intelligence could tell if someone is gay or straight based on their face. LGBTQ organizations lambasted the study, noting how harmful the notion of automated sexuality identification could be in countries that criminalize homosexuality. Last year, researchers at Keele University in England claimed their algorithm trained on YouTube videos of children could predict autism. Earlier this year, a paper in the Journal of Big Data not only attempted to ‚Äúinfer personality traits from facial images,‚Äù but cited Cesare Lombroso, the 19th-century scientist who championed the notion that criminality was inherited.
Each of those papers sparked a backlash, though none led to new products or medical tools. The authors of the Harrisburg paper, however, claimed their algorithm was specifically designed for use by law enforcement.
‚ÄúCrime is one of the most prominent issues in modern society,‚Äù said Jonathan W. Korn, a PhD student at Harrisburg and former New York police officer, in a quote from the deleted press release. ‚ÄúThe development of machines that are capable of performing cognitive tasks, such as identifying the criminality of [a] person from their facial image, will enable a significant advantage for law enforcement agencies and other intelligence agencies to prevent crime from occurring in their designated areas.‚Äù Korn didn‚Äôt respond to a request for comment. Nathaniel Ashby, one of the paper‚Äôs coauthors, declined to comment.
Springer Nature did not respond to a request for comment before this article was initially published. In a statement after the article was initially published, Springer said, ‚ÄúWe acknowledge the concern regarding this paper and would like to clarify at no time was this accepted for publication. It was submitted to a forthcoming conference for which Springer will publish the proceedings of in the book series Transactions on Computational Science and Computational Intelligence and went through a thorough peer review process. The series editor‚Äôs decision to reject the final paper was made on Tuesday 16th June and was officially communicated to the authors on Monday 22nd June. The details of the review process and conclusions drawn remain confidential between the editor, peer reviewers and authors.‚Äù Civil liberties groups have long warned against law enforcement use of facial recognition. The software is less accurate on darker-skinned people than lighter-skinned people, according to a report from AI researchers Timnit Gebru and Joy Buolamwini, both of whom signed the CCT letter.
Business What Sam Altman‚Äôs Firing Means for the Future of OpenAI Steven Levy Business Sam Altman‚Äôs Sudden Exit Sends Shockwaves Through OpenAI and Beyond Will Knight Gear Humanity‚Äôs Most Obnoxious Vehicle Gets an Electric (and Nearly Silent) Makeover Boone Ashworth Security The Startup That Transformed the Hack-for-Hire Industry Andy Greenberg In 2018, the ACLU found that Amazon‚Äôs facial-recognition product, Rekognition, misidentified members of Congress as criminals, erring more frequently on black officials than white ones. Amazon recently announced a one-year moratorium on selling the product to police.
The Harrisburg paper has seemingly never been publicly posted, but publishing problematic research alone can be dangerous. Last year Berlin-based security researcher Adam Harvey found that facial-recognition data sets from American universities were used by surveillance firms linked to the Chinese government. Because AI research created for one purpose can be used for another, papers require intense ethical scrutiny even if they don‚Äôt directly lead to new products or methods.
‚ÄúLike computers or the internal combustion engine, AI is a general-purpose technology that can be used to automate a great many tasks, including ones that should not be undertaken in the first place,‚Äù the letter reads.
Updated, 6-24-20, 1:30pm ET: This article has been updated to include a statement from Springer Nature.
The Last of Us Part II and its crisis-strewn path to release Former eBay execs allegedly made life hell for critics The best sex tech and toys for every body AI, AR, and the (somewhat) speculative future of a tech-fueled FBI Facebook groups are destroying America üëÅ What is intelligence, anyway ? Plus: Get the latest AI news ‚ú® Optimize your home life with our Gear team‚Äôs best picks, from robot vacuums to affordable mattresses to smart speakers Senior Writer X Topics face recognition artificial intelligence algorithms machine learning ethics Steven Levy Nelson C.J.
Peter Guest Andy Greenberg Joel Khalili David Gilbert Kari McMahon Jacopo Prisco Facebook X Pinterest YouTube Instagram Tiktok More From WIRED Subscribe Newsletters Mattresses Reviews FAQ Wired Staff Coupons Black Friday Editorial Standards Archive Contact Advertise Contact Us Customer Care Jobs Press Center RSS Accessibility Help Cond√© Nast Store Do Not Sell My Personal Info ¬© 2023 Cond√© Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights.
WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Cond√© Nast.
Ad Choices Select international site United States LargeChevron UK Italia Jap√≥n Czech Republic & Slovakia
