Open Navigation Menu To revist this article, visit My Profile, then View saved stories.
Close Alert Backchannel Business Culture Gear Ideas Science Security Merch To revist this article, visit My Profile, then View saved stories.
Close Alert Search Backchannel Business Culture Gear Ideas Science Security Merch Podcasts Video Artificial Intelligence Climate Games Newsletters Magazine Events Wired Insider Jobs Coupons Tom Simonite Business Are These the Hidden Deepfakes in the Anthony Bourdain Movie? Morgan Neville, director of the movie Roadrunner , said he used software in several places to mimic the voice of celebrity chef Anthony Bourdain, who died three years ago.
Photograph: BFA/Alamy Save this story Save Save this story Save Application Deepfakes End User Small company Sector Entertainment Source Data Speech Technology Machine learning Neural Network When Roadrunner, a documentary about late TV chef and traveler Anthony Bourdain, opened in theaters last month, its director, Morgan Neville, spiced up promotional interviews with an unconventional disclosure for a documentarian. Some words viewers hear Bourdain speak in the film were faked by artificial intelligence software used to mimic the star‚Äôs voice.
Accusations from Bourdain fans that Neville had acted unethically quickly came to dominate coverage of the film. Despite that attention, how much of the fake Bourdain‚Äôs voice is in the two-hour movie, and what it said, has been unclear‚Äîuntil now.
In an interview that made his film infamous, Neville told The New Yorker that he had generated three fake Bourdain clips with the permission of his estate, all from words the chef had written or said but that were not available as audio. He revealed only one, an email Bourdain ‚Äúreads‚Äù in the film‚Äôs trailer , but boasted that the other two clips would be undetectable. ‚ÄúIf you watch the film,‚Äù The New Yorker quoted the Oscar-winning Neville saying, ‚Äúyou probably don‚Äôt know what the other lines are that were spoken by the AI, and you‚Äôre not going to know.‚Äù Audio experts at Pindrop, a startup that helps banks and others fight phone fraud, think they do know. If the company‚Äôs analysis is correct, the deepfake Bourdain controversy is rooted in less than 50 seconds of audio in the 118-minute film.
Pindrop‚Äôs analysis flagged the email quote disclosed by Neville and also a clip early in the film apparently drawn from an essay Bourdain wrote about Vietnam titled ‚ÄúThe Hungry American,‚Äù collected in his 2008 book, The Nasty Bits.
 It also highlighted audio midway through the film in which the chef observes that many chefs and writers have a ‚Äúrelentless instinct to fuck up a good thing.‚Äù The same sentences appear in an interview of Bourdain with food site First We Feast on the occasion of his 60th birthday in 2016, two years to the month before he died by suicide.
All three clips sound recognizably like Bourdain. On close listening, though, they appear to bear signatures of synthetic speech, such as odd prosody and fricatives such as ‚Äús‚Äù and ‚Äúf‚Äù sounds. One Reddit user independently flagged the same three clips as Pindrop, writing that they were easy to hear on watching the film for a second time. The film‚Äôs distributor, Focus Features, did not respond to requests for comment; Neville‚Äôs production company declined to comment.
The director of Roadrunner said this clip of the chef musing on happiness was synthesized using AI software.
Audio source: Pindrop When Neville predicted that his use of AI-generated media, sometimes termed deepfakes , would be undetectable, he may have overestimated the sophistication of his own fakery. He likely did not anticipate the controversy or attention his use of the technique would draw from fans and audio experts. When the furor reached the ears of researchers at Pindrop, they saw the perfect test case for software they built to detect audio deepfakes; they set it to work when the movie debuted on streaming services earlier this month. ‚ÄúWe‚Äôre always looking for ways to test our systems, especially in real real conditions‚Äîthis was a new way to validate our technology,‚Äù says Collin Davis, Pindrop‚Äôs chief technology officer.
Pindrop‚Äôs results may have resolved the mystery of Neville‚Äôs missing deepfakes, but the episode portends future controversies as deepfakes become more sophisticated and accessible for both creative and malicious projects.
Culture The Future of Game Accessibility Is Surprisingly Simple Geoffrey Bunting Science SpaceX‚Äôs Starship Lost Shortly After Launch of Second Test Flight Ramin Skibba Business Elon Musk May Have Just Signed X‚Äôs Death Warrant Vittoria Elliott Business OpenAI Ousts CEO Sam Altman Will Knight Deepfake technology has become more convincing and easier to access in recent years. Some people have been victimized by pornographic deepfakes used for titillation or harassment. But very few in society have been directly touched, or deceived, by the technology. Despite fearful discussions in academia and Congress about the potential for mass deepfake deception, the threat has so far been largely hypothetical.
Neville‚Äôs project made deepfakes very real to Bourdain fans. Millions feel a personal connection with the chef, who could make raw authenticity crackle off the screen. The fake clips were a pointed reminder that those relationships were always filtered through technology and by media professionals like Neville. ‚ÄúIf you learn that the technology you thought was enabling this authentic relationship is actually undermining it, that creates a crisis,‚Äù says William Little, a media studies professor at University of Virginia. He teaches a class on AI and film and will be adding Roadrunner to the syllabus as a case study in some questions raised by the technology.
Analysts at a fraud-detection startup believe this clip of Bourdain may have been synthesized using AI software.
Audio source: Pindrop Neville, who never met Bourdain, told GQ that he turned to deepfake audio because he wanted to draw on the star‚Äôs thoughts that weren‚Äôt available on tape. ‚ÄúI wasn‚Äôt putting words into his mouth. I was just trying to make them come alive,‚Äù he said. It‚Äôs possible he also saw the technology as a way to win publicity for the film.
Deepfaking the subject of this particular film even has a certain logic: Roadrunner is about Bourdain‚Äôs different identities and the conflicting feelings they evoked in those around him and the star himself. Was Bourdain the unvarnished but goodhearted hero viewers came to love, or the ‚Äúpain in the ass‚Äù friends say he could be off camera? An empathic explorer or just another white guy parachuting into foreign locales with a camera crew? And why did he struggle to be happy? Culture The Future of Game Accessibility Is Surprisingly Simple Geoffrey Bunting Science SpaceX‚Äôs Starship Lost Shortly After Launch of Second Test Flight Ramin Skibba Business Elon Musk May Have Just Signed X‚Äôs Death Warrant Vittoria Elliott Business OpenAI Ousts CEO Sam Altman Will Knight Neville‚Äôs use of deepfakes in pursuing those questions is in some ways not hugely different from more established and accepted documentary techniques that also have a degree of artifice. Some used in Roadrunner may have seemed deceptive in earlier times.
Neville has Bourdain narrate the film of his own life from beyond the grave in a tapestry of audio drawn from TV shows, audiobooks, radio, and podcasts. The deepfakes provide just a few tiny threads. And the film uses conventional sleights of editing that combine audio and video from different times and places in sometimes reality-bending ways. In one scene, a business associate of Bourdain's recounts a notable phone call, against early footage of the star talking on a flip phone. Did that clip from the archive capture his side of that same call? Likely not, but the illusion helps tell the story.
More than a century since the first motion pictures, audiences are used to such tricks. Media industry and audience expectations for deepfakes are still a work in progress. ‚ÄúThis is something everyone is grappling with,‚Äù says Sam Gregory, who works on deepfakes policy at the nonprofit Witness and often talks with media producers and tech companies about disclosure. ‚ÄúPeople generally coalesce around the idea that you need to have some way to signify to consumers or viewers that there is some manipulation.‚Äù The analysts believe this clip of the star talking about Vietnam may have been synthesized using AI software. Audio source: Pindrop Some directors have tried. In the 2020 documentary Welcome to Chechnya , about LGBTQ activists fleeing persecution, some subjects are digitally masked with synthetic faces that mimic their facial movements. The film‚Äôs producers intentionally stopped short of spoofing reality too closely, giving their digital masks an eerie blurriness they call a halo as a form of disclosure.
Audio provides less scope for such signals but it is still possible to inform listeners about the source of what they‚Äôre hearing. At one point in Roadrunner, a caption advises viewers they are hearing ‚ÄúVOICE OVER - OUTTAKE.‚Äù It‚Äôs not clear why Neville didn‚Äôt use a ‚Äúsynthetic audio‚Äù caption for his AI-generated clips‚Äîor if disclosing them in the film, not just interviews in which he boasted they were undetectable, would have softened the backlash.
Culture The Future of Game Accessibility Is Surprisingly Simple Geoffrey Bunting Science SpaceX‚Äôs Starship Lost Shortly After Launch of Second Test Flight Ramin Skibba Business Elon Musk May Have Just Signed X‚Äôs Death Warrant Vittoria Elliott Business OpenAI Ousts CEO Sam Altman Will Knight Pindrop‚Äôs contribution to the Roadrunner controversy illustrates how deepfake detectors can help uncover deception but also that such technology is no panacea.
To scan for fake Bourdain, the company processed the film‚Äôs soundtrack to remove noise and to make speech more prominent, then ran the segments containing speech through a deepfakes detector based on machine learning that looks for signatures of synthetic voices. Elie Khoury, Pindrop‚Äôs director of research, says some of those artifacts can be perceived by the human ear, but others require technological help.
Pindrop‚Äôs system gave every four-second segment of speech in Roadrunner a deepfake score from 1 to 100; the company identified the two missing synthetic clips after reviewing the 30 segments that scored highest, which also included the fake clip disclosed by Neville. The results of that process show the power but also some limitations of deepfake detection. Some segments other than the three Pindrop ultimately homed in on also scored highly on the initial scan.
Most were easily eliminated as false positives by giveaways such as that they matched visuals on screen like Bourdain‚Äôs lips moving, or drawing on standard audio forensic techniques that detected conventional sound processing, heavy music, or background noise. Davis of Pindrop says that when the company provides fraud detection in call centers, false positives can be checked by prompting a caller who triggered the system to provide extra security information. But not every example of alleged deepfake deception will allow easy verification or cross checking.
A disputed video of a politician detained in the military coup in Myanmar this year illustrates that problem. In the clip, the man claims to have given Burmese leader Aung San Suu Kyi corrupt payments in cash and gold.
His voice and face appear distorted.
 Accusations it was synthetic surged after a screenshot from an online deepfake detector declaring the clip fake with 93 percent certainty was posted to Twitter. The case is far from closed, because there is no way to confirm that claim.
Deepfake detectors are a nascent art and different systems can produce wildly divergent results.
 Deep audio and video forensic expertise is needed to interpret or check the results from such tools. ‚ÄúIf you‚Äôre not careful, putting detectors out there can make it more difficult to tell what is fake or not,‚Äù Gregory of Witness says. He still considers the Myanmar video‚Äôs authenticity unknown.
One remaining mystery about the Bourdain deepfakes suggests the controversy may still have more lessons to teach. Neville told GQ that he had deepfake Bourdains made by four different companies and chose the one that sounded best, but he has not identified any of them.
Culture The Future of Game Accessibility Is Surprisingly Simple Geoffrey Bunting Science SpaceX‚Äôs Starship Lost Shortly After Launch of Second Test Flight Ramin Skibba Business Elon Musk May Have Just Signed X‚Äôs Death Warrant Vittoria Elliott Business OpenAI Ousts CEO Sam Altman Will Knight WIRED contacted 10 companies that advertise their ability to synthesize or clone voices, from small startups to Google and Microsoft‚Äîan exercise that highlighted how the technology is now widely available. All denied working with Neville on his project. A Pindrop analysis suggested that Bourdain was likely given posthumous voice using a version of a technique first published by Google‚Äôs DeepMind AI division in 2016 that has since been integrated into Google‚Äôs virtual assistant and widely reimplemented in open source software. A spokesperson for DeepMind said the company supports the idea that ‚Äúno voices should be used without permission.‚Äù üì© The latest on tech, science, and more: Get our newsletters ! Hundreds of ways to get s#!+ done ‚Äîand we still don't Why I'll never finish Legend of Zelda: Breath of the Wild How the far right exploded on Steam and Discord Where to get discounts with your student email address Big Tech is bending to the Indian government's will üëÅÔ∏è Explore AI like never before with our new database üéÆ WIRED Games: Get the latest tips, reviews, and more ‚ú® Optimize your home life with our Gear team‚Äôs best picks, from robot vacuums to affordable mattresses to smart speakers Senior Editor X Topics artificial intelligence Deepfakes machine learning Movies David Gilbert David Gilbert Amit Katwala Kari McMahon Will Knight Joel Khalili Andy Greenberg Andy Greenberg Facebook X Pinterest YouTube Instagram Tiktok More From WIRED Subscribe Newsletters Mattresses Reviews FAQ Wired Staff Coupons Black Friday Editorial Standards Archive Contact Advertise Contact Us Customer Care Jobs Press Center RSS Accessibility Help Cond√© Nast Store Do Not Sell My Personal Info ¬© 2023 Cond√© Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights.
WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Cond√© Nast.
Ad Choices Select international site United States LargeChevron UK Italia Jap√≥n Czech Republic & Slovakia
