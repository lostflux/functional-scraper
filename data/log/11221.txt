Open Navigation Menu To revist this article, visit My Profile, then View saved stories.
Close Alert Backchannel Business Culture Gear Ideas Science Security Merch To revist this article, visit My Profile, then View saved stories.
Close Alert Search Backchannel Business Culture Gear Ideas Science Security Merch Podcasts Video Artificial Intelligence Climate Games Newsletters Magazine Events Wired Insider Jobs Coupons Tom Simonite Business Google Has a Plan to Stop Its New AI From Being Dirty and Rude Photograph: Juan Moyano/Getty Images Save this story Save Save this story Save Application Personal assistant End User Consumer Sector Consumer services Source Data Speech Technology Natural language processing Silicon Valley CEOs usually focus on the positives when announcing their company‚Äôs next big thing. In 2007, Apple‚Äôs Steve Jobs lauded the first iPhone‚Äôs ‚Äúrevolutionary user interface‚Äù and ‚Äúbreakthrough software.‚Äù Google CEO Sundar Pichai took a different tack at his company‚Äôs annual conference Wednesday when he announced a beta test of Google‚Äôs ‚Äúmost advanced conversational AI yet.‚Äù Pichai said the chatbot, known as LaMDA 2, can converse on any topic and had performed well in tests with Google employees. He announced a forthcoming app called AI Test Kitchen that will make the bot available for outsiders to try. But Pichai added a stark warning. ‚ÄúWhile we have improved safety, the model might still generate inaccurate, inappropriate or offensive responses,‚Äù he said.
Pichai‚Äôs vacillating pitch illustrates the mixture of excitement, puzzlement, and concern swirling around a string of recent breakthroughs in the capabilities of machine-learning software that processes language.
The technology has already improved the power of auto-complete and web search.
 It has also created new categories of productivity apps that help workers by generating fluent text or programming code.
 And when Pichai first disclosed the LaMDA project last year , he said it could eventually be put to work inside Google‚Äôs search engine, virtual assistant, and workplace apps. Yet despite all that dazzling promise, it‚Äôs unclear how to reliably control these new AI wordsmiths.
Google‚Äôs LaMDA, or Language Model for Dialogue Applications, is an example of what machine-learning researchers call a large language model. The term is used to describe software that builds up a statistical feeling for the patterns of language by processing huge volumes of text, usually sourced online. LaMDA, for example, was initially trained with more than a trillion words from online forums, Q&A sites, Wikipedia, and other webpages. This vast trove of data helps the algorithm perform tasks like generating text in different styles, interpreting new text, or functioning as a chatbot. And these systems, if they work, won‚Äôt be anything like the frustrating chatbots you use today. Right now Google Assistant and Amazon‚Äôs Alexa can only perform certain preprogrammed tasks, and they deflect when presented with something they don‚Äôt understand. What Google is now proposing is a computer you can actually talk to.
Chat logs released by Google show that LaMDA can‚Äîat least at times‚Äîbe informative, thought-provoking, or even funny. Testing the chatbot prompted Google vice president and AI researcher Blaise Ag√ºera y Arcas to write a personal essay last December arguing the technology could provide new insights into the nature of language and intelligence. ‚ÄúIt can be very hard to shake the idea that there‚Äôs a ‚Äòwho,‚Äô not an ‚Äòit‚Äô, on the other side of the screen,‚Äù he wrote.
Pichai made clear when he announced the first version of LaMDA last year , and again on Wednesday, that he sees it potentially providing a path to voice interfaces vastly broader than the often frustratingly limited capabilities of services like Alexa, Google Assistant, and Apple‚Äôs Siri. Now Google‚Äôs leaders appear to be convinced they may have finally found the path to creating computers you can genuinely talk with.
At the same time, large language models have proven fluent in talking dirty, nasty, and plain racist. Scraping billions of words of text from the web inevitably sweeps in a lot of unsavory content. OpenAI, the company behind language generator GPT-3 , has reported that its creation can perpetuate stereotypes about gender and race, and it asks customers to implement filters to screen out unsavory content.
Science SpaceX‚Äôs Starship Lost Shortly After Launch of Second Test Flight Ramin Skibba Business Elon Musk May Have Just Signed X‚Äôs Death Warrant Vittoria Elliott Business OpenAI Ousts CEO Sam Altman Will Knight Business Who Is Mira Murati, OpenAI‚Äôs New Interim CEO? Steven Levy LaMDA can be toxic too. But Pichai said that Google can tame the system if more people chat with it and provide feedback. Internal testing with thousands of Google employees has already reduced the LaMDA‚Äôs propensity to make inaccurate or offensive statements, he said.
Pichai presented Google‚Äôs forthcoming AI Test Kitchen app as a way for outsiders to help Google continue that sanitization project, while also testing ideas about how to turn an advanced but occasionally off-kilter chatbot into a product. Google has not said when the app will be released, or who will get access first.
The app will initially include three different experiences powered by LaMDA. ‚ÄúEach is meant to give you a sense of what it might be like to have LaMDA in your hands and use it for things you care about,‚Äù Pichai said.
One of those demos has the bot pose as an interactive storyteller, prompting a user to complete the prompt ‚ÄúImagine I‚Äôm at ‚Ä¶‚Äù It responds with a fictional description of a scene and can elaborate on it in response to follow up questions. Another is a version of LaMDA tuned to talk obsessively about dogs, in a test of Google‚Äôs ability to keep the chatbot on a specific topic.
The app‚Äôs third offering is an enhanced to-do list. In a live demo Wednesday, a Google employee tapped out ‚ÄúI want to plant a vegetable garden.‚Äù LaMDA produced a six point list of steps towards that goal. The app displayed a warning: ‚ÄúMay give inaccurate/inappropriate information.‚Äù Tapping on the list item that read ‚ÄúResearch what grows well in your area‚Äù prompted LaMDA to list substeps such as ‚ÄúSee what grows in your neighbors‚Äô yards.‚Äù Gathering feedback on how those three demos perform should help improve LaMDA, but it is unclear if it can fully tame such a system, says Percy Liang, director of Stanford‚Äôs Center for Research on Foundation Models, which was created last year to study large-scale AI systems such as LaMDA. Liang likens AI experts‚Äô existing techniques for controlling large language models to engineering with duct tape. ‚ÄúWe have this thing that‚Äôs very powerful, but when we use it we discover these gaping problems and we patch them,‚Äù Liang says. ‚ÄúMaybe if you do this enough times you‚Äôll get to something really good, or maybe there will always be holes in the system.‚Äù Science SpaceX‚Äôs Starship Lost Shortly After Launch of Second Test Flight Ramin Skibba Business Elon Musk May Have Just Signed X‚Äôs Death Warrant Vittoria Elliott Business OpenAI Ousts CEO Sam Altman Will Knight Business Who Is Mira Murati, OpenAI‚Äôs New Interim CEO? Steven Levy Given the many unknowns about large language models and the potential for powerful but flawed chatbots to cause trouble, Google should consider inviting outsiders to do more than just try limited demos of LaMDA, says Sameer Singh, a fellow at the Allen Institute for AI and a professor at UC Irvine. ‚ÄúThere has to be more conversation about how they‚Äôre making this safe and testing so outsiders can contribute to those efforts,‚Äù he says.
Pichai said Google would be consulting social scientists and human rights experts about LaMDA, but he didn't specify what access or input they might have. He said the project would follow Google‚Äôs AI principles, a set of guidelines introduced in 2018 after thousands of Google employees protested the company‚Äôs work on a Pentagon project to use AI to interpret drone surveillance footage.
Pichai did not mention a more recent and relevant scandal that Singh says adds reasons for Google to be careful and transparent as it productizes LaMDA. In late 2020, managers at the company objected to in-house researchers contributing to a research paper raising concerns about the limitations of large language models, including that they can generate offensive text. Two researchers, Timnit Gebru and Margaret Mitchell , were forced out of Google, but the paper that triggered the dispute was later presented at a peer-reviewed conference. One day you may be able to ask Google‚Äôs LaMDA to summarize the document‚Äôs key points‚Äîif you trust it to do so.
Updated 05/12/2022, 04:10pm ET: A previous version of this story incorrectly stated the name of Stanford's Center for Research on Foundation Models.
You Might Also Like ‚Ä¶ üìß Find the best bargains on quality gear with our Deals newsletter ‚Äú Someone is using photos of me to talk to men‚Äù First-gen social media users have nowhere to go The truth behind the biggest (and dumbest) battery myths We asked a Savile Row tailor to test all the ‚Äúbest‚Äù T-shirts you see in social media ads My kid wants to be an influencer.
 Is that bad? üåû See if you take a shine to our picks for the best sunglasses and sun protection Senior Editor X Topics artificial intelligence Google chatbots Will Knight Will Knight Will Knight Reece Rogers Will Knight Reece Rogers Khari Johnson Khari Johnson Facebook X Pinterest YouTube Instagram Tiktok More From WIRED Subscribe Newsletters Mattresses Reviews FAQ Wired Staff Coupons Black Friday Editorial Standards Archive Contact Advertise Contact Us Customer Care Jobs Press Center RSS Accessibility Help Cond√© Nast Store Do Not Sell My Personal Info ¬© 2023 Cond√© Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights.
WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Cond√© Nast.
Ad Choices Select international site United States LargeChevron UK Italia Jap√≥n Czech Republic & Slovakia
