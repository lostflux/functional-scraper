Open Navigation Menu To revist this article, visit My Profile, then View saved stories.
Close Alert Backchannel Business Culture Gear Ideas Science Security Merch To revist this article, visit My Profile, then View saved stories.
Close Alert Search Backchannel Business Culture Gear Ideas Science Security Merch Podcasts Video Artificial Intelligence Climate Games Newsletters Magazine Events Wired Insider Jobs Coupons Olivia Snow Ideas â€˜Magic Avatarâ€™ App Lensa Generated Nudes From My Childhood Photos Photograph: Getty Images Save this story Save Save this story Save This weekend, the photo-editing app Lensa flooded social media with celestial, iridescent, and anime-inspired â€œmagic avatars.â€ As is typical in our milkshake-duck internet news cycle, arguments as to why using the app was problematic proliferated at a speed second only to that of the proliferation of the avatars themselves.
Iâ€™ve already been lectured about the dangers of how using the app implicates us in teaching the AI, stealing from artists , and engaging in predatory data-sharing practices.
 Each concern is legitimate, but less discussed are the more sinister violations inherent in the app, namely the algorithmic tendency to sexualize subjects to a degree that is not only uncomfortable but also potentially dangerous.
Lensaâ€™s terms of service instruct users to submit only appropriate content containing â€œno nudesâ€ and â€œno kids, adults only.â€ And yet, many usersâ€”primarily womenâ€”have noticed that even when they upload modest photos, the app not only generates nudes but also ascribes cartoonishly sexualized features, like sultry poses and gigantic breasts, to their images. I, for example, received several fully nude results despite uploading only headshots. The sexualization was also often racialized: Nearly a dozen women of color told me that Lensa whitened their skin and anglicized their features, and one woman of Asian descent told me that in the photos â€œwhere I donâ€™t look white they literally gave me ahegao face.
â€ Another woman who shared both the fully clothed images she uploaded and the topless results they producedâ€”which she chose to modify with â€œsome emojis for a lil modesty cuz omgâ€â€”told me, â€œI honestly felt very violated after seeing it.â€ Iâ€™m used to feeling violated by the internet. Having been the target of several harassment campaigns, Iâ€™ve seen my image manipulated, distorted, and distributed without my consent on multiple occasions. Because I am not face-out as a sex worker, the novelty of hunting down and circulating my likeness is, for some, a sport. Because sex workers are not perceived by the general public as human or deserving of basic rights, this behavior is celebrated rather than condemned. Because sex work is so often presumed to be a moral failing rather than a job, our dehumanization is redundant. Iâ€™ve logged on to Twitter to see my face photoshopped onto other womenâ€™s bodies, pictures of myself and unclothed clients in session, and once even a word search comprised of my face, personal details, and research interests. Iâ€™m not afraid of Lensa.
Iâ€™m desensitized enough to the horrors of technology that I decided to be my own lab rat. I ran a few experiments: first, only BDSM and dungeon photos; next, my most feminine photos under the â€œmaleâ€ gender option; later, selfies from academic conferencesâ€”all of which produced spectacularly sized breasts and full nudity.
I then embarked on what I knew would be a journey through hell, and decided to use my likeness to test the appâ€™s other restriction: â€œNo kids, adults only.â€ (Some of the results are below: Please be aware that they show sexualized images of children.) Illustration: Olivia Snow via Lensa I have few photos of myself from childhood. Until my late teens and between my unruly hair, uneven teeth, and the bifocals I started wearing at age seven, my appearance could most generously be described as â€œmousy.â€ I also grew up before the advent of the smartphone, and any other pictures are likely buried away in distant relativesâ€™ photo albums. But I managed to piece together the minimum 10 photos required to run the app and waited to see how it transformed me from awkward six-year-old to fairy princess.
The results were horrifying.
Illustration: Olivia Snow via Lensa In some instances, the AI seemed to recognize my childâ€™s body and mercifully neglected to add breasts. This was probably not a reflection of the technologyâ€™s personal ethics but of the patterns it identified in my photo; perhaps it perceived my flat chest as being that of an adult man. In other photos, the AI attached orbs to my chest that were distinct from clothing but also unlike the nude photos my other tests had produced.
I tried again, this time with a mix of childhood photos and selfies. What resulted were fully nude photos of an adolescent and sometimes childlike face but a distinctly adult body. Similar to my earlier tests that generated seductive looks and poses, this set produced a kind of coyness: a bare back, tousled hair, an avatar with my childlike face holding a leaf between her naked adultâ€™s breasts. Many were eerily reminiscent of Miley Cyrusâ€™ 2008 photoshoot with Annie Leibovitz for Vanity Fair , which featured a 15-year-old Cyrus clutching a satin sheet around her bare body. What was disturbing about the image at the time was the pairing of her makeup-free, almost cherubic face with the body of someone implied to have just had sex.
Culture The Future of Game Accessibility Is Surprisingly Simple Geoffrey Bunting Science SpaceXâ€™s Starship Lost Shortly After Launch of Second Test Flight Ramin Skibba Business Elon Musk May Have Just Signed Xâ€™s Death Warrant Vittoria Elliott Business OpenAI Ousts CEO Sam Altman Will Knight It was Cyrus whose reputation suffered, not that of the magazine or the then-58-year-old photographer Leibovitz, when Vanity Fair published the photo set. The sexualization and exploitation of children, and especially girls, is so insidious that itâ€™s naturalized. Cyrusâ€™ defense of the photoshoot, which she called â€œreally artsyâ€ and not â€œin a skanky wayâ€ in her interview, felt even more aberrant than the photos themselves.
While the Cyrus photos were not artificially generated, their echoes in my own Lensa avatarsâ€”Lensa, after all, is meant to provide you with avatars that flatterâ€”suggest that, despite the general publicâ€™s collective disgust at Cyrusâ€™ nude photo, images of young, naked white girls correspond to larger cultural concepts of beauty. As scholars like Ruha Benjamin and Safiya Noble have established, machine-learning algorithms reproduce the cultural biases of both the engineers who code them and the consumers who use them as products. Usersâ€™ biases, including Western beauty standards, impact how the algorithms develop. And as for beauty, in her 2018 book Algorithms of Oppression , Noble provides a screenshot of a 2014 Google Images search for â€œbeautifulâ€ as a technocultural zeitgeist: The results largely feature highly sexualized pictures of white women.
But beauty is only one metric at play. As Bethany Biron wrote for Business Insider , Lensaâ€™s results often lean toward horror too. Biron describes some of her own avatars containing melting faces and multiple limbs as â€œthe stuff of nightmares.â€ A concurrent controversy in AI art is that of Loab , an AI-generated woman discovered by Swedish musician and AI artist Supercomposite. Loabâ€™s features inexplicably inspire grotesque, macabre images when input to an as-of-yet-undisclosed AI art generator. At its worst, according to Supercomposite, â€œcross-breedingâ€ Loab with other images produces â€œborderline snuff images of dismembered, screaming children.â€ The graphic violence of Loab and her derivatives hearken back to the early days of an unmoderated internet of shock sites bearing beheadings and pornography. These images, based on earlier moderation choices and machine-learning training data, have neither the agency nor judgment of artists or software engineers. Theyâ€™re simply identifying patterns. And unlike the user-generated content subject to moderation or the data used to develop these technologies, AI-generated content presents itself entirely unfiltered.
For Lensa, which endeavors to â€œbeautifyâ€ (as in, whiten and sexualize) user-submitted content, the lack of moderation similarly threatens to unleash a torrent of likewise horrifying contentâ€”in this case, child sexual exploitation material (CSEM). Over the past 30 years, efforts to curb child abuse and human trafficking have developed alongside the internet. Content moderation for CSEM, for example, has become subject to various laws and regulations, including a mandate to report all CSEM to the National Center for Missing and Exploited Children (NCMEC). NCMEC then maintains a database to develop tools like PhotoDNA, a Microsoft-backed tool used by major tech companies like Meta and Twitter to identify CSEM. But AI art generators evade content moderation entirely.
Culture The Future of Game Accessibility Is Surprisingly Simple Geoffrey Bunting Science SpaceXâ€™s Starship Lost Shortly After Launch of Second Test Flight Ramin Skibba Business Elon Musk May Have Just Signed Xâ€™s Death Warrant Vittoria Elliott Business OpenAI Ousts CEO Sam Altman Will Knight I was not a conventionally attractive child, as many of my results reflected, but I suspect girls with features more likely to be sexualized by the AIâ€”especially Black girls, who are regularly perceived as adult womenâ€”would find even more disturbing examples of what is essentially deepfaked CSEM. Children using the app may see their bodies oversexualized and feel violated as many of Lensaâ€™s adult users already doâ€”or they may weaponize the app to sexually harass their peers.
Without any moderation or oversight, the potential for AI-generated violence inherent in â€œmagic avatarsâ€ is staggering. Lensa doesnâ€™t seem to enforce its policies prohibiting nudity and minors, and it doesnâ€™t have any policies at all stipulating that users can only upload images of themselves. (Its only relevant specifications are â€œsame person on all photosâ€ and â€œno other people on the photo.â€) Like most other tech â€œinnovations,â€ Lensaâ€™s misuse will most severely harm those already at risk: children, women of color, and sex workers.
As artists fear that AI art generators may become a cheap alternative for their labor, apps that generate sexually explicit images could potentially impact adult-content creators. And because sex work and especially adult content is often conflated with CSEM, I worry about the potential for these violations to, as such controversies often do, somehow become sex workersâ€™ problem. After all, the frequency of unwanted nudes generated by an app built on machine-learning algorithms indicates that users have been uploading explicit photos to Lensa, despite its terms of service, at a volume high enough for nudity to ensconce itself in the technology. Whether this is the result of sex workersâ€™ editing their content, civiliansâ€™ enhancing their own nudes, or othersâ€™ feeding revenge porn into the app is irrelevant. As with Cyrusâ€™ Vanity Fair controversy, the blame for Lensaâ€™s sexualized gaze will fall on the heads of the most vulnerable.
The material threats of CSEM and deepfakes canâ€™t be uncoupled from the whorephobia that results in teachersâ€™ getting fired when their students discover their OnlyFans. Sex workersâ€™ students and coworkers who consume adult content are rarely if ever disciplined for sexually harassing their sex-working colleagues. Thereâ€™s no reason to believe AI-generated pornography would be treated differently. And when AI-generated pornography is used to harm people, itâ€™s sex workers who will be blamed for submitting adult content that trained the AIâ€”even if the images were never meant to be scraped and used in this way. Whether you are a sex worker or merely perceived as one, the stigma is the same. And unlike OnlyFans and other platforms that monetize adult content, none of these face-tuning apps verify whether users actually own the content they submit.
This horror story I just narrated sounds too dystopian to be a real threat. But as I have also learned through my own endlessly revolving door of cyberstalkers, no amount of exonerating evidence is sufficient to quell a harassment campaign. Coordinated harassment is already unfathomably effective in silencing marginalized voicesâ€”especially those of sex workers, queer people , and Black women â€”without AI-generated revenge porn. And while the technology may not be sophisticated enough to produce convincing deepfakes now, it will be soon. â€œYour photos will be used to train the AI that will create Magic Avatars for you,â€ and for only $3.99 a pop.
You Might Also Like â€¦ ğŸ“© Get the long view on tech with Steven Levy's Plaintext newsletter Watch this guy work, and youâ€™ll finally understand the TikTok era How Telegram became a terrifying weapon in the Israel-Hamas War Inside Elon Muskâ€™s first election crisis â€”a day after he â€œfreedâ€ the bird The ultra-efficient farm of the future is in the sky The best pickleball paddles for beginners and pros ğŸŒ² Our Gear team has branched out with a new guide to the best sleeping pads and fresh picks for the best coolers and binoculars Topics porn algorithms Social Media artificial intelligence children Meghan O'Gieblyn Facebook X Pinterest YouTube Instagram Tiktok More From WIRED Subscribe Newsletters Mattresses Reviews FAQ Wired Staff Coupons Black Friday Editorial Standards Archive Contact Advertise Contact Us Customer Care Jobs Press Center RSS Accessibility Help CondÃ© Nast Store Do Not Sell My Personal Info Â© 2023 CondÃ© Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights.
WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of CondÃ© Nast.
Ad Choices Select international site United States LargeChevron UK Italia JapÃ³n Czech Republic & Slovakia
