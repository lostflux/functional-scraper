Open Navigation Menu To revist this article, visit My Profile, then View saved stories.
Close Alert Backchannel Business Culture Gear Ideas Science Security Merch To revist this article, visit My Profile, then View saved stories.
Close Alert Search Backchannel Business Culture Gear Ideas Science Security Merch Podcasts Video Artificial Intelligence Climate Games Newsletters Magazine Events Wired Insider Jobs Coupons Will Knight Business How Censorship Can Influence Artificial Intelligence Illustration: Elena Lacey; Getty Images Save this story Save Save this story Save Application Content moderation Ethics Text analysis End User Research Government Sector Social media Research Publishing Source Data Text Technology Machine learning Natural language processing Artificial intelligence is hardly confined by international borders, as businesses, universities, and governments tap a global pool of ideas, algorithms, and talent. Yet the AI programs that result from this global gold rush can still reflect deep cultural divides.
New research shows how government censorship affects AI algorithms—and can influence the applications built with those algorithms.
Margaret Roberts , a political science professor at UC San Diego, and Eddie Yang, a PhD student there, examined AI language algorithms trained on two sources: the Chinese-language version of Wikipedia , which is blocked within China; and Baidu Baike , a similar site operated by China’s dominant search engine, Baidu, that is subject to government censorship. Baidu declined to comment.
The researchers were curious whether censorship of certain words and phrases could be learned by AI algorithms and find its way into software that uses those algorithms. This might influence the language that a chatbot or a voice assistant uses, the phrasing by a translation program, or the text of autocomplete tools.
The type of language algorithm they used learns by analyzing the way words appear together in large quantities of text. It represents different words as connected nodes in a physical space; the closer words appear, the more similar their meaning.
A translation program might infer the meaning of an unknown word by looking at these relationships in two different languages, for example.
The UCSD researchers found key differences in the resulting AI algorithms that the researchers said seem to reflect the information that is censored in China. For example, the one trained on Chinese Wikipedia represented “democracy” closer to positive words, such as “stability.” The algorithm trained on Baike Baidu represented “democracy” closer to “chaos.” Roberts and Yang then used the algorithms to build two programs to assess the sentiment—the positive versus negative meaning—of news headlines. They found that one trained on Chinese Wikipedia assigned more positive scores to headlines that mentioned terms including “election,” “freedom,” and “democracy,” while the one trained on Baidu Baike assigned more positive scores to headlines featuring “surveillance,” “social control,” and “CCP.” The study will be presented at the 2021 Conference on Fairness Accountability and Transparency (FAccT) in March.
In recent years, researchers have highlighted how race and gender biases can lurk in many artificial intelligence systems. Algorithms trained on text scraped from the web or old books, for instance, will learn to replicate the biases displayed by the human authors of that text. In 2018, researchers at Google demonstrated cultural biases in image recognition algorithms, which may, for example, recognize only Western wedding scenes.
By Tom Simonite Roberts notes that the differences seen in their study may not be due entirely to government censorship. Some may be the result of self-censorship or simply cultural differences between those writing the encyclopedia articles. But she says it is important to recognize that government policy can cause other forms of bias to lurk in AI systems. “We see this as a starting point for trying to understand how government-shaped training data appears within machine learning,” Roberts says.
Roberts says researchers and policymakers need to consider how governments in the future might influence how AI systems are trained in order to make censorship more effective or export particular values.
Business What Sam Altman’s Firing Means for the Future of OpenAI Steven Levy Business Sam Altman’s Sudden Exit Sends Shockwaves Through OpenAI and Beyond Will Knight Gear Humanity’s Most Obnoxious Vehicle Gets an Electric (and Nearly Silent) Makeover Boone Ashworth Security The Startup That Transformed the Hack-for-Hire Industry Andy Greenberg Graeme Hirst , a professor at the University of Toronto who specializes in computational linguistics and natural language processing, has a few qualms with the study methodology. Without carefully studying the differences between Chinese Wikipedia and Baidu Baike, Hirst says, it is hard to ascribe variations in the algorithms to censorship. It is also possible that Chinese Wikipedia contains anti-Chinese or overtly pro-democracy content, he says. Hirst adds that it is unclear how the sentiment analysis was done and whether bias may have been introduced there.
Others see it as a welcome contribution to the field.
“In a certain sense, this is not surprising,” says Suresh Venkatasubramanian , a professor at the University of Utah who studies AI ethics and cofounded the FAcct conference.
Venkatasubramanian points out that AI algorithms trained on Western news articles might contain their own anti-China biases. “But I think it's still important to do the work to show it happening,” he says. “Then you can start asking how it shows up, how do you measure it, what does it look like and so on.” 📩 The latest on tech, science, and more: Get our newsletters ! The unsettling truth about the “Mostly Harmless” hiker Worrisome new coronavirus strains are emerging. Why now ? Want to write a book this year? These tools can help This Chinese lab is aiming for big AI breakthroughs White nationalism is far worse than a “disease” 🎮 WIRED Games: Get the latest tips, reviews, and more 💻 Upgrade your work game with our Gear team’s favorite laptops , keyboards , typing alternatives , and noise-canceling headphones Senior Writer X Topics censorship artificial intelligence language China Lila Hassan Steven Levy David Gilbert Kari McMahon Nelson C.J.
Peter Guest Andy Greenberg Joel Khalili Facebook X Pinterest YouTube Instagram Tiktok More From WIRED Subscribe Newsletters Mattresses Reviews FAQ Wired Staff Coupons Black Friday Editorial Standards Archive Contact Advertise Contact Us Customer Care Jobs Press Center RSS Accessibility Help Condé Nast Store Do Not Sell My Personal Info © 2023 Condé Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights.
WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast.
Ad Choices Select international site United States LargeChevron UK Italia Japón Czech Republic & Slovakia
