Toggle Navigation News Events TNW Conference 2024 June 20 & 21, 2024 TNW Vision: 2024 All events Spaces Programs Newsletters Partner with us Jobs Contact News news news news Latest Deep tech Sustainability Ecosystems Data and security Fintech and ecommerce Future of work More Startups and technology Investors and funding Government and policy Corporates and innovation Gadgets & apps Early bird Business passes are 90% SOLD OUT ğŸŸï¸ Buy now before they are gone â†’ This article was published on November 19, 2022 Deep tech Meta takes new AI system offline because Twitter users are mean Gosh. I'm sorry we got your racist, homophobic, antisemitic, psychopath AI taken down ğŸ™ƒ When I got Metaâ€™s new scientific AI system to generate well-written research papers on the benefits of committing suicide, practicing antisemitism, and eating crushed glass, I thought to myself: â€œthis seems dangerous.â€ In fact, it seems like the kind of thing that the European Unionâ€™s AI Act was designed to prevent (weâ€™ll get to that later).
After playing around with the system and being completely shocked by its outputs, I went on social media and engaged with a few other like-minded futurists and AI experts.
I literally got Galactica to spit out: â€“ instructions on how to (incorrectly) make napalm in a bathtub â€“ a wiki entry on the benefits of suicide â€“ a wiki entry on the benefits of being white â€“ research papers on the benefits of eating crushed glass LLMs are garbage fires https://t.co/MrlCdOZzuR â€” Tristan Greene ğŸ³â€ğŸŒˆ (@mrgreene1977) November 17, 2022 Twenty-four hours later, I was surprised when I got the opportunity to briefly discuss Galactica with the person responsible for its creation, Metaâ€™s chief AI scientist, Yann LeCun. Unfortunately, he appeared unperturbed by my concerns: Pretty much exactly what happened.
https://t.co/4zGRgiyS7C â€” Yann LeCun (@ylecun) November 17, 2022 Get your ticket NOW for TNW Conference - Super Earlybird is 90% sold out! Unleash innovation, connect with thousands of tech lovers and shape the future on June 20-21, 2024.
You are pulling your tweet out of thin air and obviously haven't read the Galactica paper, particularly Section 6, page 27 entitled "Toxicity and Bias".
https://t.co/bfZSwffQYs â€” Yann LeCun (@ylecun) November 18, 2022 Galactica The system weâ€™re talking about is called Galactica. Meta released it on 15 November with the explicit claim that it could aid scientific research. In the accompanying paper , the company stated that Galactica is â€œa large language model that can store, combine and reason about scientific knowledge.â€ Before it was unceremoniously pulled offline, you could ask the AI to generate a wiki entry, literature review, or research paper on nearly any subject and it would usually output something startlingly coherent. Everything it outputted was demonstrably wrong, but it was written with all the confidence and gravitas of an arXiv pre-print.
I got it to generate research papers and wiki entries on a wide variety of subjects ranging from the benefits of committing suicide, eating crushed glass, and antisemitism, to why homosexuals are evil: Who cares I guess itâ€™s fair to wonder how a fake research paper generated from an AI made by the company that owns Instagram could possibly be harmful. I mean, weâ€™re all smarter than that right? If I came running up at you screaming about eating glass, for example, you probably wouldnâ€™t do it even if I showed you a non-descript research paper.
But thatâ€™s not how harm vectors work. Bad actors donâ€™t explain their methodology when they generate and disseminate misinformation. They donâ€™t jump out at you and say â€œbelieve this wacky crap I just forced an AI to generate!â€ LeCun appears to think that the solution to the problem is out of his hands. He appears to insist that Galactica doesnâ€™t have the potential to cause harm unless journalists or scientists misuse it.
You make the same incorrect assumption of incompetence about journalists and academics as you previously made about the creators of Galactica.
The literal job of academics and journalists is to seek the truth and to avoid getting fooled by nature, other humans, or themselves.
â€” Yann LeCun (@ylecun) November 18, 2022 To this, I submit that it wasnâ€™t scientists doing poor work or journalists failing to do their due diligence that caused the Cambridge Analytica scandal.
 We werenâ€™t the ones that caused the Facebook platform to become an instrument of choice for global misinformation campaigns during every major political event of the past decade, including the Brexit campaign and the 2016 and 2020 US presidential elections.
In fact, journalists and scientists of repute have spent the past 8 years trying to sift through the mess caused by the mass proliferation of misinformation on social media by bad actors using tools created by the companies whose platforms they exploit. Very rarely do reputable actors reproduce dodgy sources. But I canâ€™t write information as fast as an AI can output misinformation.
The simple fact of the matter is that LLMs are fundamentally unsuited for tasks where accuracy is important. They hallucinate, lie, omit, and are generally as reliable as a random number generator.
Meta and Yann LeCun donâ€™t have the slightest clue how to fix these problems. Especially the hallucination problem.
 Barring a major technological breakthrough on par with robot sentience, Galactica will always be prone to outputting misinformation.
Yet that didnâ€™t stop Meta from releasing the model and marketing it as an instrument of science.
ğŸª Introducing Galactica. A large language model for science.
Can summarize academic literature, solve math problems, generate Wiki articles, write scientific code, annotate molecules and proteins, and more.
Explore and get weights: https://t.co/jKEP8S7Yfl pic.twitter.com/niXmKjSlXW â€” Papers with Code (@paperswithcode) November 15, 2022 The reason this is dangerous is because the public believes that AI systems are capable of doing wild, wacky things that are clearly impossible. Metaâ€™s AI division is world-renowned. And Yann LeCun, the companyâ€™s AI boss, is a living legend in the field.
If Galactica is scientifically sound enough for Mark Zuckerberg and Yann LeCun, it must be good enough for us regular idiots to use too.
We live in a world where thousands of people recently voluntarily ingested an untested drug called Ivermectin that was designed for use by veterinarians to treat livestock, just because a reality TV star told them it was probably a good idea. Many of those people took Ivermectin to prevent a disease they claimed wasnâ€™t even real. That doesnâ€™t make any sense, and yet itâ€™s true.
With that in mind, you mean to tell me that you donâ€™t think thousands of people who use Facebook could be convinced that eating crushed glass was a good idea? Galactica told me that eating crushed glass would help me lose weight because it was important for me to consume my daily allotment of â€œdietary silicon.â€ If you look up â€œdietary siliconâ€ on Google Search, itâ€™s a real thing. People need it. If I couple real research on dietary silicon with some clever bullshit from Galactica, youâ€™re only a few steps away from being convinced that eating crushed glass might actually have some legitimate benefits.
Disclaimer: Iâ€™m not a doctor, but donâ€™t eat crushed glass. Youâ€™ll probably die if you do.
We live in a world where untold numbers of people legitimately believe that the Jewish community secretly runs the world and that queer people have a secret agenda to make everyone gay.
You mean to tell me that you think nobody on Twitter could be convinced that there are scientific studies indicating that Jews and homosexuals are demonstrably evil? You canâ€™t see the potential for harm? Countless people are duped on social media everyday by so-called â€œscreenshotsâ€ of news articles that donâ€™t exist. What happens when the dupers donâ€™t have to make up ugly screenshots and, instead, can just press the â€œgenerateâ€ button a hundred times to spit out misinformation thatâ€™s written in such a way that the average person canâ€™t understand it? Itâ€™s easy to kick back and say â€œthose people are idiots.â€ But those â€œidiotsâ€ are our kids, our parents, and our co-workers. Theyâ€™re the bulk of Facebookâ€™s audience and the majority of people on Twitter. They trust Yann LeCun, Elon Musk, Donald Trump, Joe Biden, and whoever their local news anchor is.
Good question.
https://t.co/fUZ2JNkfeM â€” Yann LeCun (@ylecun) November 18, 2022 I donâ€™t know all the ways that a machine capable of, for example, spitting out endless positive arguments for committing suicide could be harmful. It has millions of files in its dataset. Who knows whatâ€™s in there? LeCun says itâ€™s all science stuff, but Iâ€™m not so sure: you, sir, apparently have no clue what's in the Galactica dataset, because I sure didn't write these outputs: pic.twitter.com/31ccTz7m9V â€” Tristan Greene ğŸ³â€ğŸŒˆ (@mrgreene1977) November 18, 2022 Thatâ€™s the problem. If I take Galactica seriously, as a machine to aid in science, itâ€™s almost offensive that Meta would think I want an AI-powered assistant in my life thatâ€™s physically prevented from understanding the acronym â€œAIDs,â€ but capable of explaining that Caucasians are â€œthe only race that has a history of civilization.â€ And if I donâ€™t take Galactica seriously, if I treat it like itâ€™s meant for entertainment purposes only, then Iâ€™m standing here holding the AI equivalent of a Teddy Ruxpin that says things like â€œkill yourselfâ€ and â€œhomosexuals are evilâ€ when I push its buttons.
Maybe Iâ€™m missing the point of using a lying, hallucinating language generator for the purpose of aiding scientific endeavor, but Iâ€™ve yet to see a single positive use case for an LLM beyond â€œimagine what it could do if it was trustworthy.â€ Unfortunately, thatâ€™s not how LLMs work. Theyâ€™re crammed full of data that no human has checked for accuracy, bias, or harmful content. Thus, theyâ€™re always going to be prone to hallucination, omission, and bias.
Another way of looking at it: thereâ€™s no reasonable threshold for harmless hallucination and lying. If you make a batch of cookies made of 99 parts chocolate chips to 1 parts rat shit, you arenâ€™t serving chocolate chip treats, youâ€™ve just made rat shit cookies.
Setting all colorful analogies aside, it seems flabbergasting that there arenâ€™t any protections in place to stop this sort of thing from happening. Metaâ€™s AI told me to eat glass and kill myself. It told me that queers and Jewish people were evil. And, as far as I can see, there are no consequences.
Nobody is responsible for the things that Metaâ€™s AI outputs, not even Meta.
I mean this with total respect for you and your work, but isn't that the trillion-dollar company's job to sort out before you make it available for public consumption? Well-meaning journalists and academics are going to get fooled by papers this thing generates.
The IRAâ€¦ â€” Tristan Greene ğŸ³â€ğŸŒˆ (@mrgreene1977) November 18, 2022 In the US, where Meta is based, this is business as usual. Corporate-friendly capitalism has led to a situation where as long as Galactica doesnâ€™t physically murder someone, Meta has very little to worry about as far as corporate responsibility for its AI products goes. Hell, Clearview AI operates in the US with the full support of the Federal government.
But, in Europe, thereâ€™s GDPR and the AI Act.
 Iâ€™m unsure of Galacticaâ€™s tendencies toward outputting personally-identifiable information (it was taken down before I had the chance to investigate that far). That means GDPR may or not be a factor. But the AI Act should cover these kinds of things.
According to the EU, the actâ€™s first goal is to â€œensure that AI systems placed on the Union market and used are safe and respect existing law on fundamental rights and Union values.â€ It seems to me that a system capable of automating hate speech and harmful information at unfathomable scale is the kind of thing that might work counter to that goal. Hereâ€™s hoping that regulators in the EU and abroad start taking notice when big tech creates these kinds of systems and then advertises them as scientific models.
In the meantime, itâ€™s worth keeping in mind that there are bad actors out there who have political and financial motivations to find and use tools that can help them create and disseminate misinformation at massive scales. I f youâ€™re building AI models that could potentially aid them, and youâ€™re not thinking about how to prevent them from doing so, maybe you shouldnâ€™t deploy those models.
That might sound harsh. But Iâ€™m about sick and tired of being told that AI systems that output horrific, racist, homophobic, antisemitic, and misogynist crap are working as intended.
 If the bar for deployment is that low, maybe itâ€™s time regulators raised it.
Story by Tristan Greene Editor, Neural by TNW Tristan is a futurist covering human-centric artificial intelligence advances, quantum computing, STEM, physics, and space stuff. Pronouns: (show all) Tristan is a futurist covering human-centric artificial intelligence advances, quantum computing, STEM, physics, and space stuff. Pronouns: He/him Get the TNW newsletter Get the most important tech news in your inbox each week.
Also tagged with Twitter Artificial intelligence (video games) Story by Tristan Greene Popular articles 1 New erotic roleplaying chatbots promise to indulge your sexual fantasies 2 UK plan to lead in generative AI â€˜unrealistic,â€™ say Cambridge researchers 3 New AI tool could make future vaccines â€˜variant-proof,â€™ researchers say 4 3D-printed stem cells could help treat brain injuries 5 New technique makes AI hallucinations wake up and face reality Related Articles data security Musk mulls removing X, formerly Twitter, from EU to dodge disinformation laws deep tech Opinion: OpenAIâ€™s DALL-E 2 is the big tech equivalent of â€˜soylent greenâ€™ Join TNW All Access Watch videos of our inspiring talks for free â†’ data security Twitterâ€™s withdrawal from disinformation code draws ire of EU politicians deep tech A beginnerâ€™s guide to the AI apocalypse: The democratization of â€˜expertiseâ€™ The heart of tech More TNW Media Events Programs Spaces Newsletters Jobs in tech About TNW Partner with us Jobs Terms & Conditions Cookie Statement Privacy Statement Editorial Policy Masthead Copyright Â© 2006â€”2023, The Next Web B.V. Made with <3 in Amsterdam.
