Artificial Intelligence View All AI, ML and Deep Learning Auto ML Data Labelling Synthetic Data Conversational AI NLP Text-to-Speech Security View All Data Security and Privacy Network Security and Privacy Software Security Computer Hardware Security Cloud and Data Storage Security Data Infrastructure View All Data Science Data Management Data Storage and Cloud Big Data and Analytics Data Networks Automation View All Industrial Automation Business Process Automation Development Automation Robotic Process Automation Test Automation Enterprise Analytics View All Business Intelligence Disaster Recovery Business Continuity Statistical Analysis Predictive Analysis More Data Decision Makers Virtual Communication Team Collaboration UCaaS Virtual Reality Collaboration Virtual Employee Experience Programming & Development Product Development Application Development Test Management Development Languages 5 Ways to Rein in Data Center Consumption in 2024 Share on Facebook Share on X Share on LinkedIn Illustration by: Leandro Stavorengo Are you ready to bring more awareness to your brand? Consider becoming a sponsor for The AI Impact Tour. Learn more about the opportunities here.
IT and data leaders have known for years that data centers are power-hungry and energy-intensive, but demand for data centers has also skyrocketed thanks to the growth of remote work and high-speed streaming, as well as the explosion of generative AI models and tools. As a result, U.S. data center demand is forecast to grow by 10% per year until 2030.
However, these days, thanks to rising costs and stakeholder pressures, organizations are determined and committed to reining in data center consumption. According to a recent study from Gartner Research, a whopping 75% of organizations will have implemented a data center infrastructure sustainability program by 2027, up from less than 5% in 2022.
“Responsibilities for sustainability are increasingly being passed down from CIOs to infrastructure and operations leaders to improve IT’s environmental performance, particularly around data centers,” said Autumn Stanish, senior principal analyst at Gartner.
Yet, at the same time, a July 2023 survey from Hitachi Vantara found that tackling data center sustainability is no easy task. It found that two-thirds of IT leaders currently measure their data center’s energy consumption; however, one-third of respondents acknowledged that their data infrastructure uses too much energy and nearly half (46%) admitted their sustainability policies don’t address the impact of storing unused data.
VB Event The AI Impact Tour Connect with the enterprise AI community at VentureBeat’s AI Impact Tour coming to a city near you! “As emerging technologies like generative AI contribute to doubling the volume of data expected in the next two years, businesses need to find the right balance between scalability, sustainability, and security,” said Bharti Patel, senior vice president of product engineering at Hitachi Vantara about the study.
Addressing data center consumption is crucial experts say. This is because of environmental impacts like carbon emissions associated with data centers (which doubled between 2017 and 2020) and resource depletion, as well as the need to boost cost efficiency, comply with regulations, tackle corporate responsibility and improve disaster resilience.
These are five key ways organizations can rein in their data center consumption in 2024: Migrate to shared cloud infrastructure According to Bridgette McAdoo, chief sustainability officer at customer experience solutions company Genesys, organizations can take an impactful step towards lowering their data center emissions by migrating to shared cloud infrastructure.
“Much like carpooling, having multiple organizations operating in the same cloud can significantly lower emissions,” said McAdoo. “Even better, choosing a multi-tenant cloud provider that runs on renewable energy takes carbon emission reduction a step further.” Right now, she explained, many organizations continue to operate with legacy on-premises technologies. “This can create multiple disadvantages such as slowing down their pace of innovation in addition to limiting their ability to put more sustainable business practices into place for lasting—and much needed—impact on our world,” she said.
Invest in liquid cooling technologies As computing power in data centers grows thanks to workload consolidation and processing-intensive applications like AI, each rack consumes more energy and generates more heat — requiring better cooling systems to keep things running smoothly and safely. The future of data center scalability is likely to involve continued growth and increased adoption of liquid cooling technologies, said William Estes, general manager at component company Anderson Power.
“With the increasing demand for high-performance computing and data storage, liquid cooling will be an important solution for managing the heat generated by the growing amount of power consumed in data centers,” he explained. Liquid cooling systems can increase energy efficiency by up to 40% compared to traditional convection methods.
Liquid cooling has seen a significant increase in adoption in recent years, with the global liquid cooling market expected to grow from $1.37 billion in 2020 to $4.38 billion by 2026.
Focus on specialized hardware tailored to specific workloads Specialized hardware tailored to specific workloads will significantly reduce energy consumption and improve efficiency in data centers, said Jonathan Friedmann, CEO and co-founder at hardware acceleration company Speedata. “By investing in purpose-built infrastructure, IT leaders can align their data center strategies with sustainability goals, ensuring a more environmentally responsible operation that optimizes for the total cost of performance, including indirect costs like power, cooling, and footprint,” he said.
He pointed out that organizations have already seen this prioritization come into play for AI through both the proliferation of GPUs, which excel at parallel processing workloads across data centers, and collaboration between suppliers like Nvidia and cloud providers like AWS. But he added that most data centers still conduct analytics – highly complex and resource-intensive but essential database workloads – with CPUs, which he explained, “were not designed to handle large amounts of data or excessive computing power, not to mention their relatively slow processing speeds.” As Moore’s Law has rapidly slowed and traditional compute has seen little advancement for over a decade, Friedmann said a shift to specialized chips “can help us not only refocus on sustainability for carbon-intensive data center workloads like big data analytics, but also drive key capabilities and accelerate innovation.” Remember that data center sustainability is a journey, not a destination According to Alpesh Saraiya, senior director of data center product management at Honeywell Building Technologies, the key to managing sustainability for the long haul is to recognize that it is more a journey rather than a destination. “As such, data center operators should evaluate a variety of carbon footprint reduction techniques, from sourcing renewable energy sources to cooling control loop optimization, which focuses on improving the performance, efficiency and stability keeping IT servers within required service level agreement constraints, to server liquid cooling options,” he said.
Data center operators also need a holistic operations management platform – one that integrates situational awareness of critical operational technology (OT) data with data from IT assets. “By digitizing and aggregating information from these disparate systems into a unified platform and benchmarking a baseline, and applying powerful analytics, such a solution can provide insights and optimizations that help operators better protect uptime, manage maintenance and reduce CO2 emissions and energy usage,” he said.
Implement a serverless logging framework that intentionally reduces CPU cycles One of the key issues behind the high-power consumption of data centers is that organizations have more CPU cycles than necessary, causing them to do more data processing and pulling than needed, said Gary Hoberman, CEO and founder of software company Unqork.
“We recommend implementing a serverless logging framework that intentionally reduces CPU cycles and believe this should become a best practice for IT leaders,” he said, explaining that the framework is a mechanism to perform logging, indexing, signaling, and most importantly, filtering at the ingestion layer. “If there is no reason to signal in real-time against the logs, then they are stored for compliance, and not forwarded to additional indexers or analysis engines, saving CPU cycles,” he said.
In addition, he recommended standardizing on Sigma, the open-source rules language for logs. “This allows companies to ensure countermeasures can be written once, and adapted to any signaling system required, should alerting mechanisms or backend databases be changed for performance or cost reasons,” he said.
VentureBeat's mission is to be a digital town square for technical decision-makers to gain knowledge about transformative enterprise technology and transact.
Discover our Briefings.
The AI Impact Tour Join us for an evening full of networking and insights at VentureBeat's AI Impact Tour, coming to San Francisco, New York, and Los Angeles! VentureBeat Homepage Follow us on Facebook Follow us on X Follow us on LinkedIn Follow us on RSS Press Releases Contact Us Advertise Share a News Tip Contribute to DataDecisionMakers Careers Privacy Policy Terms of Service Do Not Sell My Personal Information © 2023 VentureBeat.
 All rights reserved.
