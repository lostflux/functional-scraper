Open Navigation Menu To revist this article, visit My Profile, then View saved stories.
Close Alert Backchannel Business Culture Gear Ideas Science Security Merch To revist this article, visit My Profile, then View saved stories.
Close Alert Search Backchannel Business Culture Gear Ideas Science Security Merch Podcasts Video Artificial Intelligence Climate Games Newsletters Magazine Events Wired Insider Jobs Coupons Katerina Cizek shirin anlen Ideas The Thorny Art of Deepfake Labeling Play/Pause Button Pause Video: WIRED Staff; Getty Images Save this story Save Save this story Save Last week, the Republican National Committee put out a video advertisement against Biden, which featured a small disclaimer in the top left of the frame: ‚ÄúBuilt entirely with AI imagery.‚Äù Critics questioned the diminished size of the disclaimer and suggested its limited value, particularly because the ad marks the first substantive use of AI in political attack advertising. As AI-generated media become more mainstream, many have argued that text-based labels, captions, and watermarks are crucial for transparency.
But do these labels actually work? Maybe not.
For a label to work, it needs to be legible. Is the text big enough to read? Are the words accessible? It should also provide audiences with meaningful context on how the media has been created and used. And in the best cases, it also discloses intent: Why has this piece of media been put into the world? Katerina Cizek is the artistic director and research scientist at Co-Creation Studio at MIT Open Documentary Lab. She is the author (with Uricchio et al.) of Collective Wisdom , and a Peabody- and two-time Emmy-winning documentarian in the emergent tech and media space.
shirin anlen is an award-winning creative technologist, artist, and researcher. She is a media technologist for Witness, which helps people use video and technology to defend human rights.
Journalism, documentary media, industry, and scientific publications have long relied on disclosures to provide audiences and users with the necessary context. Journalistic and documentary films generally use overlay text to cite sources. Warning labels and tags are ubiquitous on manufactured goods, foods, and drugs. In scientific reporting, it‚Äôs essential to disclose how data and analysis were captured. But labeling synthetic media, AI-generated content, and deepfakes is often seen as an unwelcome burden, especially on social media platforms. It‚Äôs a slapped-on afterthought. A boring compliance in an age of mis/disinformation.
As such, many existing AI media disclosure practices, like watermarks and labels, can be easily removed. Even when they‚Äôre there, audience members‚Äô eyes‚Äînow trained on rapid-fire visual input‚Äîseem to unsee watermarks and disclosures. For example, in September 2019, the well-known Italian satirical TV show Striscia la Notizia posted a low-fidelity face-swap video of former prime minister Matteo Renzi sitting at a desk insulting his then coalition partner Matteo Salvini with exaggerated hand gestures on social media. Despite a Striscia watermark and a clear text-based disclaimer, according to deepfakes researcher Henry Adjer, some viewers believed the video was genuine.
This is called context shift: Once any piece of media, even labeled and watermarked, is distributed across politicized and closed social media groups, its creators lose control of how it is framed, interpreted, and shared. As we found in a joint research study between Witness and MIT, when satire mixes with deepfakes it often creates confusion, as in the case of this Striscia video. These sorts of simple text-based labels can create the additional misconception that anything that doesn‚Äôt have a label is not manipulated, when in reality, that may not be true.
Technologists are working on ways to quickly and accurately trace the origins of synthetic media, like cryptographic provenance and detailed file metadata. When it comes to alternative labeling methods, artists and human rights activists are offering promising new ways to better identify this kind of content by reframing labeling as a creative act rather than an add-on.
When a disclosure is baked into the media itself, it can‚Äôt be removed, and it can actually be used as a tool to push audiences to understand how a piece of media was created and why. For example, in David France‚Äôs documentary Welcome to Chechnya , vulnerable interviewees were digitally disguised with the help of inventive synthetic media tools like those used to create deepfakes. In addition, subtle halos appeared around their faces, a clue for viewers that the images they were watching had been manipulated, and that these subjects were taking an immense risk in sharing their stories. And in Kendrick Lamar‚Äôs 2022 music video, ‚Äú The Heart Part 5 ,‚Äù the directors used deepfake technology to transform Lamar‚Äôs face into both deceased and living celebrities such as Will Smith, O. J. Simpson, and Kobe Bryant. This use of technology is written directly into the lyrics of the song and choreography, like when Lamar uses his hand to swipe over his face, clearly indicating a deepfake edit. The resulting video is a meta-commentary on deepfakes themselves.
Business What Sam Altman‚Äôs Firing Means for the Future of OpenAI Steven Levy Business Sam Altman‚Äôs Sudden Exit Sends Shockwaves Through OpenAI and Beyond Will Knight Gear Humanity‚Äôs Most Obnoxious Vehicle Gets an Electric (and Nearly Silent) Makeover Boone Ashworth Security The Startup That Transformed the Hack-for-Hire Industry Andy Greenberg Activists‚Äô and artists‚Äô new takes on disclosure, like these, introduce new ways of seeing. Viewers of France‚Äôs documentary and Lamar‚Äôs music video are respectfully offered the visual language, vocabulary, and context to connect with and understand what they‚Äôre consuming. Both videos transcend easy answers and give audiences space to interpret for themselves. By contrast, despite their text-based labels, the Biden ad and the Italian satirical video fail to bring audiences into ‚Äúthe know‚Äù and leave them wondering, ‚ÄúIs this real or fake?‚Äù As creators work to develop more detailed frameworks for deepfake and AI disclosure, disciplines and modes like accessibility theory, interactive storytelling, TikTok, footnoting practices, and museum image description guidelines all have useful tools to offer. In the art project Alt-Text as Poetry , audiences are encouraged to draft alt-text descriptions of images for visually impaired audiences that are poetic rather than perfunctory. Just like artistic disclosures, alt-text helps explain‚Äîor disclose‚Äîcontextual information, ideally in a creative way. The artists explain that they approach access ‚Äúgenerously, centering disability culture, rather than focusing on compliance.‚Äù On TikTok, tags on videos and hashtags in captions provide insights into how users create videos and interact with each other through remixes, duets, snappy editing, AI effects, and filters. As a result, the app‚Äôs labeling system becomes an integral and fun part of the platform's engagement mechanism, showcasing the creative potential and social benefits of revealing the production process.
These context-driven labeling models engage users while making clear how these images have been created and manipulated. When creators go beyond the bare minimum of compliance, they can produce work that is more innovative and more principled. Art can illuminate.
WIRED Opinion publishes articles by outside contributors representing a wide range of viewpoints. Read more opinions here , and see our submission guidelines here.
 Submit an op-ed at ideas@wired.com.
You Might Also Like ‚Ä¶ üì® Make the most of chatbots with our AI Unlocked newsletter Taylor Swift, Star Wars, Stranger Things , and Deadpool have one man in common Generative AI is playing a surprising role in Israel-Hamas disinformation The new era of social media looks as bad for privacy as the last one Johnny Cash‚Äôs Taylor Swift cover predicts the boring future of AI music Your internet browser does not belong to you üîå Charge right into summer with the best travel adapters , power banks , and USB hubs Topics Social Media Deepfakes art Alt Text Meghan O'Gieblyn Facebook X Pinterest YouTube Instagram Tiktok More From WIRED Subscribe Newsletters Mattresses Reviews FAQ Wired Staff Coupons Black Friday Editorial Standards Archive Contact Advertise Contact Us Customer Care Jobs Press Center RSS Accessibility Help Cond√© Nast Store Do Not Sell My Personal Info ¬© 2023 Cond√© Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights.
WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Cond√© Nast.
Ad Choices Select international site United States LargeChevron UK Italia Jap√≥n Czech Republic & Slovakia
