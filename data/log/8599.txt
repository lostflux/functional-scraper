Open Navigation Menu To revist this article, visit My Profile, then View saved stories.
Close Alert Backchannel Business Culture Gear Ideas Science Security Merch To revist this article, visit My Profile, then View saved stories.
Close Alert Search Backchannel Business Culture Gear Ideas Science Security Merch Podcasts Video Artificial Intelligence Climate Games Newsletters Magazine Events Wired Insider Jobs Coupons Khari Johnson Business This Agency Wants to Figure Out Exactly How Much You Trust AI Boston Dynamics' robot dog, with the real thing.
Photograph: Michael Reichel/picture alliance/Getty Images Save this story Save Save this story Save Application Ethics Face recognition Human-computer interaction End User Consumer Government Big company Sector Consumer services IT The National Institute of Standards and Technology (NIST) is a federal agency best known for measuring things like time or the number of photons that pass through a chicken.
 Now NIST wants to put a number on a person‚Äôs trust in artificial intelligence.
Trust is part of how we judge the potential for danger, and it's an important factor in the adoption of AI. As AI takes on more and more complicated tasks, officials at NIST say, trust is an essential part of the evolving relationship between people and machines.
In a research paper , creators of the attempt to quantify user trust in AI say they want to help businesses and developers who deploy AI systems make informed decisions and identify areas where people don‚Äôt trust AI. NIST views the AI initiative as an extension of its more traditional work establishing trust in measurement systems. Public comment is being accepted until July 30.
Brian Stanton is coauthor of the paper and a NIST cognitive psychologist who focuses on AI system trustworthiness. Without trust, Stanton says, adoption of AI will slow or halt. He says many factors may affect a person‚Äôs trust in AI, such as their exposure to science fiction or the presence of AI skeptics among friends and family.
NIST is a part of the US Department of Commerce that has grown in prominence in the age of artificial intelligence. Under an executive order by former president Trump, NIST in 2019 released a plan for engaging with private industry to create standards for the use of AI. In January, Congress directed NIST to create a framework for trustworthy AI to guide use of the technology. One problem area: Studies by academics and NIST itself have found that some facial-recognition systems misidentify Asian and Black people 100 times more often than white people.
The trust initiative comes amid increased government scrutiny of AI. The Office of Management and Budget has said acceptance and adoption of AI will depend on public trust. Mentions of AI in Congress are increasing, and historic antitrust cases continue against tech giants including Amazon, Facebook, and Google. In April, the Federal Trade Commission told businesses to tell the truth about AI they use and not exaggerate what‚Äôs possible. ‚ÄúHold yourself accountable‚Äîor be ready for the FTC to do it for you,‚Äù the statement said.
Science SpaceX‚Äôs Starship Lost Shortly After Launch of Second Test Flight Ramin Skibba Gear The Best Home Depot Black Friday Deals Matt Jancer Gear 16 Early Black Friday Deals From Walmart Matt Jancer Business Why Teslas Totaled in the US Are Mysteriously Reincarnated in Ukraine Aarian Marshall NIST wants to measure trust in AI in two ways. A user trust potential score is meant to measure things about a person using an AI system, including their age, gender, cultural beliefs, and experience with other AI systems. The second score, the perceived system trustworthiness score, will cover more technical factors such as whether an outdated user interface makes people call AI into doubt. The proposed system score assigns weights to nine characteristics like accuracy and explainability. Factors that play into trusting AI and weights for factors like reliability and security are still being determined.
An AI system used by doctors to diagnose disease should be more accurate than one recommending music.
The NIST paper says expectations around an AI system will reflect its use. For example, a system used by doctors to diagnose disease should be more accurate than one recommending music.
Masooda Bashir, a professor in the University of Illinois‚Äô School of Information Sciences, studies how people trust or mistrust autonomous vehicles. She wants to see user trust measurement evolve to the point that you can pick your trust settings the same way people pick a color for a car.
Bashir called the NIST proposal a positive development, but she thinks the user trust score should reflect more factors, including a person‚Äôs mood and changing attitude toward AI as they get to know how a system performs. In a 2016 study , Bashir and coauthors found that stress levels can influence people‚Äôs attitudes about trust in AI. Those kinds of differences, she said, should help determine the weight given to the factors for trust identified by NIST.
Harvard University assistant professor Himabindu Lakkaraju studies the role trust plays in human decisionmaking in professional settings. She‚Äôs working with nearly 200 doctors at hospitals in Massachusetts to understand how trust in AI can change how doctors diagnose a patient.
For common illnesses like the flu, AI isn‚Äôt very helpful, since human professionals can recognize them pretty easily. But Lakkaraju found that AI can help doctors diagnose hard-to-identify illnesses like autoimmune diseases. In her latest work, Lakkaraju and coworkers gave doctors records of roughly 2,000 patients and predictions from an AI system, then asked them to predict whether the patient would have a stroke in six months. They varied the information supplied about the AI system, including its accuracy, confidence interval, and an explanation of how the system works. They found doctors‚Äô predictions were the most accurate when they were given the most information about the AI system.
Lakkaraju says she‚Äôs happy to see that NIST is trying to quantify trust, but she says the agency should consider the role explanations can play in human trust of AI systems. In the experiment, the accuracy of predicting strokes by doctors went down when doctors were given an explanation without data to inform the decision, implying that an explanation alone can lead people to trust AI too much.
Science SpaceX‚Äôs Starship Lost Shortly After Launch of Second Test Flight Ramin Skibba Gear The Best Home Depot Black Friday Deals Matt Jancer Gear 16 Early Black Friday Deals From Walmart Matt Jancer Business Why Teslas Totaled in the US Are Mysteriously Reincarnated in Ukraine Aarian Marshall ‚ÄúExplanations can bring about unusually high trust even when it is not warranted, which is a recipe for problems,‚Äù she says. ‚ÄúBut once you start putting numbers on how good the explanation is, then people's trust slowly calibrates.‚Äù Other nations are also trying to confront the question of trust in AI. The US is among 40 countries that signed onto AI principles that emphasize trustworthiness. A document signed by about a dozen European countries says trustworthiness and innovation go hand in hand, and can be considered ‚Äútwo sides of the same coin.‚Äù NIST and the OECD, a group of 38 countries with advanced economies, are working on tools to designate AI systems as high or low risk. The Canadian government created an algorithm impact assessment process in 2019 for businesses and government agencies. There, AI falls into four categories‚Äîfrom no impact on people‚Äôs lives or the rights of communities to very high risk and perpetuating harm on individuals and communities. Rating an algorithm takes about 30 minutes. The Canadian approach requires that developers notify users for all but the lowest-risk systems.
European Union lawmakers are considering AI regulations that could help define global standards for the kind of AI that‚Äôs considered low or high risk and how to regulate the technology. Like Europe‚Äôs landmark GDPR privacy law, the EU AI strategy could lead the largest companies in the world that deploy artificial intelligence to change their practices worldwide.
The regulation calls for the creation of a public registry of high-risk forms of AI in use in a database managed by the European Commission. Examples of AI deemed high risk included in the document include AI used for education, employment, or as safety components for utilities like electricity, gas, or water. That report will likely be amended before passage, but the draft calls for a ban on AI for social scoring of citizens by governments and real-time facial recognition.
The EU report also encourages allowing businesses and researchers to experiment in areas called ‚Äúsandboxes,‚Äù designed to make sure the legal framework is ‚Äúinnovation-friendly, future-proof, and resilient to disruption.‚Äù Earlier this month, the Biden administration introduced the National Artificial Intelligence Research Resource Task Force aimed at sharing government data for research on issues like health care or autonomous driving. Ultimate plans would require approval from Congress.
For now, the AI user trust score is being developed for AI practitioners. Over time, though, the scores could empower individuals to avoid untrustworthy AI and nudge the marketplace toward deploying robust, tested, trusted systems. Of course that‚Äôs if they know AI is being used at all.
üì© The latest on tech, science, and more: Get our newsletters ! What really happened when Google ousted Timnit Gebru Wait, vaccine lotteries actually work? How to turn off Amazon Sidewalk They rage-quit the school system‚Äî and they're not going back Apple World's full scope is coming into focus üëÅÔ∏è Explore AI like never before with our new database üéÆ WIRED Games: Get the latest tips, reviews, and more üèÉüèΩ‚Äç‚ôÄÔ∏è Want the best tools to get healthy? Check out our Gear team‚Äôs picks for the best fitness trackers , running gear (including shoes and socks ), and best headphones Senior Writer X Topics artificial intelligence algorithms machine learning government Vittoria Elliott Steven Levy Jacopo Prisco Will Knight Nelson C.J.
Peter Guest Andy Greenberg Joel Khalili Facebook X Pinterest YouTube Instagram Tiktok More From WIRED Subscribe Newsletters Mattresses Reviews FAQ Wired Staff Coupons Black Friday Editorial Standards Archive Contact Advertise Contact Us Customer Care Jobs Press Center RSS Accessibility Help Cond√© Nast Store Do Not Sell My Personal Info ¬© 2023 Cond√© Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights.
WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Cond√© Nast.
Ad Choices Select international site United States LargeChevron UK Italia Jap√≥n Czech Republic & Slovakia
