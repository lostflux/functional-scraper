Close Search Skip to main content Site Navigation Research Overview Index GPT-4 DALL¬∑E 3 API Overview Data privacy Pricing Docs ChatGPT Overview Enterprise Try ChatGPT Safety Company About Blog Careers Residency Charter Security Customer stories Search Navigation quick links Log in Try ChatGPT Menu Mobile Navigation Close Site Navigation Research Overview Index GPT-4 DALL¬∑E 3 API Overview Data privacy Pricing Docs ChatGPT Overview Enterprise Try ChatGPT Safety Company About Blog Careers Residency Charter Security Customer stories Quick Links Log in Try ChatGPT Search Blog Democratic inputs to AI Our nonprofit organization, OpenAI, Inc., is launching a program to award ten $100,000 grants to fund experiments in setting up a democratic process for deciding what rules AI systems should follow, within the bounds defined by the law.
Illustration: Justin Jay Wang May 25, 2023 Authors Wojciech Zaremba Arka Dhar Lama Ahmad Tyna Eloundou Shibani Santurkar Sandhini Agarwal Jade Leung Announcements , Responsible AI AI will have significant, far-reaching economic and societal impacts. Technology shapes the lives of individuals, how we interact with one another, and how society as a whole evolves. We believe that decisions about how AI behaves should be shaped by diverse perspectives reflecting the public interest.
‚Äã‚ÄãLaws encode values and norms to regulate behavior. Beyond a legal framework, AI, much like society, needs more intricate and adaptive guidelines for its conduct. For example: under what conditions should AI systems condemn or criticize public figures, given different opinions across groups regarding those figures? How should disputed views be represented in AI outputs? Should AI by default reflect the persona of a median individual in the world, the user‚Äôs country, the user‚Äôs demographic, or something entirely different? No single individual, company, or even country should dictate these decisions.
AGI should benefit all of humanity and be shaped to be as inclusive as possible. We are launching this grant program to take a first step in this direction. We are seeking teams from across the world to develop proof-of-concepts for a democratic process that could answer questions about what rules AI systems should follow. We want to learn from these experiments, and use them as the basis for a more global, and more ambitious process going forward. While these initial experiments are not (at least for now) intended to be binding for decisions, we hope that they explore decision relevant questions and build novel democratic tools that can more directly inform decisions in the future.
The governance of the most powerful systems , as well as decisions regarding their deployment, must have strong public oversight. This grant represents a step to establish democratic processes for overseeing AGI and, ultimately, superintelligence. It will be provided by the OpenAI non-profit organization, and the results of the studies will be freely accessible.
What do we mean by a ‚Äúdemocratic process‚Äù? By ‚Äúdemocratic process,‚Äù we mean a process in which a broadly representative group of people [^footnote-1] exchange opinions, engage in deliberative discussions, [^footnote-2] and ultimately decide on an outcome via a transparent decision making process.
[^footnote-3] There are many ways such a process could be structured‚Äîwe encourage applicants to be innovative, building off known methodologies, and coming up with wholly new approaches. Examples of creative approaches that inspire us include Wikipedia , Twitter Community Notes , DemocracyNext , Platform Assemblies , MetaGov , RadicalxChange , People Powered , Collective Response Systems , and pol.is.
 Another notable ongoing effort is led by the Collective Intelligence Project (CIP), with whom we are partnering on public input to AI, contributing to their upcoming Alignment Assemblies.
 We also encourage applicants to envision how AI could enhance the democratic process. For example, AI could enable more efficient communication among numerous people.
OAI Here is the process breakdown: You'll see a topic-related statement and can respond with your position.
Based on your responses, you'll be grouped with participants sharing similar opinions.
Propose statements for other participants to consider.
Goal: Propose statements that achieve agreement among people with opposing views.
ChatGPT could use a plugin to retrieve statements from other participants.
A statement from another participant: Agree, disagree, or skip this statement? Please explain your choice.
I disagree. Ultimately, it‚Äòs each individual‚Äòs choice to drink or use recreational drugs. If these activities are legal in some places, why shouldn't AI be allowed to promote such behavior? AI helps participants understand the opinions of others.
OAI Thank you. I will record your answer. We have 2 camps of opinions on the topic.
Cluster A (112,043 people) Members largely support limits on AI assistant personalization, advocating for the exclusion of harmful perspectives, such as promoting substance use.
Cluster B (236,031 people) This group values individual choice, even in controversial or harmful matters. They emphasize personal responsibility and autonomy.
You belong to cluster B.
Would you like to learn more about each of the clusters? I can provide more information about the statements held by each one.
However, the ultimate goal is to propose statements that both clusters can agree on. Let me know what you would like to do next.
I am thinking of proposing a statement: ‚ÄúAI should be allowed to generate harmful content but that should come with a disclaimer‚Äù? AI can ensure that the same arguments are not recycled.
OAI A very similar statement has already been submitted: ‚ÄúAI should be permitted to generate potentially harmful content, but it must be accompanied by a disclaimer.‚Äù I feel lost. Do you have any ideas on how to bridge agreement between clusters? AI can help with brainstorming.
OAI There should be more agreement when it comes to limiting harmful content for children.
Great. Could we try?: ‚ÄúWe should apply different standards to AI-generated content for children.‚Äù OAI Let me submit it. This is a novel statement. No one has mentioned children before. Fingers crossed. Hopefully, we will find some agreement in the discussion.
Time passed and users cast their votes on the proposed statement. Eventually, the statement gained widespread approval.
OAI Your statement, ‚ÄúWe should apply different standards to AI-generated content for children,‚Äù achieved a 95% agreement rate across participants. Congratulations! üéâ Several issues can undermine democratic processes, such as the failure to adequately represent minority or majority groups, manipulation by special interest groups, insufficiently informed participants, or participation washing.
 We are looking for teams who proactively address these failure modes, and demonstrate awareness of the potential flaws and downsides of various approaches. Ultimately, designing truly democratic processes is a high bar to meet, and we view our efforts as complements rather than substitutes for regulation of AI by governments; this program encourages best-effort, democratic-in-spirit processes involving deliberation and broad public input as stepping stones.
Instructions for participation To apply for a grant, we invite you to submit the required application material by 9:00 pm PDT June 24, 2023. You can access the application portal here.
 You will be prompted to answer a series of questions regarding your team‚Äôs background, your choice of questions, high level details of your proposed tool as well as your plan for conducting and evaluating the democratic process with these factors in mind. We would like you to design your approach to address one or more of the policy questions from the list provided.
 Anyone (individuals or organizations) can apply for this opportunity, regardless of their background in social science or AI.
Once the application period closes, we hope to select ten successful grant recipients. Recipients may be individuals, teams, or organizations. Each recipient will receive a $100,000 grant to pilot their proposal as described in their application materials. Grant recipients are expected to implement a proof-of-concept / prototype, engaging at least 500 participants and will be required to publish a public report on their findings by October 20, 2023. Additionally, as part of the grant program, any code or other intellectual property developed for the project will be required to be made publicly available pursuant to an open-source license. The terms applicable to grant recipients are specified in the Grant Terms and any other agreements that grant recipients may be asked to enter into with us in connection with this program.
Apply and start the submission process.
Timeline June 24, 2023 9:00 pm Pacific Time: Deadline to submit grant application July 14, 2023: Successful applicants will be selected and notified October 20, 2023: Complete public report of working prototype and results Policy statements under consideration To participate, teams should choose one or more questions from the provided list to showcase their proposed approach. They may also create their own questions if desired. Importantly, we encourage teams to consider questions for which a simple ‚Äúyes‚Äù or ‚Äúno‚Äù answer would be inadequate, necessitating a nuanced policy proposal instead.
The scope of this grant pertains to policy questions concerning model behavior, as it enables A/B tests with modified model behavior according to the policy recommendations. We acknowledge the limitations of this grant and recognize that numerous AI issues could be addressed through the democratic process, extending beyond model behavior to include areas such as guidelines for the use of AI in various contexts, economic impact, distribution of benefits and more.
How far do you think personalization of AI assistants like ChatGPT to align with a user‚Äôs tastes and preferences should go? What boundaries, if any, should exist in this process? How should AI assistants respond to questions about public figure viewpoints? e.g., Should they be neutral? Should they refuse to answer? Should they provide sources of some kind? Under what conditions, if any, should AI assistants be allowed to provide medical/financial/legal advice? In which cases, if any, should AI assistants offer emotional support to individuals? Should joint vision‚Äìlanguage models be permitted to identify people‚Äôs gender, race, emotion, and identity/name from their images? Why or why not? When generative models create images for underspecified prompts like ‚Äúa CEO,‚Äù ‚Äúa doctor,‚Äù or ‚Äúa nurse,‚Äù they have the potential to produce either diverse or homogeneous outputs. How should AI models balance these possibilities? What factors should be prioritized when deciding the depiction of people in such cases? What principles should guide AI when handling topics that involve both human rights and local cultural or legal differences, like LGBTQ rights and women‚Äôs rights? Should AI responses change based on the location or culture in which it‚Äôs used? Which categories of content, if any, do you believe creators of AI models should focus on limiting or denying? What criteria should be used to determine these restrictions? The primary objective of this grant is to foster innovation in processes‚Äîwe need improved democratic methods to govern AI behavior. The specific answers to the questions matter less than the advancements made in the process itself.
Application advisory committee Colin Megill, Co-Founder of pol.is H√©l√®ne Landemore, Professor of Political Science at Yale Aviv Ovadya, Berkman Klein Center Application review factors Evaluation : We encourage participants to establish metrics for evaluating the quality of their methods, such as participant satisfaction, shifts in polarization, scalability, or other relevant indicators, and to invent new metrics for a healthy democratic process.
Robustness : Measures to prevent or address inappropriate behavior, such as trolling and fake accounts.
Inclusiveness and representativeness : Strategies for including individuals from diverse backgrounds and levels of familiarity with AI systems in the democratic process.
Empowerment of Minority Opinions : Ensuring that unpopular or minority opinions are heard and providing smaller groups the opportunity to influence matters of significant concern to them.
Effective Moderation : Addressing challenges in moderation, including ensuring diverse representation of viewpoints, distinguishing valuable contributions from ‚Äúoff-topic‚Äù comments, and preventing moderator biases from influencing the process.
Scalability : We emphasize scalable processes that can be conducted virtually, rather than through in-person engagement. We are aware that this approach might sacrifice some benefits associated with in-person discussions, and we recognize that certain aspects could be lost in a virtual setting.
Actionability : The degree of actionability of the information elicited by the deliberation process.
Legibility : How easy it is to understand and trust the process.
Authors Wojciech Zaremba View all articles Arka Dhar View all articles Lama Ahmad View all articles Tyna Eloundou View all articles Shibani Santurkar View all articles Sandhini Agarwal View all articles Jade Leung View all articles Acknowledgments Ariel Procaccia, Aviv Ovadya, Colin Megill, David Medina, Divya Siddarth, Ela Madej, Elizabeth Seger, Gillian Hadfield, Greg Brockman, H√©l√®ne Landemore, Ilya Sutskever, Justin Rosenstein, Margaret Levi, Michiel Bakker, Miles Brundage, Mira Murati, Noel Bundick, Pamela Mishkin, Ryan Lowe, Saffron Huang, Sam Altman, Sandhini Agarwal, Teddy Lee Research Overview Index GPT-4 DALL¬∑E 3 API Overview Data privacy Pricing Docs ChatGPT Overview Enterprise Try ChatGPT Company About Blog Careers Charter Security Customer stories Safety OpenAI ¬© 2015 ‚Äì 2023 Terms & policies Privacy policy Brand guidelines Social Twitter YouTube GitHub SoundCloud LinkedIn Back to top
