Open Navigation Menu To revist this article, visit My Profile, then View saved stories.
Close Alert Backchannel Business Culture Gear Ideas Science Security Merch To revist this article, visit My Profile, then View saved stories.
Close Alert Search Backchannel Business Culture Gear Ideas Science Security Merch Podcasts Video Artificial Intelligence Climate Games Newsletters Magazine Events Wired Insider Jobs Coupons Thor Benson Security This Disinformation Is Just for You Photograph: Robert Brook/Getty Images Save this story Save Save this story Save Itâ€™s now well understood that generative AI will increase the spread of disinformation on the internet. From deepfakes to fake news articles to bots, AI will generate not only more disinformation, but more convincing disinformation. But what people are only starting to understand is how disinformation will become more targeted and better able to engage with people and sway their opinions.
When Russia tried to influence the 2016 US presidential election via the now disbanded Internet Research Agency , the operation was run by humans who often had little cultural fluency or even fluency in the English language and so were not always able to relate to the groups they were targeting. With generative AI tools, those waging disinformation campaigns will be able to finely tune their approach by profiling individuals and groups. These operatives can produce content that seems legitimate and relatable to the people on the other end and even target individuals with personalized disinformation based on data theyâ€™ve collected. Generative AI will also make it much easier to produce disinformation and will thus increase the amount of disinformation thatâ€™s freely flowing on the internet, experts say.
â€œGenerative AI lowers the financial barrier for creating content thatâ€™s tailored to certain audiences,â€ says Kate Starbird, an associate professor in the Department of Human Centered Design & Engineering at the University of Washington. â€œYou can tailor it to audiences and make sure the narrative hits on the values and beliefs of those audiences, as well as the strategic part of the narrative.â€ Rather than producing just a handful of articles a day, Starbird adds, â€œYou can actually write one article and tailor it to 12 different audiences. It takes five minutes for each one of them.â€ Considering how much content people post to social media and other platforms, itâ€™s very easy to collect data to build a disinformation campaign. Once operatives are able to profile different groups of people throughout a country, they can teach the generative AI system theyâ€™re using to create content that manipulates those targets in highly sophisticated ways.
â€œYouâ€™re going to see that capacity to fine-tune. Youâ€™re going to see that precision increase. Youâ€™re going to see the relevancy increase,â€ says Renee Diresta, the technical research manager at Stanford Internet Observatory.
Business What Sam Altmanâ€™s Firing Means for the Future of OpenAI Steven Levy Business Sam Altmanâ€™s Sudden Exit Sends Shockwaves Through OpenAI and Beyond Will Knight Gear Humanityâ€™s Most Obnoxious Vehicle Gets an Electric (and Nearly Silent) Makeover Boone Ashworth Security The Startup That Transformed the Hack-for-Hire Industry Andy Greenberg Hany Farid, a professor of computer science at the University of California, Berkeley, says this kind of customized disinformation is going to be â€œeverywhere.â€ Though bad actors will probably target people by groups when waging a large-scale disinformation campaign, they could also use generative AI to target individuals.
â€œYou could say something like, â€˜Hereâ€™s a bunch of tweets from this user. Please write me something that will be engaging to them.â€™ Thatâ€™ll get automated. I think thatâ€™s probably coming,â€ Farid says.
Purveyors of disinformation will try all sorts of tactics until they find what works best, Farid says, and much of whatâ€™s happening with these disinformation campaigns likely wonâ€™t be fully understood until after theyâ€™ve been in operation for some time. Plus, they only need to be somewhat effective to achieve their aims.
â€œIf I want to launch a disinformation campaign, I can fail 99 percent of the time. You fail all the time, but it doesnâ€™t matter,â€ Farid says. â€œEvery once in a while, the QAnon gets through. Most of your campaigns can fail, but the ones that donâ€™t can wreak havoc.â€ Farid says we saw during the 2016 election cycle how the recommendation algorithms on platforms like Facebook radicalized people and helped spread disinformation and conspiracy theories. In the lead-up to the 2024 US election, Facebookâ€™s algorithmâ€”itself a form of AIâ€”will likely be recommending some AI-generated posts instead of only pushing content created entirely by human actors. Weâ€™ve reached the point where AI will be used to create disinformation that another AI then recommends to you.
â€œWeâ€™ve been pretty well tricked by very low-quality content. We are entering a period where weâ€™re going to get higher-quality disinformation and propaganda,â€ Starbird says. â€œItâ€™s going to be much easier to produce content thatâ€™s tailored for specific audiences than it ever was before. I think weâ€™re just going to have to be aware that thatâ€™s here now.â€ What can be done about this problem? Unfortunately, only so much. Diresta says people need to be made aware of these potential threats and be more careful about what content they engage with. She says youâ€™ll want to check whether your source is a website or social media profile that was created very recently, for example. Farid says AI companies also need to be pressured to implement safeguards so thereâ€™s less disinformation being created overall.
The Biden administration recently struck a deal with some of the largest AI companiesâ€”ChatGPT maker OpenAI, Google, Amazon, Microsoft, and Metaâ€”that encourages them to create specific guardrails for their AI tools, including external testing of AI tools and watermarking of content created by AI. These AI companies have also created a group focused on developing safety standards for AI tools, and Congress is debating how to regulate AI.
Despite such efforts, AI is accelerating faster than itâ€™s being reined in, and Silicon Valley often fails to keep promises to only release safe, tested products. And even if some companies behave responsibly, that doesnâ€™t mean all of the players in this space will act accordingly.
â€œThis is the classic story of the last 20 years: Unleash technology, invade everybodyâ€™s privacy, wreak havoc, become trillion-dollar-valuation companies, and then say, â€˜Well, yeah, some bad stuff happened,â€™â€ Farid says. â€œWeâ€™re sort of repeating the same mistakes, but now itâ€™s supercharged because weâ€™re releasing this stuff on the back of mobile devices, social media, and a mess that already exists.â€ You Might Also Like â€¦ ğŸ“© Get the long view on tech with Steven Levy's Plaintext newsletter Watch this guy work, and youâ€™ll finally understand the TikTok era How Telegram became a terrifying weapon in the Israel-Hamas War Inside Elon Muskâ€™s first election crisis â€”a day after he â€œfreedâ€ the bird The ultra-efficient farm of the future is in the sky The best pickleball paddles for beginners and pros ğŸŒ² Our Gear team has branched out with a new guide to the best sleeping pads and fresh picks for the best coolers and binoculars Topics disinformation artificial intelligence fake news content moderation algorithms elections Social Media Andrew Couts Andy Greenberg David Gilbert Lily Hay Newman Darren Loucaides David Gilbert Lily Hay Newman Matt Burgess Facebook X Pinterest YouTube Instagram Tiktok More From WIRED Subscribe Newsletters Mattresses Reviews FAQ Wired Staff Coupons Black Friday Editorial Standards Archive Contact Advertise Contact Us Customer Care Jobs Press Center RSS Accessibility Help CondÃ© Nast Store Do Not Sell My Personal Info Â© 2023 CondÃ© Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights.
WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of CondÃ© Nast.
Ad Choices Select international site United States LargeChevron UK Italia JapÃ³n Czech Republic & Slovakia
