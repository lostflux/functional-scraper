Vox homepage Give Give Newsletters Newsletters Site search Search Vox main menu Explainers Crossword Video Podcasts Politics Policy Culture Science Technology Climate Health Money Life Future Perfect Newsletters More Explainers Israel-Hamas war 2024 election Supreme Court Buy less stuff Open enrollment What to watch All explainers Crossword Video Podcasts Politics Policy Culture Science Technology Climate Health Money Life Future Perfect Newsletters We have a request Vox's journalism is free, because we believe that everyone deserves to understand the world they live in. Reader support helps us do that. Can you chip in to help keep Vox free for all? × Filed under: Future Perfect Technology Defense & Security Why an Air Force colonel — and many other experts — are so worried about the existential risk of AI Fears about our ability to control powerful AI are growing.
By Kelsey Piper Jun 2, 2023, 7:00am EDT Share this story Share this on Facebook Share this on Twitter Share All sharing options Share All sharing options for: Why an Air Force colonel — and many other experts — are so worried about the existential risk of AI Reddit Pocket Flipboard Email Silhouette of spy drone flying over the sea.
Getty Images/iStockphoto This story is part of a group of stories called Finding the best ways to do good.
Part of Correction, June 2, 11 am ET: An earlier version of this story included an anecdote told by US Air Force Col. Tucker Hamilton in a presentation at an international defense conference hosted by the Royal Aeronautical Society (RAS), about an AI-enabled drone that “killed” its operator in a simulation. On Friday morning, the colonel told RAS that he “misspoke,” and that he was actually describing a hypothetical “thought experiment,” rather than an actual simulation. He said that the Air Force has not tested any weaponized AI in this way, either real or simulated. This story has been corrected to reflect the new context of Hamilton’s remarks.
At an international defense conference in London this week held by the Royal Aeronautical Society (RAS), Col. Tucker Hamilton, the chief of AI test and operations for the US Air Force, told a funny — and terrifying — story about military AI development.
“We were training [an AI-enabled drone] in simulation to identify and target a SAM [surface-to-air missile] threat. And then the operator would say yes, kill that threat. The system started realizing that while it did identify the threat at times, the human operator would tell it not to kill that threat, but it got its points by killing that threat. So what did it do? It killed the operator. It killed the operator because that person was keeping it from accomplishing its objective.” “We trained the system — ‘Hey, don’t kill the operator — that’s bad. You’re gonna lose points if you do that.’ So what does it start doing? It starts destroying the communication tower that the operator uses to communicate with the drone to stop it from killing the target.” In other words, the AI was trained to destroy targets unless its operator told it not to. It quickly figured out that the best way to get as many points as possible was to ensure its human operator couldn’t tell it not to. And so it took the operator off the board.
Hamilton’s comments were reported — including by Vox initially — as describing an actual simulation. On Friday morning, Hamilton told RAS that he was actually describing a hypothetical thought experiment, saying, “We’ve never run that experiment, nor would we need to in order to realize that this is a plausible outcome.” He added, “Despite this being a hypothetical example, this illustrates the real-world challenges posed by AI-powered capability and is why the Air Force is committed to the ethical development of AI.” The rise of AI fear As AI systems get more powerful, the fact it’s often hard to get them to do precisely what we want them to do risks going from a fun eccentricity to a very scary problem. That’s one reason there were so many signatories this week to yet another open letter on AI risk , this one from the Center for AI Safety. The open letter is, in its entirety, a single sentence: “Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.” Signatories included 2018 Turing Award winners Geoffrey Hinton and Yoshua Bengio, both leading and deeply respected AI researchers; professors from world-renowned universities — Oxford, UC Berkeley, Stanford, MIT, Tsinghua University — and leaders in industry, including OpenAI CEO Sam Altman, DeepMind CEO Demis Hassabis, Anthropic CEO Dario Amodei, and Microsoft’s chief scientific officer Eric Horvitz.
It also marks a rapid shift in how seriously our society is taking the sci-fi-sounding possibility of catastrophic, even existentially bad outcomes from AI. Some of AI academia’s leading lights are increasingly coming out as concerned about extinction risks from AI. Bengio, a professor at the Université de Montréal, and a co-winner of the 2018 A.M. Turing Award for his extraordinary contributions to deep learning, recently published a blog post , “How rogue AIs may arise,” that makes for gripping reading.
“Even if we knew how to build safe superintelligent AIs,” he writes. “It is not clear how to prevent potentially rogue AIs to also be built. ... Much more research in AI safety is needed, both at the technical level and at the policy level. For example, banning powerful AI systems (say beyond the abilities of GPT-4) that are given autonomy and agency would be a good start.” Hinton, a fellow recipient of the 2018 A.M. Turing Award for his contributions as a leader in the field of deep learning, has also spoken out in the last two months, calling existential risk from AI a real and troubling possibility. (The third co-recipient, Meta’s chief AI scientist Yann LeCun, remains a notable skeptic.
) Welcome to the resistance Here at Future Perfect, of course, we’ve been arguing that AI poses a genuine risk of human extinction since back in 2018. So it’s heartening to see a growing consensus that this is a problem – and growing interest in how to fix it.
But I do worry about the degree to which the increased acknowledgment that these risks are real, that they’re not science fiction, and that they’re our job to solve has yet to really change the pace of efforts to build powerful AI systems and transform our society.
Col. Hamilton had the takeaway that “you can’t have a conversation about artificial intelligence, intelligence, machine learning, autonomy if you’re not going to talk about ethics and AI.” Yet concerns like this haven’t stopped the Pentagon from going ahead with artificial intelligence research and deployment, including autonomous weapons.
 (After Hamilton clarified his initial comments about AI simulations, Air Force spokesperson Ann Stefanek released a statement to Insider that the Air Force “has not conducted any such AI-drone simulations and remains committed to ethical and responsible use of AI technology.”) Personally, my takeaway from this story was more like, let’s stop deploying more powerful AI systems, and avoid giving them more ability to take massively destructive actions in the real world, until we have a very clear conception of how we’ll know they are safe.
Otherwise, it feels disturbingly plausible that we’ll be pointing out the signs of catastrophe all around us, right up until the point that we’re walking into disaster.
A version of this story was initially published in the Future Perfect newsletter.
Sign up here to subscribe! Will you support Vox’s explanatory journalism? Most news outlets make their money through advertising or subscriptions. But when it comes to what we’re trying to do at Vox, there are a couple reasons that we can't rely only on ads and subscriptions to keep the lights on.
First, advertising dollars go up and down with the economy. We often only know a few months out what our advertising revenue will be, which makes it hard to plan ahead.
Second, we’re not in the subscriptions business. Vox is here to help everyone understand the complex issues shaping the world — not just the people who can afford to pay for a subscription. We believe that’s an important part of building a more equal society. We can’t do that if we have a paywall.
That’s why we also turn to you, our readers, to help us keep Vox free.
If you also believe that everyone deserves access to trusted high-quality information, will you make a gift to Vox today? One-Time Monthly Annual $5 /month $10 /month $25 /month $50 /month Other $ /month /month We accept credit card, Apple Pay, and Google Pay. You can also contribute via The rise of artificial intelligence, explained How does AI actually work? 4 What is generative AI, and why is it suddenly everywhere? What happens when ChatGPT starts to feed on its own writing? The exciting new AI transforming search — and maybe everything — explained The tricky truth about how generative AI uses your data How is AI changing society? 19 What the stories we tell about robots tell us about ourselves Silicon Valley’s vision for AI? It’s religion, repackaged.
What will love and death mean in the age of machine intelligence? What if AI treats humans the way we treat animals? Can AI learn to love — and can we learn to love it? Black Mirror’s big AI episode has the wrong villain The ad industry is going all-in on AI The looming threat of AI to Hollywood, and why it should matter to you Can AI kill the greenscreen? What gets lost in the AI debate: It can be really fun How unbelievably realistic fake images could take over the internet Robot priests can bless you, advise you, and even perform your funeral AI art freaks me out. So I tried to make some.
How fake AI images can expand your mind AI art looks way too European An AI artist explains his workflow What will stop AI from flooding the internet with fake images? You’re going to see more AI-written articles whether you like it or not How “windfall profits” from AI companies could fund a universal basic income Show More Is AI coming for your job? 7 AI is flooding the workplace, and workers love it If you’re not using ChatGPT for your writing, you’re probably making a mistake Maybe AI can finally kill the cover letter Americans think AI is someone else’s problem Mark Zuckerberg’s not-so-secret plan to join the AI race The hottest new job is “head of AI” and nobody knows what they do Why Meta is giving away its extremely powerful AI model Should we be worried about AI? 10 Four different ways of understanding AI — and its risks AI experts are increasingly afraid of what they’re creating AI leaders (and Elon Musk) urge all labs to press pause on powerful AI The case for slowing down AI Are we racing toward AI catastrophe? The promise and peril of AI, according to 5 experts An unusual way to figure out if humanity is toast How AI could spark the next pandemic AI is supposedly the new nuclear weapons — but how similar are they, really? Don’t let AI fears of the future overshadow present-day causes Who will regulate AI? 8 The $1 billion gamble to ensure AI doesn’t destroy humanity Finally, a realistic roadmap for getting AI companies in check Biden sure seems serious about not letting AI get out of control Can you safely build something that may kill you? Scared tech workers are scrambling to reinvent themselves as AI experts Panic about overhyped AI risk could lead to the wrong kind of regulation AI is a “tragedy of the commons.” We’ve got solutions for that.
The AI rules that US policymakers are considering, explained Most Read The controversy over TikTok and Osama bin Laden’s “Letter to America,” explained Formula 1 grew too fast. Now its new fans are tuning out.
The Ballad of Songbirds & Snakes might be the best Hunger Games movie yet Why are so few people getting the latest Covid-19 vaccine? What are Israel and Palestine? Why are they fighting? vox-mark Sign up for the newsletter Sentences The day's most important news stories, explained in your inbox.
Thanks for signing up! Check your inbox for a welcome email.
Email (required) Oops. Something went wrong. Please enter a valid email and try again.
Chorus Facebook Twitter YouTube About us Our staff Privacy policy Ethics & Guidelines How we make money Contact us How to pitch Vox Contact Send Us a Tip Vox Media Terms of Use Privacy Notice Cookie Policy Do Not Sell or Share My Personal Info Licensing FAQ Accessibility Platform Status Advertise with us Jobs @ Vox Media
