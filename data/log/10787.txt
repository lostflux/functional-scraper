CEO of DeepMind is ‘not a pessimist’ but warns of threat from AI and says we must be active in shaping ‘a middle way’ "https://www.theguardian.com/technology/2023/oct/24/ai-risk-climate-crisis-google-deepmind-chief-demis-hassabis-regulation\">AI risk must be treated as seriously as climate crisis, says Google DeepMind chief US edition US edition UK edition Australia edition International edition Europe edition The Guardian - Back to home The Guardian News Opinion Sport Culture Lifestyle Show More Show More document.addEventListener('DOMContentLoaded', function(){ var columnInput = document.getElementById('News-button'); if (!columnInput) return; // Sticky nav replaces the nav so element no longer exists for users in test. columnInput.addEventListener('keydown', function(e){ // keyCode: 13 => Enter key | keyCode: 32 => Space key if (e.keyCode === 13 || e.keyCode === 32) { e.preventDefault() document.getElementById('News-checkbox-input').click(); } }) }) News View all News US news World news Environment US politics Ukraine Soccer Business Tech Science Newsletters Wellness document.addEventListener('DOMContentLoaded', function(){ var columnInput = document.getElementById('Opinion-button'); if (!columnInput) return; // Sticky nav replaces the nav so element no longer exists for users in test. columnInput.addEventListener('keydown', function(e){ // keyCode: 13 => Enter key | keyCode: 32 => Space key if (e.keyCode === 13 || e.keyCode === 32) { e.preventDefault() document.getElementById('Opinion-checkbox-input').click(); } }) }) Opinion View all Opinion The Guardian view Columnists Letters Opinion videos Cartoons document.addEventListener('DOMContentLoaded', function(){ var columnInput = document.getElementById('Sport-button'); if (!columnInput) return; // Sticky nav replaces the nav so element no longer exists for users in test. columnInput.addEventListener('keydown', function(e){ // keyCode: 13 => Enter key | keyCode: 32 => Space key if (e.keyCode === 13 || e.keyCode === 32) { e.preventDefault() document.getElementById('Sport-checkbox-input').click(); } }) }) Sport View all Sport Soccer NFL Tennis MLB MLS NBA NHL F1 Golf document.addEventListener('DOMContentLoaded', function(){ var columnInput = document.getElementById('Culture-button'); if (!columnInput) return; // Sticky nav replaces the nav so element no longer exists for users in test. columnInput.addEventListener('keydown', function(e){ // keyCode: 13 => Enter key | keyCode: 32 => Space key if (e.keyCode === 13 || e.keyCode === 32) { e.preventDefault() document.getElementById('Culture-checkbox-input').click(); } }) }) Culture View all Culture Film Books Music Art & design TV & radio Stage Classical Games document.addEventListener('DOMContentLoaded', function(){ var columnInput = document.getElementById('Lifestyle-button'); if (!columnInput) return; // Sticky nav replaces the nav so element no longer exists for users in test. columnInput.addEventListener('keydown', function(e){ // keyCode: 13 => Enter key | keyCode: 32 => Space key if (e.keyCode === 13 || e.keyCode === 32) { e.preventDefault() document.getElementById('Lifestyle-checkbox-input').click(); } }) }) Lifestyle View all Lifestyle Wellness Fashion Food Recipes Love & sex Home & garden Health & fitness Family Travel Money Search input google-search Search Support us Print subscriptions document.addEventListener('DOMContentLoaded', function(){ var columnInput = document.getElementById('US-edition-button'); if (!columnInput) return; // Sticky nav replaces the nav so element no longer exists for users in test. columnInput.addEventListener('keydown', function(e){ // keyCode: 13 => Enter key | keyCode: 32 => Space key if (e.keyCode === 13 || e.keyCode === 32) { e.preventDefault() document.getElementById('US-edition-checkbox-input').click(); } }) }) US edition UK edition Australia edition International edition Europe edition Search jobs Digital Archive Guardian Puzzles app Guardian Licensing The Guardian app Video Podcasts Pictures Inside the Guardian Guardian Weekly Crosswords Wordiply Corrections Facebook Twitter Search jobs Digital Archive Guardian Puzzles app Guardian Licensing US World Environment US Politics Ukraine Soccer Business Tech Science Newsletters Wellness An AI hand reaches out to touch the earth transforming it into pixelated data.
Composite: Guardian Design/Getty Images An AI hand reaches out to touch the earth transforming it into pixelated data.
Composite: Guardian Design/Getty Images The AI race Technology Hope or horror? The great AI debate dividing its pioneers Global technology editor CEO of DeepMind is ‘not a pessimist’ but warns of threat from AI and says we must be active in shaping ‘a middle way’ AI risk must be treated as seriously as climate crisis, says Google DeepMind chief Tue 24 Oct 2023 08.00 EDT Demis Hassabis says he is not in the “pessimistic” camp about artificial intelligence. But that did not stop the CEO of Google DeepMind signing a statement in May warning that the threat of extinction from AI should be treated as a societal risk comparable to pandemics or nuclear weapons.
That uneasy gap between hope and horror, and the desire to bridge it, is a key reason why Rishi Sunak convened next week’s global AI safety summit in Bletchley Park, a symbolic choice as the base of the visionary codebreakers – including computing pioneer Alan Turing – who deciphered German communications during the second world war.
“I am not in the pessimistic camp about AI obviously, otherwise I wouldn’t be working on it,” Hassabis tells the Guardian in an interview at Google DeepMind’s base in King’s Cross, London.
“But I’m not in the ‘there’s nothing to see here and nothing to worry about’ [camp]. It’s a middle way. This can go well but we’ve got to be active about shaping that.” Hassabis, a 47-year-old Briton, co-founded UK company DeepMind in 2010. It was bought by Google in 2014 and has achieved stunning breakthroughs in AI under his leadership. The company is now known as Google DeepMind after merging with the search firm’s other AI operations, with Hassabis at the helm as CEO.
His unit is behind the AlphaFold program that can predict the 3D shapes of proteins in the human body – as well as nearly all catalogued proteins known to science. This is a revolutionary achievement that will help achieve breakthroughs in areas such as discovering new medicines because it maps out the biological building blocks of life. This year Hassabis was jointly awarded one of the most prestigious prizes in science , the Lasker basic medical research award, for the work on AlphaFold. Many winners of the award go on to win a Nobel prize.
Last month Hassabis’ team released AlphaMissense, which uses the same AI protein program to spot protein malformations that could cause disease.
Hassabis says he would have preferred the May statement to contain references to AI’s potential benefits. “I would have had a line saying about all the incredible opportunities that AI is going to bring: medicine, science, all the things help in everyday life, assisting in everyday life.” He says AI advances will trigger “disruption” in the jobs market –skilled professions such as law, medicine and finance are at risk, according to experts – but he says the impact will be “positive overall” as the economy adapts. This has also led to talk among AI professionals of the technology funding a universal basic income or even a universal basic service, which provides services such as transport and accommodation for free.
“Some kind of sharing of the upsides would be needed in some form,” says Hassabis.
Q&A 'What is the AI race series?' Show Growing alarm about the threats posed by uncontrolled innovation in artificial intelligence has prompted global leaders to hold the first ever safety summit. Hosted by the UK on 1 and 2 November, the biggest names and the biggest companies at the frontier of AI will discuss what – if anything – can be done to regulate the technology.
Ahead of the summit, the Guardian is looking at different aspects of AI, how it already affects our lives – and what might come next.
But the OECD, an influential international organisation, says jobs at the highest risk from AI-driven automation are highly skilled and represent about 27% of employment across its 38 member countries , which include the UK, Japan, Germany, the US, Australia and Canada. No wonder the OECD talks of an “AI revolution which could fundamentally change the workplace”.
Nonetheless, the summit will focus on threats from frontier AI, the term for cutting-edge systems that could cause significant loss of life. These include the ability to make bioweapons, create sophisticated cyber-attacks and to evade human control. The latter issue refers to fears about artificial general intelligence, or “god-like” AI, meaning a system that operates with above or beyond human levels of intelligence.
The pessimistic camp that voices these fears has strong credentials. Geoffrey Hinton, a British computer scientist often described as one of the “godfathers” of modern AI, quit his job at Google this year in order to voice his fears about the technology more freely.
Hinton told the Guardian in May of his concerns that AI firms are trying to build intelligences with the potential to outthink humanity.
“My confidence that this wasn’t coming for quite a while has been shaken by the realisation that biological intelligence and digital intelligence are very different, and digital intelligence is probably much better.” Stuart Russell, another senior British computer scientist, has warned of a scenario where the UN asks an AI system to create a self-multiplying catalyst to de-acidify the oceans, with the safety instruction that the outcome is non-toxic and that no fish are harmed. But the result uses up a quarter of the oxygen in the atmosphere and subjects humans to a slow and painful death.
Both Hinton and Russell are attending the summit along with Hassabis, world politicians, other tech CEOs and civil society figures.
Referring to AGI, Hassabis says “we’re a long time before the systems become anywhere on the horizon” but says future generation systems will carry risks. Hence the summit.
Critics of the summit argue that the focus on existential risk ignores short-term problems such as deepfakes.
The government seems to have acknowledged the immediate concerns, with the agenda for the summit referring to election disruption and AI tools producing biased outcomes. Hassabis argues that there are three categories of risk and all “equally important” and need to be worked on simultaneously.
The first is near-term risks such as deepfakes and bias. “Those types of issues … obviously are very pressing issues, especially with elections next year,” he said. “So there … we need solutions now.” Google DeepMind has already launched a tool that watermarks AI-generated images.
The second risk category is rogue actors accessing AI tools, via publicly available and adjustable systems known as open source models, and using them to cause harm.
“How does one restrict access to bad actors, but somehow enable all the good use cases? That’s a big debate.” The third is AGI, which is no longer discussed as a fantastical possibility. Hassabis says super powerful systems could be a “decade away plus” but the thinking on controlling them needs to start immediately.
There are also alternative views in this field. Yann LeCun, the chief AI scientist at Mark Zuckerberg’s Meta and a respected figure in AI, said last week that fears AI could wipe out humanity were “preposterous”.
Nonetheless, a concern among those worried about superintelligent systems is the notion that they could evade control.
“Can it exfiltrate its own code, can extract its own code, improve its own code,” says Hassabis. “Can it copy itself unauthorised? Because these would all be undesirable behaviours, because if you want to shut it down, you don’t want it getting around that by copying itself somewhere else. There’s a lot of behaviours like that, that would be undesirable in a powerful system.” He said tests would have to be designed to head off the threat of such autonomous behaviour. “You’ve got to actually develop a test to test that … and then you can mitigate it, and maybe even legislate against it at some point. But the research has to be done first.” The situation is further complicated by the fact that highly capable generative AI tools – technology that produces plausible text, image and voice from simple human prompts – are already out there and the regulatory framework to regulate them is still being built.
Signs of a framework are emerging, such as commitments to AI safety signed by major western tech firms at the White House in July. But the commitments are voluntary.
Hassabis talks of starting with an IPCC-style body before moving eventually to an entity “equivalent to” the anti-nuclear proliferation International Atomic Energy Agency, although he stresses that none of the regulatory analogies are “directly applicable” to AI. This is new territory.
If you are in the pessimistic camp, it could take years to build a solid regulatory framework. And as Hassabis says, work on safety needs to start “yesterday”.
Explore more on these topics Technology The AI race Artificial intelligence (AI) Google Computing analysis More on this story More on this story Sam Altman ‘was working on new venture’ before sacking from OpenAI 5h ago John Legend and Sia among singers to trial AI versions of voices with YouTube 3d ago Like horses laid off by the car: BT tech chief’s AI job losses analogy draws anger 9 Nov 2023 AI could cause ‘catastrophic’ financial crisis, says Yuval Noah Harari 9 Nov 2023 ‘A kind of magic’: Peter Blake says possibilities of AI are endless for art 5 Nov 2023 Elon Musk unveils Grok, an AI chatbot with a ‘rebellious streak’ 5 Nov 2023 No utopia: experts question Elon Musk’s vision of world without work 3 Nov 2023 ‘Bletchley made me more optimistic’: how experts reacted to AI summit 3 Nov 2023 AI could pose risk to humanity on scale of nuclear war, Sunak warns 2 Nov 2023 When Musk met Sunak: the prime minister was more starry-eyed than a SpaceX telescope 3 Nov 2023 … … Most viewed Most viewed US World Environment US Politics Ukraine Soccer Business Tech Science Newsletters Wellness News Opinion Sport Culture Lifestyle About us Help Complaints & corrections SecureDrop Work for us Privacy policy Cookie policy Terms & conditions Contact us All topics All writers Digital newspaper archive Facebook YouTube Instagram LinkedIn Twitter Newsletters Advertise with us Guardian Labs Search jobs Back to top
