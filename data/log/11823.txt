Artificial Intelligence View All AI, ML and Deep Learning Auto ML Data Labelling Synthetic Data Conversational AI NLP Text-to-Speech Security View All Data Security and Privacy Network Security and Privacy Software Security Computer Hardware Security Cloud and Data Storage Security Data Infrastructure View All Data Science Data Management Data Storage and Cloud Big Data and Analytics Data Networks Automation View All Industrial Automation Business Process Automation Development Automation Robotic Process Automation Test Automation Enterprise Analytics View All Business Intelligence Disaster Recovery Business Continuity Statistical Analysis Predictive Analysis More Data Decision Makers Virtual Communication Team Collaboration UCaaS Virtual Reality Collaboration Virtual Employee Experience Programming & Development Product Development Application Development Test Management Development Languages Intel details Nervana, a neural network chip for inference-based workloads (Updated) Share on Facebook Share on X Share on LinkedIn Intel Nervana Are you ready to bring more awareness to your brand? Consider becoming a sponsor for The AI Impact Tour. Learn more about the opportunities here.
At a press event at the 2019 Consumer Electronics Show, Intel announced the Nervana Neural Network Processor (NNP-I), an AI chip for inference-based workloads that fits into a GPU-like form factor. It wasn’t an unexpected reveal — Intel announced it was working on a new generation of inference chip way back in 2017 — but its appearance at the press conference today made clear the company’s ambition to capture a large slice of the budding AI chip market.
NNP-I is built on a 10-nanometer Intel process, and will include Ice Lake cores to handle general operations as well as neural network acceleration, Naveen Rao, corporate vice president and general manager of AI at Intel, said in a tweet.
Nervana is optimized for image recognition, Intel said onstage. And it has an architecture distinct from other chips: It lacks a standard cache hierarchy, and on-chip memory is managed by software directly. Additionally, because of its high-speed on- and off-chip interconnects, it’s able to distribute neural network parameters across multiple chips, achieving very high parallelism.
Nervana also uses a new numeric format — Flexpoint — that allows the sort of scalar computations central to neural network inference to be implemented as fixed-point multiplications and additions, resulting in an even greater increase in parallelism while improving power efficiency.
VB Event The AI Impact Tour Connect with the enterprise AI community at VentureBeat’s AI Impact Tour coming to a city near you! “This makes multiple chips act as one large virtual chip that can accommodate larger models, allowing customers to capture more insight from their data,” Intel explained in a 2017 blog post. “Better memory management enables the chip to achieve high levels of utilization of the massive amount of compute on each die. This translates to achieving faster training time for Deep Learning models.” And as previously revealed, Intel said it’s working with Facebook as a development partner.
“Facebook is pleased to be partnering with Intel on a new generation of power-optimized, highly tuned AI inference chip that will be a leap in inference workload acceleration,” Facebook said in a statement.
Navin Shenoy, Intel executive vice president in the Data Center Group, said Nervana will go into production this year. He also said that Intel expects to have a neural network processor for training, code-named “Spring Crest,” available later this year.
For Intel, Nervana is yet another step toward its ambitious goal of capturing the $200 billion AI market.
In August, it bought Vertex.ai , a startup developing a platform-agnostic AI model suite, for an undisclosed amount. Meanwhile, the chipmaker’s 2015 acquisition of Altera brought field-programmable gate array (an integrated, reconfigurable circuit) into its product lineup, and its purchases of Movidius and Nervana bolstered its real-time processing portfolio.
Of note, Nervana’s neural network processor — the processor announced today — can reportedly deliver up to 10 times the AI training performance of competing graphics cards.
“After 50 years, this is the biggest opportunity for the company,” Shenoy said at the company’s Data Centric Innovation Summit this year. “We have 20 percent of this market today … Our strategy is to drive a new era of data center technology.” Updated on January 8 at 2:10 p.m. Pacific: Updated with additional architectural details from Intel vice president and general manager of AI Naveen Rao.
VentureBeat's mission is to be a digital town square for technical decision-makers to gain knowledge about transformative enterprise technology and transact.
Discover our Briefings.
The AI Impact Tour Join us for an evening full of networking and insights at VentureBeat's AI Impact Tour, coming to San Francisco, New York, and Los Angeles! VentureBeat Homepage Follow us on Facebook Follow us on X Follow us on LinkedIn Follow us on RSS Press Releases Contact Us Advertise Share a News Tip Contribute to DataDecisionMakers Careers Privacy Policy Terms of Service Do Not Sell My Personal Information © 2023 VentureBeat.
 All rights reserved.
