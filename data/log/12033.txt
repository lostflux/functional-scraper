Artificial Intelligence View All AI, ML and Deep Learning Auto ML Data Labelling Synthetic Data Conversational AI NLP Text-to-Speech Security View All Data Security and Privacy Network Security and Privacy Software Security Computer Hardware Security Cloud and Data Storage Security Data Infrastructure View All Data Science Data Management Data Storage and Cloud Big Data and Analytics Data Networks Automation View All Industrial Automation Business Process Automation Development Automation Robotic Process Automation Test Automation Enterprise Analytics View All Business Intelligence Disaster Recovery Business Continuity Statistical Analysis Predictive Analysis More Data Decision Makers Virtual Communication Team Collaboration UCaaS Virtual Reality Collaboration Virtual Employee Experience Programming & Development Product Development Application Development Test Management Development Languages Intel plans to ship its first-generation Neural Network Processor by the end of the year Share on Facebook Share on X Share on LinkedIn Are you ready to bring more awareness to your brand? Consider becoming a sponsor for The AI Impact Tour. Learn more about the opportunities here.
Intel’s hardware for accelerating AI computation is finally on its way to customers. The company announced today that its first-generation Neural Network Processor , code named “Lake Crest,” will be rolling out to a small set of partners soon to help them drastically accelerate how much machine learning work they can do.
The NNPs are designed to very quickly tackle the math that underpins artificial intelligence applications, specifically neural networks, a currently popular branch of machine learning. One of the big problems with the large, deep neural networks that are popular right now is that they can be very computationally intensive, which makes them harder to test and deploy rapidly.
At first, the NNPs will only get released to a small number of Intel partners which the company plans to begin outfitting before the end of this year. The hardware is being developed in close collaboration with Facebook, one of the companies that’s trying to push the boundaries on rapid development and testing of neural nets.
Customers will be able to access the NNPs through Intel’s Nervana Cloud service, though the company plans to make the hardware more available in the future, according to Naveen Rao, the vice president and general manager of Intel’s AI products group.
VB Event The AI Impact Tour Connect with the enterprise AI community at VentureBeat’s AI Impact Tour coming to a city near you! Observers should expect rapid iteration on the new silicon, with a faster release cadence than some of Intel’s other products. Rao said that the current fast-moving nature of the AI field means that customers want new neural network chips with new capabilities as quickly as possible, in contrast to the stability needs for CPUs and other hardware.
“When you’re working with a CPU, there are a lot of expectations put on a CPU,” Rao said in an interview. “And we’re very thoughtful about additions and changes to the CPU architecture. When you’re in a vastly changing field like neural networks, it’s valued more to iterate quickly.” Right now there are three generations of the silicon currently in flight at Intel, and the company plans to hit at least a yearly cadence for its hardware releases.
VentureBeat's mission is to be a digital town square for technical decision-makers to gain knowledge about transformative enterprise technology and transact.
Discover our Briefings.
The AI Impact Tour Join us for an evening full of networking and insights at VentureBeat's AI Impact Tour, coming to San Francisco, New York, and Los Angeles! VentureBeat Homepage Follow us on Facebook Follow us on X Follow us on LinkedIn Follow us on RSS Press Releases Contact Us Advertise Share a News Tip Contribute to DataDecisionMakers Careers Privacy Policy Terms of Service Do Not Sell My Personal Information © 2023 VentureBeat.
 All rights reserved.
