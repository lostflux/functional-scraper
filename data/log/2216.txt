Open Navigation Menu To revist this article, visit My Profile, then View saved stories.
Close Alert Backchannel Business Culture Gear Ideas Science Security Merch To revist this article, visit My Profile, then View saved stories.
Close Alert Search Backchannel Business Culture Gear Ideas Science Security Merch Podcasts Video Artificial Intelligence Climate Games Newsletters Magazine Events Wired Insider Jobs Coupons Will Knight Business Six Months Ago Elon Musk Called for a Pause on AI. Instead Development Sped Up Photograph: Win McNamee/Getty Images Save this story Save Save this story Save Six months ago this week, many prominent AI researchers, engineers, and entrepreneurs signed an open letter calling for a six-month pause on development of AI systems more capable than OpenAIâ€™s latest GPT-4 language generator. It argued that AI is advancing so quickly and unpredictably that it could eliminate countless jobs, flood us with disinformation, andâ€” as a wave of panicky headlines reported â€”destroy humanity. Whoops! As you may have noticed, the letter did not result in a pause in AI development, or even a slow down to a more measured pace. Companies have instead accelerated their efforts to build more advanced AI.
Elon Musk, one of the most prominent signatories, didnâ€™t wait long to ignore his own call for a slowdown. In July he announced xAI , a new company he said would seek to go beyond existing AI and compete with OpenAI, Google, and Microsoft. And many Google employees who also signed the open letter have stuck with their company as it prepares to release an AI model called Gemini , which boasts broader capabilities than OpenAIâ€™s GPT-4.
WIRED reached out to more than a dozen signatories of the letter to ask what effect they think it had and whether their alarm about AI has deepened or faded in the past six months. None who responded seemed to have expected AI research to really grind to a halt.
â€œI never thought that companies were voluntarily going to pause,â€ says Max Tegmark, an astrophysicist at MIT who leads the Future of Life Institute, the organization behind the letterâ€”an admission that some might argue makes the whole project look cynical. Tegmark says his main goal was not to pause AI but to legitimize conversation about the dangers of the technology, up to and including the fact that it might turn on humanity. The result â€œexceeded my expectations,â€ he says.
The responses to my follow-up also show the huge diversity of concerns experts have about AIâ€”and that many signers arenâ€™t actually obsessed with existential risk.
Lars Kotthoff , an associate professor at the University of Wyoming, says he wouldnâ€™t sign the same letter today because many who called for a pause are still working to advance AI. â€œIâ€™m open to signing letters that go in a similar direction, but not exactly like this one,â€ Kotthoff says. He adds that what concerns him most today is the prospect of a â€œsocietal backlash against AI developments, which might precipitate another AI winterâ€ by quashing research funding and making people spurn AI products and tools.
Having signed the letter, what have I done for the last year or so? Iâ€™ve been doing AI research.
Stephen Mander, Lancaster University Other signers told me they would gladly sign again, but their big worries seem to involve near-term problems, such as disinformation and job losses , rather than Terminator scenarios.
â€œIn the age of the internet and Trump, I can more easily see how AI can lead to destruction of human civilization by distorting information and corrupting knowledge,â€ says Richard Kiehl , a professor working on microelectronics at Arizona State University.
Business What Sam Altmanâ€™s Firing Means for the Future of OpenAI Steven Levy Business Sam Altmanâ€™s Sudden Exit Sends Shockwaves Through OpenAI and Beyond Will Knight Gear Humanityâ€™s Most Obnoxious Vehicle Gets an Electric (and Nearly Silent) Makeover Boone Ashworth Security The Startup That Transformed the Hack-for-Hire Industry Andy Greenberg â€œAre we going to get Skynet thatâ€™s going to hack into all these military servers and launch nukes all over the planet? I really donâ€™t think so,â€ says Stephen Mander , a PhD student working on AI at Lancaster University in the UK. He does see widespread job displacement looming, however, and calls it an â€œexistential riskâ€ to social stability. But he also worries that the letter may have spurred more people to experiment with AI and acknowledges that he didnâ€™t act on the letterâ€™s call to slow down. â€œHaving signed the letter, what have I done for the last year or so? Iâ€™ve been doing AI research,â€ he says.
Despite the letterâ€™s failure to trigger a widespread pause, it did help propel the idea that AI could snuff out humanity into a mainstream topic of discussion. It was followed by a public statement signed by the leaders of OpenAI and Googleâ€™s DeepMind AI division that compared the existential risk posed by AI to that of nuclear weapons and pandemics. Next month, the British government will host an international â€œAI safetyâ€ conference , where leaders from numerous countries will discuss possible harms AI could cause, including existential threats.
Perhaps AI doomers hijacked the narrative with the pause letter, but the unease around the recent, rapid progress in AI is real enoughâ€”and understandable. A few weeks before the letter was written, OpenAI had released GPT-4, a large language model that gave ChatGPT new power to answer questions and caught AI researchers by surprise.
 As the potential of GPT-4 and other language models has become more apparent, surveys suggest that the public is becoming more worried than excited about AI technology. The obvious ways these tools could be misused is spurring regulators around the world into action.
The letterâ€™s demand for a six-month moratorium on AI development may have created the impression that its signatories expected bad things to happen soon. But for many of them, a key theme seems to be uncertaintyâ€”around how capable AI actually is, how rapidly things may change, and how the technology is being developed.
â€œMâ€‹â€‹any AI skeptics want to hear a concrete doom scenario, but to me, the fact that it is difficult to imagine detailed, concrete scenarios is kind of the pointâ€”it shows how hard it is for even world-class AI experts to predict the future of AI and how it will impact a complex worldâ€ says Scott Niekum , a professor at the University of Massachusetts Amherst who works on AI risk and signed the letter. â€œAnd when you combine that prediction difficulty with lagging progress in safety, interpretability, and regulation, I think that should raise some alarms.â€ Uncertainty is hardly proof that humanity is in danger. But the fact that so many people working in AI still seem unsettled may be reason enough for the companies developing AI to take a more thoughtfulâ€”or slowerâ€”approach.
â€œMany people who would be in a great position to take advantage of further progress would now instead prefer to see a pause,â€ says signee Vincent Conitzer , a professor who works on AI at CMU. â€œIf nothing else, that should be a signal that something very unusual is up.â€ You Might Also Like â€¦ ğŸ“¨ Make the most of chatbots with our AI Unlocked newsletter Taylor Swift, Star Wars, Stranger Things , and Deadpool have one man in common Generative AI is playing a surprising role in Israel-Hamas disinformation The new era of social media looks as bad for privacy as the last one Johnny Cashâ€™s Taylor Swift cover predicts the boring future of AI music Your internet browser does not belong to you ğŸ”Œ Charge right into summer with the best travel adapters , power banks , and USB hubs Senior Writer X Topics Fast Forward artificial intelligence ethics machine learning Elon Musk ChatGPT Alphabet Will Knight Matt Burgess Steven Levy Will Knight Reece Rogers Vittoria Elliott Will Knight Peter Guest Facebook X Pinterest YouTube Instagram Tiktok More From WIRED Subscribe Newsletters Mattresses Reviews FAQ Wired Staff Coupons Black Friday Editorial Standards Archive Contact Advertise Contact Us Customer Care Jobs Press Center RSS Accessibility Help CondÃ© Nast Store Do Not Sell My Personal Info Â© 2023 CondÃ© Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights.
WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of CondÃ© Nast.
Ad Choices Select international site United States LargeChevron UK Italia JapÃ³n Czech Republic & Slovakia
