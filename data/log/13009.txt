Artificial Intelligence View All AI, ML and Deep Learning Auto ML Data Labelling Synthetic Data Conversational AI NLP Text-to-Speech Security View All Data Security and Privacy Network Security and Privacy Software Security Computer Hardware Security Cloud and Data Storage Security Data Infrastructure View All Data Science Data Management Data Storage and Cloud Big Data and Analytics Data Networks Automation View All Industrial Automation Business Process Automation Development Automation Robotic Process Automation Test Automation Enterprise Analytics View All Business Intelligence Disaster Recovery Business Continuity Statistical Analysis Predictive Analysis More Data Decision Makers Virtual Communication Team Collaboration UCaaS Virtual Reality Collaboration Virtual Employee Experience Programming & Development Product Development Application Development Test Management Development Languages Why AI will need more emotional intelligence Share on Facebook Share on X Share on LinkedIn Roman the digital human from Soul Machines Are you ready to bring more awareness to your brand? Consider becoming a sponsor for The AI Impact Tour. Learn more about the opportunities here.
Emotional intelligence (EQ) — the ability to pick up on what other people are feeling or thinking, primarily using body language and tone of voice — is a difficult endeavor, even for some humans. When a human misreads their fellow human, they could lose a friendship, a relationship, a job. Stakes are even higher for bots — if their makers can’t teach them empathy, they could cease to exist.
Naveen Joshi, the founder and CEO of enterprise development company Allerin, recently wrote about how EQ will make all the difference in whether AI becomes more widely used by society. “Even the most sophisticated AI technologies lack essential factors like emotional intelligence and the ability to contextualize information like human beings,” he wrote, nailing the basic stumbling block.
Bots like Alexa don’t actually know us. They don’t know how we’re feeling, or what we are thinking. They can’t pick up on the unspoken gestures and frowns. They lack even basic empathy, essentially communicating only in trivia and small talk.
The curious thing about this is that it’s not obvious. When we talk to Alexa, we tend to see the bot as another human, someone who lives inside a small speaker. The bot talks, it tells jokes. Part of the reason we don’t want to think too hard about EQ with bots is that there’s a bit of an “uncanny valley” for AI, that awkward gap where our minds essentially make up the difference between what is obviously a set of algorithms and something that seems more human. We bridge that gap mentally, but as bots evolve and get smarter and show more emotion, we’ll actually start questioning them more — we’ll start realizing they are not human.
VB Event The AI Impact Tour Connect with the enterprise AI community at VentureBeat’s AI Impact Tour coming to a city near you! The “uncanny valley” is a term used to describe what happens when we see a human avatar. It’s hard to bridge that divide — the more the avatar looks human, the more we start filling in gaps, until at some point we realize it is not human at all. That’s when things start falling apart.
Think of the most recent Final Fantasy movie, called Kingsglaive: Final Fantasy XV.
 At first, it’s astounding how much the digital actors look like humans. Then there’s a slight misalignment, or a facial twitch, or a squint that doesn’t look quite right. I never finished the movie because eventually I stopped believing it was real and I stopped caring about these digital actors.
This will happen with bots. First, we’ll stop seeing them as digital creations and start connecting to them emotionally. But they will always be subroutines on top of subroutines. At some point, we’ll stop bridging the gap between ourselves and Alexa or Cortana. This is where things will become the most interesting, because bot developers will have to figure out how to solve the massive problem of understanding you, the user. Are you sick? In a bad mood? Recently broken up with a boyfriend? Tired? If the bot doesn’t know how to read you, we won’t think of the bot as valuable. “Alexa, how is the weather?” works fine for now, but soon we will want a lot more.
This valley — the rising programmatic accomplishments mirrored by our eventual mistrust as bots seem more and more human — is the single greatest challenge AI developers face. That’s because humans can’t trust things that do not show empathy. It’s not possible. It goes against our nature. We don’t last long in a job, a friendship, or any relationship that is not built on trust and empathy. And we won’t rely more and more on a bot unless it seeks to understand us and demonstates that it “knows” us.
The worst part? We don’t know when this split will occur. For now, bots are mindless minions that do our bidding. Google Home is a sidekick that tells us NFL scores. But when we want to send a bot on an errand to pick up the kids in an autonomous car? When the bot will fill in for us in an interview? When we want a bot that cares for an elderly person? The AI of the not-so-distant future had better be ready to tackle more complex challenges than simply looking up the weather.
VentureBeat's mission is to be a digital town square for technical decision-makers to gain knowledge about transformative enterprise technology and transact.
Discover our Briefings.
The AI Impact Tour Join us for an evening full of networking and insights at VentureBeat's AI Impact Tour, coming to San Francisco, New York, and Los Angeles! VentureBeat Homepage Follow us on Facebook Follow us on X Follow us on LinkedIn Follow us on RSS Press Releases Contact Us Advertise Share a News Tip Contribute to DataDecisionMakers Careers Privacy Policy Terms of Service Do Not Sell My Personal Information © 2023 VentureBeat.
 All rights reserved.
