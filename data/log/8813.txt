Artificial Intelligence View All AI, ML and Deep Learning Auto ML Data Labelling Synthetic Data Conversational AI NLP Text-to-Speech Security View All Data Security and Privacy Network Security and Privacy Software Security Computer Hardware Security Cloud and Data Storage Security Data Infrastructure View All Data Science Data Management Data Storage and Cloud Big Data and Analytics Data Networks Automation View All Industrial Automation Business Process Automation Development Automation Robotic Process Automation Test Automation Enterprise Analytics View All Business Intelligence Disaster Recovery Business Continuity Statistical Analysis Predictive Analysis More Data Decision Makers Virtual Communication Team Collaboration UCaaS Virtual Reality Collaboration Virtual Employee Experience Programming & Development Product Development Application Development Test Management Development Languages Google Brain’s XLNet bests BERT at 20 NLP tasks Share on Facebook Share on X Share on LinkedIn Google AI logo on screen at Google Event Center in Sunnyvale, California Are you ready to bring more awareness to your brand? Consider becoming a sponsor for The AI Impact Tour. Learn more about the opportunities here.
A group of Google Brain and Carnegie Mellon University researchers this week introduced XLNet, an AI model capable of outperforming Google’s cutting-edge BERT in 20 NLP tasks and achieving state-of-the-art results on 18 benchmark tasks. BERT ( Bidirectional Encoder Representations from Transform ) is Google’s language representation model for unsupervised pretraining of NLP models first introduced last fall.
XLNet achieved state-of-the-art performance in several tasks, including seven GLUE language understanding tasks, three reading comprehension tasks like SQuAD , and seven text classification tasks that include processing of Yelp and IMDB data sets. Text classification with XLNet saw a marked reduction of up to 16% in error rates compared to BERT. Google open-sourced BERT in the fall of 2018.
XLNet harnesses the best of autoregressive and autoencoding methods used for unsupervised pretraining through a variety of techniques detailed in an arXiv paper published Wednesday by a group of six authors.
“XLNet is a generalized autoregressive pretraining method that allows learning bidirectional context learning by maximizing the expected likelihood over all permutations of the factorization order and […] overcomes the limitations of BERT thanks to its autoregressive formulation,” the paper reads.
VB Event The AI Impact Tour Connect with the enterprise AI community at VentureBeat’s AI Impact Tour coming to a city near you! The model’s name is derived from Transformer-XL , an autoregressive model released in January by the same team of researchers. XLNet adopts Transformer-XL’s pretraining methods for segment recurrence mechanism and relative encoding schemes. The model also borrows from NADE , which was created by researchers from Google DeepMind, Twitter, and academia for its permutation language modeling methods.
XLNet is the most recent NLP model to emerge that performs better than BERT.
Microsoft AI researchers introduced Multi-Task Deep Neural Network (MT-DNN) in May.
 The model is based on BERT but achieves better performance on a number of GLUE language understanding benchmark performance tasks.
VentureBeat's mission is to be a digital town square for technical decision-makers to gain knowledge about transformative enterprise technology and transact.
Discover our Briefings.
The AI Impact Tour Join us for an evening full of networking and insights at VentureBeat's AI Impact Tour, coming to San Francisco, New York, and Los Angeles! VentureBeat Homepage Follow us on Facebook Follow us on X Follow us on LinkedIn Follow us on RSS Press Releases Contact Us Advertise Share a News Tip Contribute to DataDecisionMakers Careers Privacy Policy Terms of Service Do Not Sell My Personal Information © 2023 VentureBeat.
 All rights reserved.
