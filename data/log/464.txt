Vox homepage Give Give Newsletters Newsletters Site search Search Vox main menu Explainers Crossword Video Podcasts Politics Policy Culture Science Technology Climate Health Money Life Future Perfect Newsletters More Explainers Israel-Hamas war 2024 election Supreme Court Buy less stuff Open enrollment What to watch All explainers Crossword Video Podcasts Politics Policy Culture Science Technology Climate Health Money Life Future Perfect Newsletters We have a request Vox's journalism is free, because we believe that everyone deserves to understand the world they live in. Reader support helps us do that. Can you chip in to help keep Vox free for all? × Filed under: Future Perfect Technology Policy Finally, a realistic roadmap for getting AI companies in check It’s time for AI regulators to move fast and break things.
By Sigal Samuel Apr 12, 2023, 2:20pm EDT Share this story Share this on Facebook Share this on Twitter Share All sharing options Share All sharing options for: Finally, a realistic roadmap for getting AI companies in check Reddit Pocket Flipboard Email CFOTO/Future Publishing via Getty Images This story is part of a group of stories called Finding the best ways to do good.
Part of New AI systems are coming at us so fast and furious that it might seem like there’s nothing we can do to stop them long enough to make sure they’re safe.
But that’s not true. There are concrete things regulators can do right now to prevent tech companies from releasing risky systems.
In a new report , the AI Now Institute — a research center studying the social implications of artificial intelligence — offers a roadmap that specifies exactly which steps policymakers can take. It’s refreshingly pragmatic and actionable, thanks to the government experience of authors Amba Kak and Sarah Myers West. Both former advisers to Federal Trade Commission chair Lina Khan, they focus on what regulators can realistically do today.
The big argument is that if we want to curb AI harms, we need to curb the concentration of power in Big Tech.
To build state-of-the-art AI systems, you need resources — a gargantuan trove of data, a huge amount of computing power — and only a few companies currently have those resources. These companies amass millions that they use to lobby government; they also become “too big to fail,” with even governments growing dependent on them for services.
So we get a situation where a few companies get to set the terms for everyone: They can build hugely consequential AI systems and then release them how and when they want, with very little accountability.
“A handful of private actors have accrued power and resources that rival nation-states while developing and evangelizing artificial intelligence as critical social infrastructure,” the report notes.
What the authors are highlighting is the hidden-in-plain-sight absurdity of how much power we’ve unwittingly ceded to a few actors that are not democratically elected.
When you think about the risks of systems like ChatGPT and GPT-4-powered Bing — like the risk of spreading disinformation that can fracture democratic society — it’s wild that companies like OpenAI and Microsoft have been able to release these systems at their own discretion. OpenAI’s mission, for example, is “to ensure that artificial general intelligence benefits all of humanity” — but so far, the company, not the public, has gotten to define what benefiting all of humanity entails.
The report says it’s past time to claw back power from the companies, and it recommends some strategies for doing just that. Let’s break them down.
Related The case for slowing down AI The rise of artificial intelligence, explained How does AI actually work? How is AI changing our society? Is AI coming for your job? Should we be worried about AI? Who will regulate AI? Concrete strategies for gaining control of AI One of the absurdities of the current situation is that when AI systems produce harm, it falls to researchers, investigative journalists, and the public to document the harms and push for change. But that means society is always carrying a heavy burden and scrambling to play catch-up after the fact.
So the report’s top recommendation is to create policies that place the burden on the companies themselves to demonstrate that they’re not doing harm. Just as a drugmaker has to prove to the FDA that a new medication is safe enough to go to market, tech companies should have to prove that their AI systems are safe before they’re released.
That would be a meaningful improvement over existing efforts to better the AI landscape, like the burgeoning industry in “audits,” where third-party evaluators peer under the hood to get transparency into how an algorithmic system works and root out bias or safety issues. It’s a good step, but the report says it shouldn’t be the primary policy response, because it tricks us into thinking of “bias” as a purely technical problem with a purely technical solution.
But bias is also about how AI is used in the real world. Take facial recognition. “It is not social progress to make black people equally visible to software that will inevitably be further weaponized against us,” Zoé Samudzi noted in 2019.
Here, again, the report reminds us of something that should be obvious but so often gets overlooked. Instead of taking an AI tool as a given and asking how we can make it fairer, we should start with the question: Should this AI tool even exist? In some cases, the answer will be no, and then the right response is not an audit, but a moratorium or a ban. For example, pseudoscience-based “emotion recognition” or “algorithmic gaydar” tech should not deployed , period.
Related Why it’s so damn hard to make AI fair and unbiased The tech industry is nimble, often switching tactics to suit its goals. Sometimes it goes from resisting regulation to claiming to support it, as we saw when it faced a chorus calling for bans on facial recognition. Companies like Microsoft supported soft moves that served to preempt bolder reform; they prescribed auditing the tech, a much weaker stance than banning police use of it altogether.
So, the report says, regulators need to keep their eyes peeled for moves like this and be ready to pivot if their approaches get co-opted or hollowed out by industry.
Regulators also need to get creative, using different tools in the policy toolbox to gain control of AI, even if those tools aren’t usually used together.
When people talk about “AI policy,” they sometimes think of it as distinct from other policy areas like data privacy. But “AI” is just a composite of data and algorithms and computational power. So data policy is AI policy.
Once we remember that, we can consider approaches that limit data collection, not only to protect consumer privacy, but also as mechanisms to mitigate some of the riskiest AI applications. Limit the supply of data and you’re limiting what can be built.
Similarly, we might not be used to talking about AI in the same breath as competition law or antitrust. But we’ve already got antitrust laws on the books and the Biden administration has signaled that it’s willing to boldly and imaginatively apply those laws to target the concentration of power among AI companies.
Related What happens when ChatGPT starts to feed on its own writing? Ultimately, the biggest hidden-in-plain-sight truth that the report reveals is that humans are in control of which technologies we deploy and when. Recent years have seen us place moratoria and bans on facial recognition tech; in the past, we’ve also organized a moratorium and created bright-line prohibitions in the field of human genetics.
Technological inevitability is a myth.
“There is nothing about artificial intelligence that is inevitable,” the report says. “Only once we stop seeing AI as synonymous with progress can we establish popular control over the trajectory of these technologies.” Will you support Vox’s explanatory journalism? Most news outlets make their money through advertising or subscriptions. But when it comes to what we’re trying to do at Vox, there are a couple reasons that we can't rely only on ads and subscriptions to keep the lights on.
First, advertising dollars go up and down with the economy. We often only know a few months out what our advertising revenue will be, which makes it hard to plan ahead.
Second, we’re not in the subscriptions business. Vox is here to help everyone understand the complex issues shaping the world — not just the people who can afford to pay for a subscription. We believe that’s an important part of building a more equal society. We can’t do that if we have a paywall.
That’s why we also turn to you, our readers, to help us keep Vox free.
If you also believe that everyone deserves access to trusted high-quality information, will you make a gift to Vox today? One-Time Monthly Annual $5 /month $10 /month $25 /month $50 /month Other $ /month /month We accept credit card, Apple Pay, and Google Pay. You can also contribute via The rise of artificial intelligence, explained How does AI actually work? 4 What is generative AI, and why is it suddenly everywhere? What happens when ChatGPT starts to feed on its own writing? The exciting new AI transforming search — and maybe everything — explained The tricky truth about how generative AI uses your data How is AI changing society? 19 What the stories we tell about robots tell us about ourselves Silicon Valley’s vision for AI? It’s religion, repackaged.
What will love and death mean in the age of machine intelligence? What if AI treats humans the way we treat animals? Can AI learn to love — and can we learn to love it? Black Mirror’s big AI episode has the wrong villain The ad industry is going all-in on AI The looming threat of AI to Hollywood, and why it should matter to you Can AI kill the greenscreen? What gets lost in the AI debate: It can be really fun How unbelievably realistic fake images could take over the internet Robot priests can bless you, advise you, and even perform your funeral AI art freaks me out. So I tried to make some.
How fake AI images can expand your mind AI art looks way too European An AI artist explains his workflow What will stop AI from flooding the internet with fake images? You’re going to see more AI-written articles whether you like it or not How “windfall profits” from AI companies could fund a universal basic income Show More Is AI coming for your job? 7 AI is flooding the workplace, and workers love it If you’re not using ChatGPT for your writing, you’re probably making a mistake Maybe AI can finally kill the cover letter Americans think AI is someone else’s problem Mark Zuckerberg’s not-so-secret plan to join the AI race The hottest new job is “head of AI” and nobody knows what they do Why Meta is giving away its extremely powerful AI model Should we be worried about AI? 10 Four different ways of understanding AI — and its risks AI experts are increasingly afraid of what they’re creating AI leaders (and Elon Musk) urge all labs to press pause on powerful AI The case for slowing down AI Are we racing toward AI catastrophe? The promise and peril of AI, according to 5 experts An unusual way to figure out if humanity is toast How AI could spark the next pandemic AI is supposedly the new nuclear weapons — but how similar are they, really? Don’t let AI fears of the future overshadow present-day causes Who will regulate AI? 8 The $1 billion gamble to ensure AI doesn’t destroy humanity Biden sure seems serious about not letting AI get out of control Can you safely build something that may kill you? Why an Air Force colonel — and many other experts — are so worried about the existential risk of AI Scared tech workers are scrambling to reinvent themselves as AI experts Panic about overhyped AI risk could lead to the wrong kind of regulation AI is a “tragedy of the commons.” We’ve got solutions for that.
The AI rules that US policymakers are considering, explained Most Read The controversy over TikTok and Osama bin Laden’s “Letter to America,” explained Formula 1 grew too fast. Now its new fans are tuning out.
The Ballad of Songbirds & Snakes might be the best Hunger Games movie yet Why are so few people getting the latest Covid-19 vaccine? What are Israel and Palestine? Why are they fighting? vox-mark Sign up for the newsletter Sentences The day's most important news stories, explained in your inbox.
Thanks for signing up! Check your inbox for a welcome email.
Email (required) Oops. Something went wrong. Please enter a valid email and try again.
Chorus Facebook Twitter YouTube About us Our staff Privacy policy Ethics & Guidelines How we make money Contact us How to pitch Vox Contact Send Us a Tip Vox Media Terms of Use Privacy Notice Cookie Policy Do Not Sell or Share My Personal Info Licensing FAQ Accessibility Platform Status Advertise with us Jobs @ Vox Media
