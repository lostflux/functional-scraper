Cohere Website For Business Docs Research Roberto Iriondo Emerging Trends in NLP Research: Top NLP Papers April 2023üìö Research Share: Stay at the forefront of NLP advances with Cohere For AI's community-curated April 2023 research üîçüß† TL;DR: Explore top NLP papers for April 2023, curated by Cohere For AI, covering topics like toxicity evaluation, large language model limitations, neural scaling laws, and retrieval-augmented models. Stay updated in the fast-evolving NLP field, and consider joining Cohere's research community.
Staying informed about the latest breakthroughs in natural language processing is crucial for language AI enthusiasts. Cohere's team has researched and collaborated with our research community to compile the most current developments in the NLP domain. This post provides a concise overview of recent progress to keep you well-informed in this fast-evolving field.
These papers delve into various aspects of natural language processing, ranging from the challenges of using black-box APIs for toxicity evaluation to understanding the limitations of large language models (LLMs) when it comes to memorizing factual knowledge. By examining cutting-edge research on topics, such as neural scaling laws, verifiability in generative search engines, and the impact of retrieval-augmented language models, we aim to provide you with valuable insights into the latest advancements in NLP. Read on to learn more about these fascinating studies and how they contribute to the ongoing development of natural language processing technology.
Cohere is dedicated to making NLP technology readily available to both developers and organizations, so they can unleash its true potential. In pursuit of this mission, we continually seek passionate individuals to join our research community and contribute to the advancement of this innovative technology. By participating in Cohere For AI , you can actively help shape the future of NLP and be a part of a collaborative and groundbreaking journey. We invite you to apply and become an integral member of our thriving research community.
Top Papers of April 2023 Highlighted by Our Research Discord Community These papers were highlighted by C4AI research Discord community members.
 Big thank you to @aranku, @hails, @EIFY, @krypticmouse, @hails, @Antonio_J#4677, @Ahmad_Anis#1832, @Ujan#304, @bhavnicksm#8949 , and the rest of the Cohere For AI NLP research community for participating.
On the Challenges of Using Black-Box APIs for Toxicity Evaluation in Research Authors: Luiza Pozzobon, Beyza Ermis, Patrick Lewis, Sara Hooker The authors of this paper explore the implications of the evolving nature of toxicity perception and commercially available black-box APIs, such as the Perspective API, on the reproducibility of research findings. These APIs are frequently retrained to address unattended weaknesses and biases, and the researchers assess the effects of these changes on the comparison of models and methods aimed at curbing toxicity. They find that research relying on inherited automatic toxicity scores have resulted in unreproducible results.
When the researchers rescored all models from HELM, a widely respected living benchmark, they discovered that the recent version of the API led to different rankings of widely used foundation models. As a result, they recommend caution when making apples-to-apples comparisons between studies. The code and data used in the study are available on GitHub.
Evaluating Verifiability in Generative Search Engines Authors: Nelson F. Liu, Tianyi Zhang, Percy Liang Generative search engines are designed to directly generate responses to user queries with in-line citations. A crucial trait for these engines is verifiability, meaning they should provide comprehensive and accurate citations. In a study, human evaluation was conducted to audit four popular generative search engines ‚Äî Bing Chat, NeevaAI, Perplexity AI, and YouChat ‚Äî across diverse queries from various sources, such as historical Google user queries and open-ended questions on Reddit.
While the responses generated by these engines were found to be fluent and seemingly informative, they often contained unsupported statements and inaccurate citations. On average, only 51.5% of generated sentences were fully supported by citations, and a mere 74.5% of citations accurately supported their associated sentences. This level of performance is concerning for systems that may be used as primary information-seeking tools, particularly given their appearance of trustworthiness. The study aims to motivate further development of trustworthy generative search engines and help researchers and users better understand the limitations of existing commercial systems.
A Theory on Adam Instability in Large-Scale Machine Learning Authors: Igor Molybog, Peter Albert, Moya Chen, Zachary DeVito, David Esiobu, Naman Goyal, Punit Singh Koura, Sharan Narang, Andrew Poulton, Ruan Silva, Binh Tang, Diana Liskovich, Puxin Xu, Yuchen Zhang, Melanie Kambadur, Stephen Roller, Susan Zhang In this research, the authors present a theory to explain the previously unexplained divergent behavior observed during the training of large language models. They propose that this phenomenon is a result of the dominant optimization algorithm used for training, known as Adam. It is observed that Adam can reach a state where the parameter update vector has a large norm and is essentially uncorrelated with the direction of descent on the training loss landscape, ultimately leading to divergence.
The authors note that this artifact is more likely to be observed in the training of deep models with large batch sizes, which is typical in large-scale language model training. To support their theory, they present observations from the training runs of language models with varying scales, including 7 billion, 30 billion, 65 billion, and 546 billion parameters. These observations highlight the role of the Adam optimization algorithm in causing divergent behavior during the training process of large language models.
Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study Authors: Boxin Wang, Wei Ping, Peng Xu, Lawrence McAfee, Zihan Liu, Mohammad Shoeybi, Yi Dong, Oleksii Kuchaiev, Bo Li, Chaowei Xiao, Anima Anandkumar, Bryan Catanzaro Large decoder-only language models (LMs) can significantly improve perplexity with retrieval, such as RETRO. However, the impact on text generation quality and downstream task accuracy remains uncertain, leaving the question: should we pretrain large autoregressive LMs with retrieval? To address this, researchers conducted a comprehensive study comparing a scalable pretrained retrieval-augmented LM (RETRO) with standard GPT and retrieval-augmented GPT at various stages. They successfully reproduced RETRO up to 9.5 billion parameters while retrieving a text corpus of 330 billion tokens.
The study revealed several key findings: (i) RETRO outperforms GPT in text generation with less degeneration, moderately higher factual accuracy, and slightly lower toxicity when using a nontoxic retrieval database, (ii) RETRO significantly outperforms GPT on knowledge-intensive tasks in the LM Evaluation Harness benchmark, but performs similarly on other tasks. Moreover, researchers introduced a simple variant, RETRO++, which greatly improves open-domain QA results and outperforms retrieval-augmented GPT across different model sizes. These findings suggest that pretraining autoregressive LMs with retrieval is a promising direction for future foundation models.
Conditional Adapters: Parameter-Efficient Transfer Learning with Fast Inference Authors: Tao Lei, Junwen Bai, Siddhartha Brahma, Joshua Ainslie, Kenton Lee, Yanqi Zhou, Nan Du, Vincent Y. Zhao, Yuexin Wu, Bo Li, Yu Zhang, Ming-Wei Chang The authors propose Conditional Adapter (CoDA), a method designed to enhance transfer learning efficiency in terms of both parameter usage and inference speed. Unlike traditional adapter approaches, CoDA leverages conditional computation to strike a balance between accuracy and speed. By incorporating sparse activation and minimal new parameters into an existing dense pretrained model, followed by a lightweight training phase, CoDA emerges as a highly efficient knowledge transfer mechanism.
Experimental results reveal that CoDA outperforms state-of-the-art Adapter approaches in various language, vision, and speech tasks. Withcondi a 2x to 8x increase in inference speed and the same parameter efficiency, CoDA achieves this improvement with little to no loss in accuracy. This demonstrates the potential of CoDA to optimize transfer learning processes in diverse applications.
Emergent and Predictable Memorization in Large Language Models Stella Biderman, USVSN Sai Prashanth, Lintang Sutawika, Hailey Schoelkopf, Quentin Anthony, Shivanshu Purohit, Edward Raf In a recent study, researchers explored the issue of memorization in LLMs, focusing on the undesired consequences of such models memorizing sensitive data like personal identifiable information (PII). The challenge lies in predicting which sequences will be memorized during a model's training, as undesirable memorization might lead to the rejection of an otherwise well-functioning model. To address this issue, the researchers conducted experiments using the Pythia model suite and aimed to extrapolate memorization behavior from lower-compute trial runs.
The study's findings revealed that intermediate checkpoints serve as better predictors of a model's memorization behavior compared to smaller fully-trained models. Furthermore, the researchers provided novel insights into the distribution of memorization scores across various models and data. These findings contribute valuable information to the ongoing discussion about the safety and security concerns associated with deploying large language models and offer potential solutions to mitigate the risks of undesirable memorization.
Stable and Low-Precision Training for Large-Scale Vision-Language Models Authors: Mitchell Wortsman, Tim Dettmers, Luke Zettlemoyer, Ari Morcos, Ali Farhadi, Ludwig Schmidt In this paper, the authors introduce new techniques for accelerating and stabilizing the training of large language-vision models. The first method, called SwitchBack, is a linear layer for int8 quantized training that speeds up the process by 13-25% while maintaining performance within 0.1 percentage points of bfloat16 training for a 1B parameter CLIP ViT-Huge model. This is the largest int8 training to date. Although the main focus is on int8, the paper also investigates float8 training through simulations, demonstrating that standard techniques can be effective if the network is trained and initialized to discourage large feature magnitudes. This is achieved by using layer-scale initialization with zeros.
For stabilizing the training, the authors examined loss spikes and discovered that they consistently occur 1-8 iterations after the squared gradients become underestimated by their AdamW second moment estimator. Based on this observation, they recommend an AdamW-Adafactor hybrid called StableAdamW. This method prevents loss spikes during the training of a CLIP ViT-Huge model and outperforms gradient clipping. By utilizing these techniques, researchers can improve the training process of large language-vision models, making it both faster and more stable.
Exploring the Curious Case of Code Prompts Authors: Li Zhang, Liam Dugan, Hainiu Xu, Chris Callison-Burch In this paper, the researchers explored whether code-prompting is the best way to interact with language models in general, as opposed to just on structured reasoning tasks. They conducted a comparison using three popular GPT models (davinci, code-davinci-002, and text-davinci-002) and a wide range of tasks, such as question-answering, sentiment analysis, and summarization. Interestingly, the results indicated that, with only a few exceptions, code prompts did not consistently outperform text prompts.
The research also revealed that the style of code prompt had a significant impact on performance for some tasks, but not all. Moreover, fine-tuning the models on text instructions led to improved relative performance of code prompts. This study highlights the need for further exploration in order to optimize the interaction between humans and language models, as well as the significance of prompt styles when it comes to the effectiveness of language model performance on various tasks.
Are Emergent Abilities of Large Language Models a Mirage? Authors: Rylan Schaeffer, Brando Miranda, Sanmi Koyejo In recent research, there's been a lot of talk about large language models exhibiting emergent abilities. These abilities are fascinating because they seem to appear suddenly and unpredictably as models scale up. However, this paper offers an alternative explanation for these emergent abilities. The authors argue that these perceived abilities could actually be a result of the researcher's choice of metrics when analyzing fixed model outputs for a specific task and model family. In other words, they suggest that the claims of emergent abilities might be a product of the analysis itself, rather than actual changes in model behavior as they scale.
To support this alternative explanation, the researchers presented a simple mathematical model and tested it in three different ways. First, they examined the effect of metric choice on the InstructGPT/GPT-3 family of models for tasks with claimed emergent abilities. Second, they conducted a meta-analysis of emergent abilities on BIG-Bench by making predictions about metric choices. Third, they demonstrated how similar metric decisions can imply apparent emergent abilities on vision tasks in various deep network architectures like convolutional, autoencoder, and transformers. Across all three analyses, the researchers found strong evidence that emergent abilities may not actually be a fundamental property of scaling AI models, suggesting that our understanding of these phenomena might need to be reconsidered.
When Not to Trust Language Models: Investigating Effectiveness and Limitations of Parametric and Non-Parametric Memories Authors: Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, Daniel Khashabi In this research paper, the authors explore the limitations of large language models (LMs) when it comes to tasks that require extensive world knowledge. Through extensive experiments on 10 different models and four augmentation methods, the authors used a new open-domain QA dataset, PopQA, with 14,000 questions to understand the strengths and weaknesses of LMs in memorizing factual knowledge. They discovered that LMs have difficulties with less popular factual knowledge, and increasing the size of these models doesn't significantly improve the memorization of obscure facts.
To address these limitations, the authors demonstrated that retrieval-augmented LMs greatly outperform much larger LMs, with unassisted LMs performing competitively only on questions involving high-popularity entities. The researchers then proposed a simple but effective method for creating powerful and efficient retrieval-augmented LMs, which access non-parametric memories only when necessary. This approach not only enhances the models' performance but also reduces inference costs, making it a promising solution for overcoming the limitations of LMs in tasks requiring a deep understanding of world knowledge.
Final Thoughts Eager to transform the way you handle substantial amounts of text? Consider integrating large language models into your processes. This compilation of state-of-the-art NLP research is designed to help you tap into the vast capabilities of this formidable technology. However, don't just rely on our suggestions ‚Äî experiment and fine-tune to identify the ideal model for your specific requirements. Moreover, you don't need to embark on this journey alone. Connect with our Discord community to exchange findings and collaborate with fellow NLP enthusiasts. When you're ready, explore our NLP API on the Cohere playground and begin shaping the future of natural language processing.
Keep reading Cohere ‚Äî Nov 16, 2023 Cohere‚Äôs Enterprise AI Models Coming Soon to Microsoft Azure AI as a Managed Service Newsroom Seraphina Goldfarb-Tarrant , Maximilian Mozes ‚Äî Nov 14, 2023 The Enterprise Guide to AI Safety For Business Cohere Team ‚Äî Nov 03, 2023 Emerging Trends in Generative AI Research: A Selection of Recent Papers Research Cohere.com Get Started About Classify Generate Responsibility Documentation Careers
