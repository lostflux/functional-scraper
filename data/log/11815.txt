Artificial Intelligence View All AI, ML and Deep Learning Auto ML Data Labelling Synthetic Data Conversational AI NLP Text-to-Speech Security View All Data Security and Privacy Network Security and Privacy Software Security Computer Hardware Security Cloud and Data Storage Security Data Infrastructure View All Data Science Data Management Data Storage and Cloud Big Data and Analytics Data Networks Automation View All Industrial Automation Business Process Automation Development Automation Robotic Process Automation Test Automation Enterprise Analytics View All Business Intelligence Disaster Recovery Business Continuity Statistical Analysis Predictive Analysis More Data Decision Makers Virtual Communication Team Collaboration UCaaS Virtual Reality Collaboration Virtual Employee Experience Programming & Development Product Development Application Development Test Management Development Languages Nvidia trains world’s largest Transformer-based language model Share on Facebook Share on X Share on LinkedIn Nvidia Are you ready to bring more awareness to your brand? Consider becoming a sponsor for The AI Impact Tour. Learn more about the opportunities here.
Nvidia today announced that it has trained the world’s largest language model, just the latest in a series of updates the GPU maker has aimed at advancing conversational AI.
To achieve this feat, Nvidia utilized model parallelism, splitting a neural network into pieces with a technique for creating models that are too big to fit within the memory of a single GPU. The model uses 8.3 billion parameters and is 24 times larger than BERT and 5 times larger than OpenAI’s GPT-2.
Nvidia also announced the fastest training and inference times of Bidirectional Encoder Representations (BERT), a popular model that was state of the art when it was open-sourced by Google in 2018.
Nvidia was able to train BERT-Large using optimized PyTorch software and a DGX-SuperPOD of more than 1,000 GPUs that is able to train BERT in 53 minutes.
VB Event The AI Impact Tour Connect with the enterprise AI community at VentureBeat’s AI Impact Tour coming to a city near you! “Without this kind of technology, it can take weeks to train one of these large language models,” Nvidia applied deep learning VP Bryan Catarazano said in a conversation with reporters and analysts.
Nvidia also claims it has achieved the fastest BERT inference time, dropping down to 2.2 milliseconds by running on a Tesla T4 GPU and TensorRT 5.1 optimized for datacenter inference. BERT inference takes up to 40 milliseconds when served by CPUs, while many conversational AI operations shoot for 10 milliseconds today, Catarazano said.
GPUs have also enabled gains for Microsoft’s Bing, which has used Nvidia hardware to cut latency time in half.
Each of the advances introduced today is meant to underline performance gains Nvidia’s GPU can provide for language understanding. Code for each of the above feats was open-sourced today to help AI practitioners and researchers explore the creation of large language models or speed training or inference with GPUs.
Alongside a sharp decline in word error rates, reduced latency has been a major enabler of adoption rates for popular AI assistants like Amazon’s Alexa, Google Assistant, and Baidu’s Duer.
Exchanges with little to no delay lead to machine-to-human conversations that feel more like human-to-human conversations, which generally happen at the speed of thought.
Like multi-turn dialogue features introduced for Microsoft’s Cortana, Alexa, and Google Assistant this year, real-time exchanges with an assistant make back-and-forth interactions feel more natural.
Evolution of the state of the art for conversational AI systems has largely revolved around the evolution of Google’s Transformer-based language model in 2017 and BERT in 2018.
Since then, BERT was surpassed by Microsoft’s MT-DNN , Google’s XLNet , and Baidu’s ERNIE , each of which builds on BERT.
Facebook introduced RoBERTa –also derived from BERT — in July. RoBERTa is currently ranked atop the GLUE benchmark leaderboard, with best in four of 9 language tasks. Each of the models outperforms human baseline on GLUE tasks.
VentureBeat's mission is to be a digital town square for technical decision-makers to gain knowledge about transformative enterprise technology and transact.
Discover our Briefings.
The AI Impact Tour Join us for an evening full of networking and insights at VentureBeat's AI Impact Tour, coming to San Francisco, New York, and Los Angeles! VentureBeat Homepage Follow us on Facebook Follow us on X Follow us on LinkedIn Follow us on RSS Press Releases Contact Us Advertise Share a News Tip Contribute to DataDecisionMakers Careers Privacy Policy Terms of Service Do Not Sell My Personal Information © 2023 VentureBeat.
 All rights reserved.
