Open Navigation Menu To revist this article, visit My Profile, then View saved stories.
Close Alert Backchannel Business Culture Gear Ideas Science Security Merch To revist this article, visit My Profile, then View saved stories.
Close Alert Search Backchannel Business Culture Gear Ideas Science Security Merch Podcasts Video Artificial Intelligence Climate Games Newsletters Magazine Events Wired Insider Jobs Coupons Bruce Schneier Nathan Sanders Ideas Nervous About ChatGPT? Try ChatGPT With a Hammer Play/Pause Button Pause Photo-illustration: WIRED Staff; Getty Images Save this story Save Save this story Save Last March, just two weeks after GPT-4 was released , researchers at Microsoft quietly announced a plan to compile millions of APIs‚Äîtools that can do everything from ordering a pizza to solving physics equations to controlling the TV in your living room‚Äîinto a compendium that would be made accessible to large language models (LLMs). This was just one milestone in the race across industry and academia to find the best ways to teach LLMs how to manipulate tools, which would supercharge the potential of AI more than any of the impressive advancements we‚Äôve seen to date.
The Microsoft project aims to teach AI how to use any and all digital tools in one fell swoop, a clever and efficient approach. Today, LLMs can do a pretty good job of recommending pizza toppings to you if you describe your dietary preferences and can draft dialog that you could use when you call the restaurant. But most AI tools can‚Äôt place the order, not even online. In contrast, Google‚Äôs seven-year-old Assistant tool can synthesize a voice on the telephone and fill out an online order form, but it can‚Äôt pick a restaurant or guess your order. By combining these capabilities, though, a tool-using AI could do it all. An LLM with access to your past conversations and tools like calorie calculators, a restaurant menu database, and your digital payment wallet could feasibly judge that you are trying to lose weight and want a low-calorie option, find the nearest restaurant with toppings you like, and place the delivery order. If it has access to your payment history, it could even guess at how generously you usually tip. If it has access to the sensors on your smartwatch or fitness tracker, it might be able to sense when your blood sugar is low and order the pie before you even realize you‚Äôre hungry.
Nathan Sanders is a data scientist affiliated with the Berkman Klein Center at Harvard University. He writes extensively on the effects of AI on democracy.
Bruce Schneier is a security technologist. He has written over a dozen books, including ‚ÄúA Hacker‚Äôs Mind: How the Rich and Powerful Bend Society‚Äôs Rules, and How to Bend them Back.‚Äù He teaches at the Harvard Kennedy School, and lives in Massachusetts.
Perhaps the most compelling potential applications of tool use are those that give AIs the ability to improve themselves. Suppose, for example, you asked a chatbot for help interpreting some facet of ancient Roman law that no one had thought to include examples of in the model‚Äôs original training. An LLM empowered to search academic databases and trigger its own training process could fine-tune its understanding of Roman law before answering. Access to specialized tools could even help a model like this better explain itself. While LLMs like GPT-4 already do a fairly good job of explaining their reasoning when asked, these explanations emerge from a ‚Äúblack box‚Äù and are vulnerable to errors and hallucinations.
 But a tool-using LLM could dissect its own internals, offering empirical assessments of its own reasoning and deterministic explanations of why it produced the answer it did.
If given access to tools for soliciting human feedback, a tool-using LLM could even generate specialized knowledge that isn‚Äôt yet captured on the web. It could post a question to Reddit or Quora or delegate a task to a human on Amazon‚Äôs Mechanical Turk. It could even seek out data about human preferences by doing survey research, either to provide an answer directly to you or to fine-tune its own training to be able to better answer questions in the future. Over time, tool-using AIs might start to look a lot like tool-using humans. An LLM can generate code much faster than any human programmer, so it can manipulate the systems and services of your computer with ease. It could also use your computer‚Äôs keyboard and cursor the way a person would, allowing it to use any program you do. And it could improve its own capabilities, using tools to ask questions, conduct research, and write code to incorporate into itself.
It‚Äôs easy to see how this kind of tool use comes with tremendous risks. Imagine an LLM being able to find someone‚Äôs phone number, call them and surreptitiously record their voice, guess what bank they use based on the largest providers in their area, impersonate them on a phone call with customer service to reset their password, and liquidate their account to make a donation to a political party. Each of these tasks invokes a simple tool‚Äîan internet search, a voice synthesizer, a bank app‚Äîand the LLM scripts the sequence of actions using the tools.
We don‚Äôt yet know how successful any of these attempts will be. As remarkably fluent as LLMs are, they weren‚Äôt built specifically for the purpose of operating tools, and it remains to be seen how their early successes in tool use will translate to future use cases like the ones described here. As such, giving the current generative AI sudden access to millions of APIs‚Äîas Microsoft plans to‚Äîcould be a little like letting a toddler loose in a weapons depot.
Culture The Future of Game Accessibility Is Surprisingly Simple Geoffrey Bunting Science SpaceX‚Äôs Starship Lost Shortly After Launch of Second Test Flight Ramin Skibba Business Elon Musk May Have Just Signed X‚Äôs Death Warrant Vittoria Elliott Business OpenAI Ousts CEO Sam Altman Will Knight Companies like Microsoft should be particularly careful about granting AIs access to certain combinations of tools. Access to tools to look up information, make specialized calculations, and examine real-world sensors all carry a modicum of risk. The ability to transmit messages beyond the immediate user of the tool or to use APIs that manipulate physical objects like locks or machines carries much larger risks. Combining these categories of tools amplifies the risks of each.
The operators of the most advanced LLMs, such as OpenAI, should continue to proceed cautiously as they begin enabling tool use and should restrict uses of their products in sensitive domains such as politics, health care, banking, and defense. But it seems clear that these industry leaders have already largely lost their moat around LLM technology‚Äîopen source is catching up.
Recognizing this trend, Meta has taken an ‚ÄúIf you can‚Äôt beat ‚Äôem, join ‚Äôem‚Äù approach and partially embraced the role of providing open source LLM platforms.
On the policy front, national‚Äîand regional‚ÄîAI prescriptions seem futile.
Europe is the only significant jurisdiction that has made meaningful progress on regulating the responsible use of AI, but it‚Äôs not entirely clear how regulators will enforce it. And the US is playing catch-up and seems destined to be much more permissive in allowing even risks deemed ‚Äú unacceptable ‚Äù by the EU. Meanwhile, no government has invested in a ‚Äú public option ‚Äù AI model that would offer an alternative to Big Tech that is more responsive and accountable to its citizens.
Regulators should consider what AIs are allowed to do autonomously, like whether they can be assigned property ownership or register a business. Perhaps more sensitive transactions should require a verified human in the loop, even at the cost of some added friction. Our legal system may be imperfect, but we largely know how to hold humans accountable for misdeeds; the trick is not to let them shunt their responsibilities to artificial third parties. We should continue pursuing AI-specific regulatory solutions while also recognizing that they are not sufficient on their own.
We must also prepare for the benign ways that tool-using AI might impact society. In the best-case scenario, such an LLM may rapidly accelerate a field like drug discovery, and the patent office and FDA should prepare for a dramatic increase in the number of legitimate drug candidates. We should reshape how we interact with our governments to take advantage of AI tools that give us all dramatically more potential to have our voices heard. And we should make sure that the economic benefits of superintelligent, labor-saving AI are equitably distributed.
We can debate whether LLMs are truly intelligent or conscious, or have agency, but AIs will become increasingly capable tool users either way. Some things are greater than the sum of their parts. An AI with the ability to manipulate and interact with even simple tools will become vastly more powerful than the tools themselves. Let‚Äôs be sure we‚Äôre ready for them.
WIRED Opinion publishes articles by outside contributors representing a wide range of viewpoints. Read more opinions here.
 Submit an op-ed at ideas@wired.com.
You Might Also Like ‚Ä¶ üìß Find the best bargains on quality gear with our Deals newsletter ‚Äú Someone is using photos of me to talk to men‚Äù First-gen social media users have nowhere to go The truth behind the biggest (and dumbest) battery myths We asked a Savile Row tailor to test all the ‚Äúbest‚Äù T-shirts you see in social media ads My kid wants to be an influencer.
 Is that bad? üåû See if you take a shine to our picks for the best sunglasses and sun protection Topics artificial intelligence machine learning big data algorithms Regulation Nika Simovich Fisher Meghan O'Gieblyn Facebook X Pinterest YouTube Instagram Tiktok More From WIRED Subscribe Newsletters Mattresses Reviews FAQ Wired Staff Coupons Black Friday Editorial Standards Archive Contact Advertise Contact Us Customer Care Jobs Press Center RSS Accessibility Help Cond√© Nast Store Do Not Sell My Personal Info ¬© 2023 Cond√© Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights.
WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Cond√© Nast.
Ad Choices Select international site United States LargeChevron UK Italia Jap√≥n Czech Republic & Slovakia
