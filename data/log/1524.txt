Featured Topics Newsletters Events Podcasts Featured Topics Newsletters Events Podcasts The UK exam debacle reminds us that algorithms can’t fix broken systems By Karen Hao archive page Ms Tech | AP, Getty When the UK first set out to find an alternative to school leaving qualifications, the premise seemed perfectly reasonable. Covid-19 had derailed any opportunity for students to take the exams in person, but the government still wanted a way to assess them for university admission decisions.
Chief among its concerns was an issue of fairness. Teachers had already made predictions of their students’ exam scores, but previous studies had shown that these could be biased on the basis of age, gender, and ethnicity. After a series of expert panels and consultations , Ofqual, the Office of Qualifications and Examinations Regulation, turned to an algorithm. From there, things went horribly wrong.
Nearly 40% of students ended up receiving exam scores downgraded from their teachers’ predictions, threatening to cost them their university spots.
Analysis of the algorithm also revealed that it had disproportionately hurt students from working-class and disadvantaged communities and inflated the scores of students from private schools. On August 16, hundreds chanted “ Fuck the algorithm ” in front of the UK's Department of Education building in London to protest the results. By the next day, Ofqual had reversed its decision.
 Students will now be awarded either their teacher’s predicted scores or the algorithm’s—whichever is higher.
The debacle feels like a textbook example of algorithmic discrimination.
 Those who have since dissected the algorithm have pointed out how predictable it was that things would go awry; it was trained, in part, not just on each student’s past academic performance but also on the past entrance-exam performance of the student’s school. The approach could only have led to punishment of outstanding outliers in favor of a consistent average.
But the root of the problem runs deeper than bad data or poor algorithmic design. The more fundamental errors were made before Ofqual even chose to pursue an algorithm. At bottom, the regulator lost sight of the ultimate goal: to help students transition into university during anxiety-ridden times. In this unprecedented situation, the exam system should have been completely rethought.
“There was just a spectacular failure of imagination,” says Hye Jung Han, a researcher at Human Rights Watch in the US, who focuses on children’s rights and technology. “They just didn’t question the very premise of so many of their processes even when they should have.” At a basic level, Ofqual faced two potential objectives after exams were canceled. The first was to avoid grade inflation and standardize the scores; the second was to assess students as accurately as possible in a way useful for university admissions. Under a directive from the secretary of state, it prioritized the first goal. “I think really that’s the moment that was the problem,” says Hannah Fry, a senior lecturer at University College London and author of Hello World: How to Be Human in the Age of the Machine.
 “They were optimizing for the wrong thing. Then it basically doesn’t matter what the algorithm is—it was never going to be perfect.” “There was just a spectacular failure of imagination.” The objective completely shaped the way Ofqual went about pursuing the problem. The need for standardization overruled everything else. The regulator then logically chose one of the best standardization tools, a statistical model, for predicting a distribution of entrance-exam scores for 2020 that would match the distribution from 2019.
Had Ofqual chosen the other objective, things would have gone quite differently. It likely would have scrapped the algorithm and worked with universities to change how the exam grades are weighted in their admissions processes. “If they just looked one step past their immediate problem and looked at what are the purpose of grades—to go to university, to be able to get jobs—they could have flexibly worked with universities and with workplaces to say, ‘Hey, this year grades are going to look different, which means that any important decisions that traditionally were made based off of grades also need to flexible and need to be changed,” says Han.
In fixating on the perceived fairness of an algorithmic solution, Ofqual blinded itself to the glaring inequities of the overall system. “There’s an inherent unfairness in defining the problem to predict student grades as if a pandemic hadn’t happened,” Han says. “It actually ignores what we already know, which is that the pandemic exposed all of these digital divides in education.” Ofqual’s failures are not unique.
In a report published last week by the Oxford Internet Institute, researchers found that one of the most common traps organizations fall into when implementing algorithms is the belief that they will fix really complex structural issues. These projects “lend themselves to a kind of magical thinking,” says Gina Neff, an associate professor at the institute, who coauthored the report. “Somehow the algorithm will simply wash away any teacher bias, wash away any attempt at cheating or gaming the system.” “I think it’s the first time that an entire nation has felt the injustice of an algorithm simultaneously.” But the truth is, algorithms cannot fix broken systems. They inherit the flaws of the systems in which they’re placed. In this case, the students and their futures ultimately bore the brunt of the harm. “I think it’s the first time that an entire nation has felt the injustice of an algorithm simultaneously,” says Fry.
Fry, Neff, and Han all worry that this won’t be the end of algorithmic gaffes. Despite the new public awareness of the problems, designing and implementing fair and beneficial algorithms is frankly really hard.
Nonetheless, they urge organizations to make the most of the lessons learned from this experience. First, return to the objective and critically think about whether it’s the right one. Second, evaluate the structural issues that need to be fixed in order to achieve the objective. (“When the government cancelled the exam in March, that should have been the signal to come up with another strategy to allow a much larger ecology of decision makers to fairly assess student performance,” Neff says.) Finally, pick a solution that’s easy to understand, implement, and contest, especially in times of uncertainty. In this case, says Fry, that means forgoing the algorithm in favor of teacher-predicted scores: “I’m not saying that’s perfect,” she says, “but it’s at least a simple and transparent system.” hide by Karen Hao Share linkedinlink opens in a new window twitterlink opens in a new window facebooklink opens in a new window emaillink opens in a new window Popular This new data poisoning tool lets artists fight back against generative AI Melissa Heikkilä Everything you need to know about artificial wombs Cassandra Willyard Deepfakes of Chinese influencers are livestreaming 24/7 Zeyi Yang How to fix the internet Katie Notopoulos Deep Dive Policy Three things to know about the White House’s executive order on AI Experts say its emphasis on content labeling, watermarking, and transparency represents important steps forward.
By Tate Ryan-Mosley archive page Melissa Heikkilä archive page How generative AI is boosting the spread of disinformation and propaganda In a new report, Freedom House documents the ways governments are now using the tech to amplify censorship.
By Tate Ryan-Mosley archive page Government technology is famously bad. It doesn’t have to be.
New York City is fixing the relationship between government and technology–and not in the ways you’d expect.
By Tate Ryan-Mosley archive page It’s shockingly easy to buy sensitive data about US military personnel A new report exposes the privacy and national security concerns created by data brokers. US senators tell MIT Technology Review the industry needs to be regulated.
By Tate Ryan-Mosley archive page Stay connected Illustration by Rose Wong Get the latest updates from MIT Technology Review Discover special offers, top stories, upcoming events, and more.
Enter your email Thank you for submitting your email! It looks like something went wrong.
We’re having trouble saving your preferences. Try refreshing this page and updating them one more time. If you continue to get this message, reach out to us at customer-service@technologyreview.com with a list of newsletters you’d like to receive.
The latest iteration of a legacy Advertise with MIT Technology Review © 2023 MIT Technology Review About About us Careers Custom content Advertise with us International Editions Republishing MIT News Help Help & FAQ My subscription Editorial guidelines Privacy policy Terms of Service Write for us Contact us twitterlink opens in a new window facebooklink opens in a new window instagramlink opens in a new window rsslink opens in a new window linkedinlink opens in a new window
