old id = 2783
Sparsity provides a competitive advantage | Nature Machine Intelligence
2021
https://www.nature.com/articles/s42256-021-00387-y

Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript.
AdvertisementSPIKING NEURAL NETWORKSSparsity provides a competitive advantageNature Machine Intelligencevolume3,pages742–743 (2021)Cite this article679Accesses16AltmetricMetricsdetailsSubjectsNeuromorphic chips that use spikes to encode information could provide fast and energy-efficient computing for ubiquitous embedded systems. A bio-plausible spike-timing solution for training spiking neural networks that makes the most of sparsity is implemented on the BrainScaleS-2 hardware platform.
The field of neuromorphic engineering aims to replicate and leverage the organizing principles of the brain in circuit architecture and data representation, which could lead to order-of-magnitude energy savings compared with today’s state-of-the-art digital processors. Essential ingredients for neuromorphic approaches are bringing computation close to memory, as with the brain’s tightly interconnected neuron and synapse elements, and encoding information in the form of all-or-none binary events known as ‘spikes’1. Wide adoption of neuromorphic processing devices is currently hindered by three main challenges. First, although neuromorphic hardware has been shown to outperform general-purpose digital processing devices such as CPUs and GPUs on specific tasks2, achieving power-performance-area trade-offs comparable to those of optimized machine-learning accelerator integrated circuits remains an open challenge3. Second, embedding intelligence on-chip requires the design of new algorithms for learning and autonomous adaptation within constrained power budgets4. Finally, although analogue circuit implementations of neuromorphic architectures provide accurate emulations of brain dynamics, new techniques are needed to shed light on the long-standing question of whether noise and mismatch are bugs or features5. Writing inNature Machine Intelligence, Göltz, Kriener et al. propose a training algorithm that leverages the sparsity of single-spike timings6, which may help to solve these three challenges at once.
This is a preview of subscription contentAccess optionsSubscribe to Nature+Get immediate online access to the entire Nature family of 50+ journals$29.99monthlySubscribe to JournalGet full journal access for 1 year$99.00only $8.25 per issueAll prices are NET prices.
VAT will be added later in the checkout.
Tax calculation will be finalised during checkout.
Buy articleGet time limited or full article access on ReadCube.
$32.00All prices are NET prices.
Additional access options:ReferencesIndiveri, G. & Liu, S.-C.
Proc. IEEE103, 1379–1397 (2015).
ArticleGoogle ScholarDavies, M. et al.
Proc. IEEE109, 911–934 (2021).
ArticleGoogle ScholarFrenkel, C., Bol, D. & Indiveri, G. Preprint athttps://arxiv.org/abs/2106.01288(2021).
Murmann, B. & Höfflinger, B. (eds).
NANO-CHIPS 2030: On-chip AI for an Efficient Data-driven World(Springer, 2020).
Chicca, E., Stefanini, F., Bartolozzi, C. & Indiveri, G.
Proc. IEEE102, 1367–1388 (2014).
ArticleGoogle ScholarGöltz, J. et al.
Nat. Mach. Intell.
https://doi.org/10.1038/s42256-021-00388-x(2021).
Rueckauer, B., Lungu, I.-A., Hu, Y., Pfeiffer, M. & Liu, S.-C.
Front. Neurosci.
11, 682 (2017).
ArticleGoogle ScholarDavidsol, S. & Furber, S. B.
Front. Neurosci.
15, 651141 (2021).
ArticleGoogle ScholarMostafa, H.
IEEE Trans. Neural Netw. Learn. Syst.
29, 3227–3235 (2017).
Google ScholarKheradpisheh, S. R. & Masquelier, T.
Int. J. Neural Syst.
30, 2050027 (2020).
ArticleGoogle ScholarFourcaud-Trocmé, N., Hansel, D., Van Vreeswijk, C. & Brunel, N.
J. Neurosci.
23, 11628–11640 (2003).
ArticleGoogle ScholarSchemmel, J., Billaudelle, S., Dauer, P. & Weis, J. Preprint athttps://arxiv.org/abs/2003.11996(2020).
Thorpe, S., Delorme, A. & Van Rullen, R.
Neural Netw.
14, 715–725 (2001).
ArticleGoogle ScholarFrenkel, C., Legat, J.-D. & Bol, D.
IEEE International Symposium on Circuits and Systems(ISCAS, 2020).
Indiveri, G. & Sandamirskaya, Y.
IEEE Signal Process. Mag.
36, 16–28 (2019).
ArticleGoogle ScholarDownload referencesAuthor informationAffiliationsInstitute of Neuroinformatics, University of Zurich and ETH Zurich, Zurich, SwitzerlandCharlotte FrenkelYou can also search for this author inPubMedGoogle ScholarCorresponding authorCorrespondence toCharlotte Frenkel.
Ethics declarationsCompeting interestsThe author declares no competing interests.
Rights and permissionsReprints and PermissionsAbout this articleCite this articleFrenkel, C. Sparsity provides a competitive advantage.
Nat Mach Intell3,742–743 (2021). https://doi.org/10.1038/s42256-021-00387-yDownload citationPublished:17 September 2021Issue Date:September 2021DOI:https://doi.org/10.1038/s42256-021-00387-yShare this articleAnyone you share the following link with will be able to read this content:Sorry, a shareable link is not currently available for this article.
Provided by the Springer Nature SharedIt content-sharing initiativeAssociated ContentFast and energy-efficient neuromorphic deep learning with first-spike timesAdvertisementExplore contentAbout the journalPublish with usSearchAdvanced searchQuick linksNature Machine Intelligence (Nat Mach Intell)ISSN2522-5839(online)nature.com sitemapDiscover contentPublishing policiesAuthor & Researcher servicesLibraries & institutionsAdvertising & partnershipsCareer developmentRegional websitesLegal & Privacy© 2022 Springer Nature LimitedSign up for theNature Briefingnewsletter — what matters in science, free to your inbox daily.
