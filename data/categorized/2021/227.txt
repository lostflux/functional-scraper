old id = 2784
Fast and energy-efficient neuromorphic deep learning with first-spike times | Nature Machine Intelligence
2021
https://www.nature.com/articles/s42256-021-00388-x

Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript.
AdvertisementFast and energy-efficient neuromorphic deep learning with first-spike timesNature Machine Intelligencevolume3,pages823–835 (2021)Cite this article2017Accesses11Citations86AltmetricMetricsdetailsSubjectsAbstractFor a biological agent operating under environmental pressure, energy consumption and reaction times are of critical importance. Similarly, engineered systems are optimized for short time-to-solution and low energy-to-solution characteristics. At the level of neuronal implementation, this implies achieving the desired results with as few and as early spikes as possible. With time-to-first-spike coding, both of these goals are inherently emerging features of learning. Here, we describe a rigorous derivation of a learning rule for such first-spike times in networks of leaky integrate-and-fire neurons, relying solely on input and output spike times, and show how this mechanism can implement error backpropagation in hierarchical spiking networks. Furthermore, we emulate our framework on the BrainScaleS-2 neuromorphic system and demonstrate its capability of harnessing the system’s speed and energy characteristics. Finally, we examine how our approach generalizes to other neuromorphic platforms by studying how its performance is affected by typical distortive effects induced by neuromorphic substrates.
Your institute does not have access to this articleAccess optionsSubscribe to Nature+Get immediate online access to the entire Nature family of 50+ journals$29.99monthlySubscribe to JournalGet full journal access for 1 year$99.00only $8.25 per issueAll prices are NET prices.
VAT will be added later in the checkout.
Tax calculation will be finalised during checkout.
Buy articleGet time limited or full article access on ReadCube.
$32.00All prices are NET prices.
Additional access options:Data availabilityWe used the MNIST66and the Yin-Yang dataset65. For the latter, seehttps://github.com/lkriener/yin_yang_data_set.
Code availabilityCode for the simulations81is available athttps://github.com/JulianGoeltz/fastAndDeep.
ReferencesKrizhevsky, A., Sutskever, I. & Hinton, G. E. ImageNet classification with deep convolutional neural networks. InAdvances in Neural Information Processing Systems1097–1105 (NIPS, 2012).
Silver, D. et al. Mastering the game of Go without human knowledge.
Nature550, 354–359 (2017).
ArticleGoogle ScholarBrown, T. B. et al. Language models are few-shot learners. Preprint athttps://arxiv.org/pdf/2005.14165.pdf(2020).
Brooks, R., Hassabis, D., Bray, D. & Shashua, A. Is the brain a good model for machine intelligence?.
Nature482, 462–463 (2012).
ArticleGoogle ScholarNg, A. What artificial intelligence can and can’t do right now.
Harvard Business Review(9 November 2016).
Hassabis, D., Kumaran, D., Summerfield, C. & Botvinick, M. Neuroscience-inspired artificial intelligence.
Neuron95, 245–258 (2017).
ArticleGoogle ScholarSejnowski, T. J.
The Deep Learning Revolution(MIT Press, 2018).
Richards, B. A. et al. A deep learning framework for neuroscience.
Nat. Neurosci.
22, 1761–1770 (2019).
ArticleGoogle ScholarPfeiffer, M. & Pfeil, T. Deep learning with spiking neurons: opportunities and challenges.
Front. Neurosci12, 774 (2018).
ArticleGoogle ScholarGerstner, W. What is different with spiking neurons? InPlausible Neural Networks for Biological Modelling. Mathematical Modelling:Theory and ApplicationsVol 13. (eds Mastebroek, H. A. K. & Vos, J. E.) 23–48 (Springer, 2001).
Izhikevich, E. M. Which model to use for cortical spiking neurons?IEEE Trans. Neural Netw.
15, 1063–1070 (2004).
ArticleGoogle ScholarGerstner, W.
Spiking Neurons(MIT Press, 1998).
Maass, W. Searching for principles of brain computation.
Curr. Opin. Behav. Sci.
11, 81–92 (2016).
ArticleGoogle ScholarDavies, M. Benchmarks for progress in neuromorphic computing.
Nat. Mach. Intell.
1, 386–388 (2019).
ArticleGoogle ScholarLinnainmaa, S.
The Representation of the Cumulative Rounding Error of an Algorithm as a Taylor Expansion of the Local Rounding Errors. Master’s thesis (in Finnish), Univ. Helsinki 6–7 (1970).
Werbos, P. J. Applications of advances in nonlinear sensitivity analysis. InSystem Modeling and Optimization.
Lecture Notes in Control and Information SciencesVol. 38 (eds Drenick, R. F. & Kozin, F.) 762–770 (Springer, 1982).
Rumelhart, D. E., Hinton, G. E. & Williams, R. J. Learning representations by back-propagating errors.
Nature323, 533–536 (1986).
MATHArticleGoogle ScholarTavanaei, A., Ghodrati, M., Kheradpisheh, S. R., Masquelier, T. & Maida, A. Deep learning in spiking neural networks.
Neural Netw111, 47–63 (2018).
ArticleGoogle ScholarNeftci, E. O., Mostafa, H. & Zenke, F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to spiking neural networks.
IEEE Signal Process.
36, 51–63 (2019).
ArticleGoogle ScholarGütig, R. & Sompolinsky, H. The tempotron: a neuron that learns spike timing-based decisions.
Nat. Neurosci.
9, 420–428 (2006).
ArticleGoogle ScholarCao, Y., Chen, Y. & Khosla, D. Spiking deep convolutional neural networks for energy-efficient object recognition.
Int. J. Comput. Vis.
113, 54–66 (2015).
MathSciNetArticleGoogle ScholarDiehl, P. U., Zarrella, G., Cassidy, A., Pedroni, B. U. & Neftci, E. Conversion of artificial recurrent neural networks to spiking neural networks for low-power neuromorphic hardware. InProc. 2016 IEEE International Conference on Rebooting Computing(ICRC) 1–8 (IEEE, 2016).
Schmitt, S. et al. Neuromorphic hardware in the loop: training a deep spiking network on the BrainScaleS wafer-scale system. InProc.
2017 International Joint Conference on Neural Networks(IJCNN) 2227–2234 (2017).
Wu, J., Chua, Y., Zhang, M., Yang, Q., Li, G., & Li, H. Deep spiking neural network with spike count based learning rule. InInternational Joint Conference on Neural Networks1–6 (IEEE, 2019).
Thakur, C. S. T. et al. Large-scale neuromorphic spiking array processors: a quest to mimic the brain.
Front. Neurosci.
12, 891 (2018).
ArticleGoogle ScholarMead, C. Neuromorphic electronic systems.
Proc. IEEE78, 1629–1636 (1990).
ArticleGoogle ScholarRoy, K., Jaiswal, A. & Panda, P. Towards spike-based machine intelligence with neuromorphic computing.
Nature575, 607–617 (2019).
ArticleGoogle ScholarPetrovici, M. A., Bill, J., Bytschok, I., Schemmel, J. & Meier, K. Stochastic inference with deterministic spiking neurons. Preprint athttps://arxiv.org/pdf/1311.3211.pdf(2013).
Neftci, E., Das, S., Pedroni, B., Kreutz-Delgado, K. & Cauwenberghs, G. Event-driven contrastive divergence for spiking neuromorphic systems.
Front. Neurosci.
7, 272 (2014).
ArticleGoogle ScholarPetrovici, M. A., Bill, J., Bytschok, I., Schemmel, J. & Meier, K. Stochastic inference with spiking neurons in the high-conductance state.
Phys. Rev. E94, 042312 (2016).
MathSciNetArticleGoogle ScholarNeftci, E. O., Pedroni, B. U., Joshi, S., Al-Shedivat, M. & Cauwenberghs, G. Stochastic synapses enable efficient brain-inspired learning machines.
Front. Neurosci.
10, 241 (2016).
ArticleGoogle ScholarLeng, L. et al. Spiking neurons with short-term synaptic plasticity form superior generative networks.
Sci. Rep.
8, 10651 (2018).
ArticleGoogle ScholarKungl, A. F. et al. Accelerated physical emulation of Bayesian inference in spiking neural networks.
Front. Neurosci.
13, 1201 (2019).
ArticleGoogle ScholarDold, D. et al. Stochasticity from function-why the Bayesian brain may need no noise.
Neural Netw.
119, 200–213 (2019).
ArticleGoogle ScholarJordan, J. et al. Deterministic networks for probabilistic computing.
Sci. Rep.
9, 18303 (2019).
ArticleGoogle ScholarHunsberger, E. & Eliasmith, C. Training spiking deep networks for neuromorphic hardware. Preprint athttps://arxiv.org/pdf/1611.05141.pdf(2016).
Kheradpisheh, S. R., Ganjtabesh, M., Thorpe, S. J. & Masquelier, T. STDP-based spiking deep convolutional neural networks for object recognition.
Neural Netw.
99, 56–67 (2018).
ArticleGoogle ScholarIlling, B., Gerstner, W. & Brea, J. Biologically plausible deep learning-but how far can we go with shallow networks?.
Neural Netw118, 90–101 (2019).
ArticleGoogle ScholarBohte, S. M., Kok, J. N. & La Poutré, J. A. Spikeprop: backpropagation for networks of spiking neurons. In8th European Symposium on Artificial Neural Networks419–424 (2000).
Zenke, F. & Ganguli, S. Superspike: supervised learning in multilayer spiking neural networks.
Neural Comput.
30, 1514–1541 (2018).
MathSciNetArticleGoogle ScholarHuh, D. & Sejnowski, T. J. Gradient descent for spiking neural networks. InAdvances in Neural Information Processing SystemsVol. 31, 1433–1443 (NIPS, 2018).
Thorpe, S., Delorme, A. & Van Rullen, R. Spike-based strategies for rapid processing.
Neural Netw.
14, 715–725 (2001).
ArticleGoogle ScholarThorpe, S., Fize, D. & Marlot, C. Speed of processing in the human visual system.
Nature381, 520–522 (1996).
ArticleGoogle ScholarJohansson, R. S. & Birznieks, I. First spikes in ensembles of human tactile afferents code complex spatial fingertip events.
Nat. Neurosci.
7, 170–177 (2004).
ArticleGoogle ScholarGollisch, T. & Meister, M. Rapid neural coding in the retina with relative spike latencies.
Science319, 1108–1111 (2008).
ArticleGoogle ScholarSchemmel, J. et al. A wafer-scale neuromorphic hardware system for large-scale neural modeling. InProc. 2010 IEEE International Symposium on Circuits and Systems1947–1950 (IEEE, 2010).
Akopyan, F. et al. TrueNorth: design and tool flow of a 65 mW 1 million neuron programmable neurosynaptic chip.
IEEE Trans. Comput. Aided Design Integrated Circuits Syst.
34, 1537–1557 (2015).
ArticleGoogle ScholarBillaudelle, S. et al. Versatile emulation of spiking neural networks on an accelerated neuromorphic substrate. InIEEE International Symposium on Circuits and Systems1–5 (IEEE, 2020).
Davies, M. et al. Loihi: a neuromorphic manycore processor with on-chip learning.
IEEE Micro38, 82–99 (2018).
ArticleGoogle ScholarMayr, C., Höppner, S., & Furber, S. SpiNNaker 2: a 10 million core processor system for brain simulation and machine learning-keynote presentation. InCommunicating Process Architectures 2017 & 2018277–280 (IOS Press, 2019).
Pei, J. et al. Towards artificial general intelligence with hybrid Tianjic chip architecture.
Nature572, 106–111 (2019).
ArticleGoogle ScholarMoradi, S., Qiao, N., Stefanini, F. & Indiveri, G. A scalable multicore architecture with heterogeneous memory structures for dynamic neuromorphic asynchronous processors (dynaps).
IEEE Trans. Biomed. Circuits Syst.
12, 106–122 (2017).
ArticleGoogle ScholarMostafa, H. Supervised learning based on temporal coding in spiking neural networks.
IEEE Trans. Neural Netw. Learn. Syst.
29, 3227–3235 (2017).
Google ScholarKheradpisheh, S. R. & Masquelier, T. S4NN: temporal backpropagation for spiking neural networks with one spike per neuron.
Int. J. Neural Syst.
30, 2050027 (2020).
ArticleGoogle ScholarRauch, A., La Camera, G., Luscher, H.-R., Senn, W. & Fusi, S. Neocortical pyramidal cells respond as integrate-and-fire neurons to in vivo-like input currents.
J. Neurophysiol.
90, 1598–1612 (2003).
ArticleGoogle ScholarGerstner, W. & Naud, R. How good are neuron models?Science326, 379–380 (2009).
ArticleGoogle ScholarTeeter, C. et al. Generalized leaky integrate-and-fire models classify multiple neuron types.
Nat. Commun.
9, 709 (2018).
ArticleGoogle ScholarGöltz, J.
Training Deep Networks with Time-to-First-Spike Coding on the BrainScaleS Wafer-Scale System. Master’s thesis, Universität Heidelberg (2019);http://www.kip.uni-heidelberg.de/Veroeffentlichungen/details.php?id=3909Friedmann, S. et al. Demonstrating hybrid learning in a flexible neuromorphic hardware system.
IEEE Trans. Biomed. Circuits Syst.
11, 128–142 (2017).
ArticleGoogle ScholarProdromakis, T. & Toumazou, C. A review on memristive devices and applications. InProc.
2010 17th IEEE International Conference on Electronics,Circuits and Systems934–937 (IEEE, 2010).
Esser, S. K., Appuswamy, R., Merolla, P., Arthur, J. V. & Modha, D. S. Backpropagation for energy-efficient neuromorphic computing. InAdvances in Neural Information Processing Systems1117–1125 (NIPS, 2015).
van De Burgt, Y., Melianas, A., Keene, S. T., Malliaras, G. & Salleo, A. Organic electronics for neuromorphic computing.
Nat. Electron.
1, 386–397 (2018).
ArticleGoogle ScholarWunderlich, T. et al. Demonstrating advantages of neuromorphic computation: a pilot study.
Front. Neurosci.
13, 260 (2019).
ArticleGoogle ScholarFeldmann, J., Youngblood, N., Wright, C., Bhaskaran, H. & Pernice, W. All-optical spiking neurosynaptic networks with self-learning capabilities.
Nature569, 208–214 (2019).
ArticleGoogle ScholarKriener, L., Göltz, J. & Petrovici, M. A. The yin-yang dataset. Preprint athttps://arxiv.org/pdf/2102.08211.pdf(2021).
LeCun, Y., Bottou, L., Bengio, Y. & Haffner, P. Gradient-based learning applied to document recognition.
Proc. IEEE86, 2278–2324 (1998).
ArticleGoogle ScholarSchemmel, J., Billaudelle, S., Dauer, P. & Weis, J. Accelerated analog neuromorphic computing. Preprint athttps://arxiv.org/pdf/2003.11996.pdf(2020).
Comsa, I. M. et al. Temporal coding in spiking neural networks with alpha synaptic function. InProc. 2020 IEEE International Conference on Acoustics,Speech and Signal Processing(ICASSP) 8529–8533 (IEEE, 2020).
Tavanaei, A., Kirby, Z. & Maida, A. S. Training spiking ConvNets by STDP and gradient descent. InProc.
2018 International Joint Conference on Neural Networks(IJCNN) 1–8 (IEEE, 2018).
Aamir, S. A. et al. An accelerated LIF neuronal network array for a large-scale mixed-signal neuromorphic architecture.
IEEE Trans. Circuits Syst. I Regular Papers65, 4299–4312 (2018).
ArticleGoogle ScholarPetrovici, M. A. et al. Characterization and compensation of network-level anomalies in mixed-signal neuromorphic modeling platforms.
PLoS ONE9, e108590 (2014).
ArticleGoogle ScholarCramer, B. et al. Training spiking multi-layer networks with surrogate gradients on an analog neuromorphic substrate. Preprint athttps://arxiv.org/pdf/2006.07239.pdf(2020).
Petrovici, M. A.
Form Versus Function:Theory and Models for Neuronal Substrates(Springer, 2016).
Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R. & Bengio, Y. Quantized neural networks: training neural networks with low precision weights and activations.
J. Mach. Learn. Res.
18, 6869–6898 (2017).
MathSciNetMATHGoogle ScholarPayeur, A., Guerguiev, J., Zenke, F., Richards, B. A. & Naud, R. Burst-dependent synaptic plasticity can coordinate learning in hierarchical circuits. Preprint atbioRxivhttps://doi.org/10.1101/2020.03.30.015511(2020).
Sacramento, J., Ponte Costa, R., Bengio, Y. & Senn, W. Dendritic cortical microcircuits approximate the backpropagation algorithm. InAdvances in Neural Information Processing SystemsVol. 31, 8721–8732 (NIPS, 2018).
Aamir, S. A. et al. A mixed-signal structured AdEx neuron for accelerated neuromorphic cores.
IEEE Trans. Biomed. Circuits Syst.
12, 1027–1037 (2018).
ArticleGoogle ScholarMüller, E. et al. Extending BrainScaleS OS for BrainscaleS-2. Preprint athttps://arxiv.org/pdf/2003.13750.pdf(2020).
Paszke, A. et al. PyTorch: an imperative style, high-performance deep learning library. InAdvances in Neural Information Processing SystemsVol. 32, 8024–8035 (NIPS, 2019).
Kingma, D. P. & Ba, J. Adam: a method for stochastic optimization. Preprint athttps://arxiv.org/pdf/1412.6980.pdf(2014).
Göltz, J. et al. Fast and energy-efficient neuromorphic deep learning with first-spike times (Zenodo, 2021);https://doi.org/10.5281/zenodo.5115007Stromatias, E. et al. Scalable energy-efficient, low-latency implementations of trained spiking deep belief networks on SpiNNaker. InProc. 2015 International Joint Conference on Neural Networks(IJCNN) 1–8 (2015).
Renner, A., Sheldon, F., Zlotnik, A., Tao, L. & Sornborger, A. The backpropagation algorithm implemented on spiking neuromorphic hardware. Preprint athttps://arxiv.org/pdf/2106.07030.pdf(2021).
Chen, G. K., Kumar, R., Sumbul, H. E., Knag, P. C. & Krishnamurthy, R. K. A 4096-neuron 1M-synapse 3.8-pJ/SOP spiking neural network with on-chip STDP learning and sparse weights in 10-nm FinFET CMOS.
IEEE J. Solid State Circuits54, 992–1002 (2018).
ArticleGoogle ScholarDownload referencesAcknowledgementsWe thank J. Jordan and N. Gürtler for valuable discussions, S. Schmitt for assistance with BrainScaleS-1, V. Karasenko, P. Spilger and Y. Stradmann for taming physics, as well as M. Davies and Intel for their ongoing support (L.K., W.S., M.A.P.). Some calculations were performed on UBELIX, the HPC cluster at the University of Bern. Our work has greatly benefitted from access to the Fenix Infrastructure resources, which are partially funded from the European Union’s Horizon 2020 research and innovation programme through the ICEI project under grant agreement no. 800858. Some simulations were performed on the bwForCluster NEMO, supported by the state of Baden–Württemberg through bwHPC and the German Research Foundation (DFG) through grant no. INST 39/963-1 FUGG. We gratefully acknowledge funding from the European Union for the Human Brain Project under grant agreements 604102 (J.S., K.M., M.A.P.), 720270 (S.B., O.B., B.C., J.S., K.M., M.A.P.), 785907 (S.B., O.B., B.C., W.S., J.S., K.M., M.A.P.), 945539 (L.K., A.B., S.B., O.B., B.C., W.S., J.S., M.A.P.) and the Manfred Stärk Foundation (J.G., A.B., D.D., A.F.K., K.M., M.A.P.).
Author informationThese authors contributed equally: J. Göltz, L. Kriener.
AffiliationsKirchhoff-Institute for Physics, Heidelberg University, Heidelberg, GermanyJ. Göltz, A. Baumbach, S. Billaudelle, O. Breitwieser, B. Cramer, D. Dold, A. F. Kungl, J. Schemmel, K. Meier & M. A. PetroviciDepartment of Physiology, University of Bern, Bern, SwitzerlandJ. Göltz, L. Kriener, W. Senn & M. A. PetroviciSiemens AI lab, Siemens AG Technology, Munich, GermanyD. DoldYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarContributionsJ.G., A.B. and M.A.P. designed the conceptual and experimental approach. J.G. derived the theory, implemented the algorithm and performed the hardware experiments. L.K. embedded the algorithm into a comprehensive training framework and performed the simulation experiments. A.B. and O.B. offered substantial software support. S.B., B.C., J.G. and A.F.K. provided low-level software for interfacing with the hardware. J.G., L.K., D.D., S.B. and M.A.P. wrote the manuscript.
Corresponding authorsCorrespondence toJ. Göltz,L. KrienerorM. A. Petrovici.
Ethics declarationsCompeting interestsThe authors declare no competing interests.
Additional informationPeer review informationNature Machine Intelligencethanks the anonymous reviewers for their contribution to the peer review of this work.
Publisher’s noteSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
Supplementary informationSupplementary InformationSupplementary text with sections SI.A to SI.F, six figures (SI.A1, SI.C1, SI.D1, SI.E1, SI.E2, SI.F1) and four tables (SI.B1, SI.F1-3).
Rights and permissionsReprints and PermissionsAbout this articleCite this articleGöltz, J., Kriener, L., Baumbach, A.
et al.
Fast and energy-efficient neuromorphic deep learning with first-spike times.
Nat Mach Intell3,823–835 (2021). https://doi.org/10.1038/s42256-021-00388-xDownload citationReceived:17 November 2020Accepted:30 July 2021Published:17 September 2021Issue Date:September 2021DOI:https://doi.org/10.1038/s42256-021-00388-xShare this articleAnyone you share the following link with will be able to read this content:Sorry, a shareable link is not currently available for this article.
Provided by the Springer Nature SharedIt content-sharing initiativeFurther readingNeuromorphic scaling advantages for energy-efficient random walk computationsNature Electronics(2022)Sparsity provides a competitive advantageNature Machine Intelligence(2021)Associated ContentSparsity provides a competitive advantageAdvertisementExplore contentAbout the journalPublish with usSearchAdvanced searchQuick linksNature Machine Intelligence (Nat Mach Intell)ISSN2522-5839(online)nature.com sitemapDiscover contentPublishing policiesAuthor & Researcher servicesLibraries & institutionsAdvertising & partnershipsCareer developmentRegional websitesLegal & Privacy© 2022 Springer Nature LimitedSign up for theNature Briefingnewsletter — what matters in science, free to your inbox daily.
