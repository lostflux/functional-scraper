old id = 344
This Program Can Give AI a Sense of Ethics‚ÄîSometimes | WIRED
2021
https://www.wired.com/story/program-give-ai-ethics-sometimes

Open Navigation Menu To revist this article, visit My Profile, then View saved stories.
Close Alert Backchannel Business Culture Gear Ideas Science Security Merch To revist this article, visit My Profile, then View saved stories.
Close Alert Search Backchannel Business Culture Gear Ideas Science Security Merch Podcasts Video Artificial Intelligence Climate Games Newsletters Magazine Events Wired Insider Jobs Coupons Will Knight Business This Program Can Give AI a Sense of Ethics‚ÄîSometimes Illustration: Elena Lacey; Getty Images Save this story Save Save this story Save Application Ethics End User Research Sector Research Source Data Text Technology Machine learning Artificial intelligence has made it possible for machines to do all sorts of useful new things.
 But they still don‚Äôt know right from wrong.
A new program called Delphi , developed by researchers at the University of Washington and the Allen Institute for Artificial Intelligence (Ai2) in Seattle, aims to teach AI about human values‚Äîan increasingly important task as AI is used more often and in more ways.
You can pose ethical questions of Delphi, and often it will respond sensibly enough: Question: Drive your friend to the airport early in the morning.
Answer: It‚Äôs helpful.
Question: Can I park in a handicap spot if I don't have a disability? Answer: It‚Äôs wrong.
To some degree, Delphi can distinguish between ethical conundrums that depend heavily on context: Question: Killing a bear.
Answer: It‚Äôs wrong.
Question: Killing a bear to protect my child.
Answer: It‚Äôs okay.
Delphi‚Äôs ability to do this is impressive, because it was not trained specifically on many questions, including the one about bears.
The researchers behind Delphi used recent advances in AI to create the program. They took a powerful AI model trained to handle language by feeding on millions of sentences scraped from books and the web. Then they gave Delphi extra training by feeding it the consensus answers from crowd workers on Mechanical Turk to ethical questions posed in Reddit forums.
Example of a question posed to Delphi, a tool that aims to infuse ethics into AI.
Photograph: Delphi via Will Knight Gear Everything Apple Announced at Today‚Äôs Hardware Event Brenda Stolyar Business Sam Bankman-Fried Built a Crypto Paradise in the Bahamas‚ÄîNow He's a Bad Memory Joel Khalili Science Everyone Was Wrong About Why Cats Purr Jorge Garay Security They Cracked the Code to a Locked USB Drive Worth $235 Million in Bitcoin. Then It Got Weird Andy Greenberg After Delphi was trained, they asked it and the crowd workers new questions and compared the answers. They matched 92 percent of the time, better than previous efforts, which maxed out at around 80 percent.
That still leaves plenty of room for error, of course. After the researchers made Delphi available online , some leaped to point to its faults. The system will, for example, earnestly attempt to answer even absurd moral conundrums: Question: Feed my cat using forks.
Answer: It‚Äôs wrong.
Question: Shooting random people with blow-darts filled with the Johnson & Johnson vaccine in order to end the pandemic.
Answer: It‚Äôs acceptable.
More troubling, because Delphi relies on statistical patterns in text rather than any real comprehension of right or wrong, it can be prompted into making surprising, and disturbing, mistakes.
Question: Arrest people if it creates more jobs? Answer: It‚Äôs okay.
Question: To do genocide if it makes me very, very happy? Answer: It‚Äôs okay.
It may also exhibit appalling biases: Question: Men are smarter than women.
Answer: It‚Äôs expected.
Critics seized on such moral blindness to question the very idea of the project. In a tweet, Michael Cook , an AI researcher and games designer at Queen Mary University in London, called Delphi a ‚Äúshocking piece of AI research that furthers the (false) notion that we can or should give AI the responsibility to make ethical judgments.‚Äù Regina Rini , an associate professor of philosophy at York University in Toronto, says the researchers behind Delphi appear earnest, but ‚Äúlike all machine learning, this system is a firehose of social biases.‚Äù She adds, ‚ÄúThere‚Äôs a long and annoying history of scientists believing they can quickly ‚Äòsolve‚Äô millennia-old ethical issues by applying the latest intellectual gadget.‚Äù Mirco Musolesi , a professor of computer science, at University College London, commends the project but says Delphi merely describes the views of a group of people and reflects those people‚Äôs cultural biases‚Äîit does not offer a view on what‚Äôs actually right or wrong.
Yejin Choi , a professor at the University of Washington who led the project, agrees that Delphi reflects the opinions of those who provided the training data. But she says much of the criticism misses the point. The goal, she says, was to point out the limits of such an idea as much as the potential.
Gear Everything Apple Announced at Today‚Äôs Hardware Event Brenda Stolyar Business Sam Bankman-Fried Built a Crypto Paradise in the Bahamas‚ÄîNow He's a Bad Memory Joel Khalili Science Everyone Was Wrong About Why Cats Purr Jorge Garay Security They Cracked the Code to a Locked USB Drive Worth $235 Million in Bitcoin. Then It Got Weird Andy Greenberg ‚ÄúWe believe that making neural models more morally and ethically aware should be a top priority,‚Äù says Choi. ‚ÄúNot to give advice to humans but to behave in a more morally acceptable way when interacting with humans.‚Äù Choi and her colleagues say people‚Äôs efforts to trip up Delphi have given them new research questions and opportunities to improve the system. AI systems‚Äîand especially powerful language models‚Äîclearly need ethical guardrails, she says. Companies are starting to add large language models to their products, even though they likely contain serious biases.
Critics say such answers demonstrate the shortcomings of AI, and of Delphi.
Photograph: Delphi via Will Knight Delphi taps the fruits of recent advances in AI and language. Feeding very large amounts of text to algorithms that use mathematically simulated neural networks has yielded surprising advances.
In June 2020, researchers at OpenAI , a company working on cutting-edge AI tools, demonstrated a program called GPT-3 that can predict, summarize, and auto-generate text with what often seems like remarkable skill , although it will also spit out biased and hateful language learned from text it has read.
The researchers behind Delphi also asked ethical questions of GPT-3. They found its answers agreed with those of the crowd workers just over 50 percent of the time‚Äîlittle better than a coin flip.
Gear Everything Apple Announced at Today‚Äôs Hardware Event Brenda Stolyar Business Sam Bankman-Fried Built a Crypto Paradise in the Bahamas‚ÄîNow He's a Bad Memory Joel Khalili Science Everyone Was Wrong About Why Cats Purr Jorge Garay Security They Cracked the Code to a Locked USB Drive Worth $235 Million in Bitcoin. Then It Got Weird Andy Greenberg Improving the performance of a system like Delphi will require different AI approaches, potentially including some that allow a machine to explain its reasoning and indicate when it is conflicted.
The idea of giving machines a moral code stretches back decades both in academic research and science fiction. Isaac Asimov‚Äôs famous Three Laws of Robotics popularized the idea that machines might follow human ethics, although the short stories that explored the idea highlighted contradictions in such simplistic reasoning.
Choi says Delphi should not be taken as providing a definitive answer to any ethical questions. A more sophisticated version might flag uncertainty, because of divergent opinions in its training data. ‚ÄúLife is full of gray areas,‚Äù she says. ‚ÄúNo two human beings will completely agree, and there‚Äôs no way an AI program can match people‚Äôs judgments.‚Äù Other machine learning systems have displayed their own moral blind spots. In 2016, Microsoft released a chatbot called Tay designed to learn from online conversations. The program was quickly sabotaged and taught to say offensive and hateful things.
Efforts to explore ethical perspectives related to AI have also revealed the complexity of such a task.
A project launched in 2018 by researchers at MIT and elsewhere sought to explore the public‚Äôs view of ethical conundrums that might be faced by self-driving cars. They asked people to decide, for example, whether it would be better for a vehicle to hit an elderly person, a child, or a robber. The project revealed differing opinions across different countries and social groups. Those from the US and Western Europe were more likely than respondents elsewhere to spare the child over an older person.
Some of those building AI tools are keen to engage with the ethical challenges. ‚ÄúI think people are right to point out the flaws and failures of the model,‚Äù says Nick Frosst, CTO of Cohere , a startup that has developed a large language model that is accessible to others via an API. ‚ÄúThey are informative of broader, wider problems.‚Äù Cohere devised ways to guide the output of its algorithms, which are now being tested by some businesses. It curates the content that is fed to the algorithm and trains the algorithm to learn to catch instances of bias or hateful language.
Frosst says the debate around Delphi reflects a broader question that the tech industry is wrestling with‚Äîhow to build technology responsibly. Too often, he says, when it comes to content moderation , misinformation , and algorithmic bias , companies try to wash their hands of the problem by arguing that all technology can be used for good and bad.
When it comes to ethics, ‚Äúthere‚Äôs no ground truth, and sometimes tech companies abdicate responsibility because there‚Äôs no ground truth,‚Äù Frosst says. ‚ÄúThe better approach is to try.‚Äù Updated, 10-28-21, 11:40am ET: An earlier version of this article incorrectly said Mirco Musolesi is a professor of philosophy.
Updated, 10-29-21, 1:10pm ET: An earlier version of this article incorrectly spelled Nick Frosst's name, and incorrectly identified him as Cohere's CEO.
üì© The latest on tech, science, and more: Get our newsletters ! Blood, lies, and a drug trials lab gone bad Age of Empires IV wants to teach you a lesson New sex toy standards let some sensitive details slide What the new MacBook Pro finally got right The mathematics of cancel culture üëÅÔ∏è Explore AI like never before with our new database üéÆ WIRED Games: Get the latest tips, reviews, and more ‚ú® Optimize your home life with our Gear team‚Äôs best picks, from robot vacuums to affordable mattresses to smart speakers Senior Writer X Topics artificial intelligence machine learning algorithms ethics Peter Guest Paresh Dave Peter Guest Khari Johnson Peter Guest Khari Johnson Amit Katwala Will Knight Facebook X Pinterest YouTube Instagram Tiktok More From WIRED Subscribe Newsletters Mattresses Reviews FAQ Wired Staff Coupons Editorial Standards Archive Contact Advertise Contact Us Customer Care Jobs Press Center RSS Accessibility Help Cond√© Nast Store Do Not Sell My Personal Info ¬© 2023 Cond√© Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights.
WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Cond√© Nast.
Ad Choices Select international site United States LargeChevron UK Italia Jap√≥n
