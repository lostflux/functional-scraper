old id = 3694
An atomic Boltzmann machine capable of self-adaption | Nature Nanotechnology
2021
https://www.nature.com/articles/s41565-020-00838-4

Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript.
AdvertisementAn atomic Boltzmann machine capable of self-adaptionNature Nanotechnologyvolume16,pages414–420 (2021)Cite this article5285Accesses8Citations335AltmetricMetricsdetailsSubjectsAbstractThe quest to implement machine learning algorithms in hardware has focused on combining various materials, each mimicking a computational primitive, to create device functionality. Ultimately, these piecewise approaches limit functionality and efficiency, while complicating scaling and on-chip learning, necessitating new approaches linking physical phenomena to machine learning models. Here, we create an atomic spin system that emulates a Boltzmann machine directly in the orbital dynamics of one well-defined material system. Utilizing the concept of orbital memory based on individual cobalt atoms on black phosphorus, we fabricate the prerequisite tuneable multi-well energy landscape by gating patterned atomic ensembles using scanning tunnelling microscopy. Exploiting the anisotropic behaviour of black phosphorus, we realize plasticity with multi-valued and interlinking synapses that lead to tuneable probability distributions. Furthermore, we observe an autonomous reorganization of the synaptic weights in response to external electrical stimuli, which evolves at a different time scale compared to neural dynamics. This self-adaptive architecture paves the way for autonomous learning directly in atomic-scale machine learning hardware.
You have full access to this article via your institution.
MainThere is a growing pursuit to create brain-inspired devices, or so-called neuromorphic architecture, to perform machine learning tasks directly in hardware. These approaches are largely based on using the complex electronic1,2,3, magnetic4,5,6and/or optical7responses of various solid-state materials to mimic a machine learning model. Most often, only particular aspects of these models can be captured in a given system, thus requiring hybrid schemes that couple different material units, circuitry and/or external computers in a serial-like fashion to solve a machine learning problem. In such systems, learning is often accomplished by combining the computational primitives of the materials with off-line computers to label data and implement learning algorithms8,9,10. Yet, one of the landmark goals of neuromorphic architecture is to create scalable parallel computing based on autonomous circuits capable of ‘on-chip’ learning11,12,13, eliminating the reliance on external computers. To this end, scalable constructions composed of a single material unit will require materials that exhibit dynamical behaviour in both neurons and synapses and self-adapt based on data. Addressing these challenges requires a fundamental understanding of how machine learning functionality, such as plasticity, can emerge from the complex dynamics of coupled stochastic ensembles. In this way, new materials, designed for machine learning can drastically improve energy efficiency and computational capabilities; furthermore, these platforms will also provide a vehicle for newly predicted functionality, such as quantum machine learning14,15,16.
Certain classes of machine learning models, such as the Boltzmann machine17(BM), are classified as energy-based models, which provide a natural link to physics-based phenomena in materials. The BM, as well as the Hopfield model, are formally equivalent to an interacting Ising spin system where fluctuating spins represent neurons that are linked by memory-bearing weighted interactions that serve as synapses18. The realization of the BM relies on the creation of a multi-modal energy landscape, strongly linked to the concept of spin glasses19,20,21. Yet, to date there are no well-known spin-based materials that exhibit a tuneable multi-well energy landscape, let alone the full functionality of a BM. Magnetic atoms on surfaces have emerged as a model platform to create tuneable networks of Ising spins22,23,24,25that can exhibit stochastic dynamics26,27. However, the main challenge towards creating magnetic materials that mimic a BM is related to the physical nature of the magnetic exchange interaction: often nearest-neighbour interactions dominate, resulting in robust, well-ordered, bistable ground states26,27and preventing the formation and adaption of a multi-well potential. Therefore, creating a fully atomic-scale platform in which the spin dynamics of coupled atoms represents a Boltzmann machine requires a new fundamental insight into how to realize (1) a multi-well energy landscape that is (2) tuneable via external synapses and (3) inherently self-adaptive based on a suitable learning protocol.
Here, we create a model Boltzmann machine capable of self-adaption realized from the stochastic orbital dynamics of individually coupled Co atoms on the surface of black phosphorus (BP). Taking a bottom-up approach, we start by representing stochastic, binary neurons using the concept of orbital memory. In this representation, the bistable valency states of individual atoms can be locally gated into a stochastic regime by the tip of a scanning tunnelling microscope (STM) (Fig.
1a)28. Utilizing atomic fabrication, we effectuate long-range coupling between atoms to produce collective stochastic behaviour, which mimics the steady-state distributions resulting from a multi-well energy landscape for a BM. Additionally, the strongly anisotropic substrate-driven interactions arising from the dielectric behaviour of BP29,30allow us to build atomic-scale synapses yielding memory-bearing, multi-valued distributions. Exploiting the separation of time scales between neural and synaptic dynamics, we demonstrate that the synapses autonomously adapt in response to various input stimuli introduced by the external gating field. This result demonstrates a model material system in which a well-controlled multi-well potential can be tuned to mimic a BM scaled to the atomic limit, where the intrinsic dynamics of the dopants represent a viable route towards atomic-scale autonomous materials for on-chip learning.
a, Schematic illustration of the experimental setup with STM tip gating the system above the switching threshold. Here, due to differences in local band bending (grey)s1ands2atoms switch stochastically, whilekremains static.
b, Isolated cobalt atom in two valency configurations (s= 0 and 1) with armchairx= [100] and zig-zagy= [010] crystallographic directions defined (Vs= −400 mV, scale bar, 1 nm).
c,d, Two cobalt atoms in state (s1,s2) = (0,0) (c) and three cobalt atoms in state (s1,s2,s3) = (0,0,0) (d) separated by approximately five to six unit cells along the armchair direction (Vs= −400 mV, scale bar, 1 nm).
e, Two-state current signalIt(t) observed in constant height when gating the single cobalt atom at the red ‘x’ inawithVs= 550 mV.
f,g, Multi-state current signalIt(t) measured at the red ‘x’ inc(f) andd(g) withVs= 550 mV.
h, Time-integrated probability distribution (P(s)) for each valency (s= 0 or 1) in the isolated Co atom.
i,j,P(s) for dimer (i) and trimer (j) systems. Error bars show 95% confidence intervals.
Orbital memory and the Boltzmann machineThe BM is defined by the following energy equation:wheres= (s1,...,sn) represents binary and stochastic spin values andE(s|b,w) defines a steady-state probability distribution for finding the system in a particular statesconditioned on memory-bearing and multi-valued weights (wij) and biases (bi) (Supplementary Information). A material representation of the BM requires (1) stochastic neural elementssiand (2) tuneable and memory-bearing interactionswij,bithat are ideally self-adaptable in response to external stimuli. It was recently demonstrated that a single Co atom exhibits stable and electrically switchable binary states, based on the concept of orbital memory stemming from its bistable valency28. We used individual and identical Co atoms residing on the surface of semiconducting BP (Supplementary Fig.
1) as building blocks for the BM.
We denote the two orbital memory states of an individual Co atom as spin valuess= 0 and 1 (Fig.
1a,b). The Co valency is extremely sensitive to its environment: when probed with STM at applied voltagesVs<Vth(whereVthis the gate voltage threshold required to induce stochastic switching), it exhibits long lifetimes that epitomize its utility as a memory. However, whenVs>Vththe atomic valency switches stochastically between its two states (seen inIt(t) in Fig.
1ewith the tip position marked ‘x’ in the STM image in Fig.
1b) with a favourability governed by the local electrostatic environment (Supplementary Figs.
2cand6). In this gated regime (Vs>Vth), we identify the valencys= 0 or 1 of an individual Co atom as a neuron in the BM representation. To correlate the state ofswith a discrete and distinct current level measured in the stochastic noise, we reduce the gate voltage belowVthto freeze the given state before identifying it with constant-current STM (Fig.
1band Supplementary Fig.
3). All subsequently displayed STM images were taken in this regime. To clearly distinguish differentsconfigurations, we colour code each distinct level inIt(t) (Fig.
1e). As the substrate is semiconducting, an applied potential between tip and substrate leads to a local voltage drop within the volume of the semiconductor underneath the tip (shaded area in Fig.
1a), implying the possibility of non-locally gating multiple atoms simultaneously (such ass1ands2in Fig.
1a).
A major step towards the realization of an atomic-scale BM was the effectuation of interatomic coupling facilitated by the controlled manipulation of Co atoms with the STM tip (seeSupplementary Informationfor details). When positioning two atoms with interatomic separation less than 4 nm along the BPxdirection (armchairx= [100], zig-zagy= [010]) (Fig.
1c), we simultaneously gate both atoms withVs>Vth, observing the emergence of four distinct and discrete states with appreciable lifetimes in current traces (It(t) in the constant-height measurement shown in Fig.
1fwith the tip position marked ‘x’ in the STM image in Fig.
1c). Using constant-current STM to correlateIt(t) with the possible state configurations, we map the four possible staticsconfigurations onto each of these current levels (Fig.
1fcolour coded in blue withsconfigurations shown in Supplementary Fig.
3). Integrating over time, we can extract the steady-state probability distributionP(s) (Fig.
1i) from the measured multi-state current (It(t)).
TheP(s) distribution shown in Fig.
1iexhibits non-zero populations of all four configurations, indicating that all configurations have appreciable lifetimes. This distribution illustrates the presence of multiple low-lying energy minima, suggesting a multi-well energy landscape of the form in equation (1) with non-uniform couplings and biases. Moreover, the large difference ofP(s) compared to the probability distribution of an isolated atom confirms substantial substrate-induced coupling between the atoms (Supplementary Fig.
4). This behaviour scales as we place a third atom in thexdirection (Fig.
1dand Supplementary Fig.
5). Above the gate threshold, we observe up to seven of eight possiblesconfigurations (Fig.
1gand Supplementary Fig.
5) and a more complex probability distribution (Fig.
1j). This illustrates appreciable interactions between all atoms beyond nearest-neighbour coupling. Additionally, we find that multi-state noise emerges for many different atomic configurations of Co atoms, where the occupational probability of the varioussconfigurations strongly depends on the interatomic coupling between individual Co atoms and the gating field (Supplementary Figs.
4and5). These examples demonstrate that the number of local energy minima increases with atom number. Likewise, this also shows that there is a large phase space of coupling and gating field in which multi-well behaviour emerges, suggesting that precise positioning within a certain range may not be necessary.
Tailoring synapses using anisotropic couplingWhile atoms coupled in thexdirection illustrate a complex probability distribution, the weightswijthat define the probability distribution are fixed by the substrate-mediated interactions. For the BM, it is imperative to create memory-bearing and switchable synaptic weightswijinterlinking the various spins, to represent more than one distribution. To do this, we take advantage of the in-plane electronic anisotropy of BP29,30, which leads to a different coupling between atoms depending on their relative orientation with respect to the surface. In this way, we modifyP(s) of chains of Co atoms formed along thexdirection utilizing the memory of a coupled satellite Co atom separated in theydirection (Fig.
2a,c,d). We define the orbital memory states of multiple satellite atoms byk=(k1,...
,km) (Fig.
2a), quantifying their influence by examining the conditional probability distributions,P(s|k).
a, Schematic representation of the three-atom system incandd, where the two atomss1ands2are strongly coupled and thekatom is weakly coupled tos1ands2.
b, Factor-graph representation of the Boltzmann machine mapped to the experimental configuration depicted inaand shown incandd.
c,d, Constant-current STM topography withk= 0 (c) andk= 1 (d) (Vs= −400 mV, scale bar, 1 nm).
e,f,It(t) measured at the red ‘x’ inc(e) andd(f) withVs= 500 mV.
g,h, Conditional, time-integrated probability distributionsP(s|k) fork= 0 (g) andk= 1 (h). Error bars show 95% confidence intervals.
To illustrate this synaptic concept, we show a configuration where a single Co atom (labelled byk) is placed approximately 2 nm in theydirection from two coupled atoms,s1ands2(Fig.
2c). As seen in Fig.
2e(k= 0) and Fig.
2f(k= 1) for the two distinct states of the top Co atom, notable differences can be seen in the multi-state current (It(t)) when switching is activated. This clearly demonstrates that thekatom valency has a substantial impact on the steady-state conditional probability distributionsP(s|k= 0) (Fig.
2g) andP(s|k= 1) (Fig.
2h), mimicking an atomic-scale synapse. The distributionsP(s|k= 0) andP(s|k= 1) can be modelled as distinct Boltzmann distributions (equations (1)) using the factor-graph representation shown in Fig.
2b. We learnb,wfor each value ofkby minimizing the Kullback–Leibler (KL) divergence between the empirical probability distributionP(s|k) of the states and the BM distributionP(s|b,w). The results of this fitting for bothk= 0 andk= 1 are shown in Table1. The conditionsk= 0 or 1 result in substantial modifications to the synaptic coupling (w12) and the biases (b1,b2), directly influencing the BM energy landscape (equation (1)). Thus, the valency of the satellite Co atom acts as discrete parametrization forb,w. Therefore, this three-atom representation can be mapped to a BM (Fig.
2band Supplementary Fig.
7) that contains two neurons and one binary synapse.
To be able to represent and learn a larger set of distributions, we introduce fourkatoms coupled to threesatoms (Fig.
3a). With two states associated with eachkatom, there are 16 possiblekconfigurations, so that we can represent 16 possible distributionsP(s|k) over the spinss. We show a subset of five possiblekconfigurations in Fig.
3. Figure3ashows constant-current STM images, below the gate threshold, identifying the valency of eachkatom for the five illustrated configurations. We employ the same characterization as in Fig.
2, measuringIt(t) above the gate threshold (Fig.
3b) and extractingP(s|k), for each value ofk. These five distributions are plotted in Fig.
3c, illustrating highly varying changes in the distributions, depending onk. For nearly every studiedkconfiguration, we observe significantly different distributions forP(s), as shown by analysing the KL divergence between allP(s|ki) distributions (Supplementary Fig.
11; see also the corresponding analysis for Fig.
4in Supplementary Fig.
12). Using the same modelling described above, for three spins with individual biasesb, pairwise couplingswand a three spin couplingw123, we find different solutionsP(s|b,w,w123) =P(s|k) for different values ofk, indicating that differentkconfigurations correspond to different synapse and bias values. As seen in Table2(and Supplementary Tables1–3), the mapping ofkontob,w,w123is highly nonlinear, which is probably related to the complex electrostatic environment of the atomic ensemble.
a, Constant-current STM images of a seven-atom cobalt ensemble with threesatoms and fourkatoms (Vs= −400 mV, scale bar, 1 nm). Thekconfigurations are:k= (0000) (i),k= (1000) (ii),k= (0001) (iii),k= (0110) (iv) andk= (1010) (v).
b,It(t) measured at the red ‘x’ positions inawithVs= 500 mV.
c, Conditional probability distributions,P(s|k). Error bars show 80% confidence intervals.
a, Constant-current STM image of a seven-atom cobalt ensemble with threesatoms and fourkatoms (Vs= −400 mV, scale bar, 1 nm).
b, Factor-graph representation of the Boltzmann machine used to model the atomic ensemble ina, with a diagram illustrating the influence of the environment on the BM.
c,It(t) taken at the red ‘x’ inawithVoff= 160 mV showing a spontaneous modification of the system’sk(before in grey and after in blue), which is observed through the distinct current levels and stochastic dynamics.
d–f, Probability distributionsP(s,k|ε) shown in blues ands-integrated probability distributionP(k|ε) (grey histograms), given the environmental conditionsε1≡Voff= 160 mV,ε2≡Voff= 180 mV,ε3≡Voff= 200 mV. Error bars show 95% confidence intervals.
Separation of time scales and self-adaptionIn Figs.
2and3, we showed that eachkconfiguration results in a different distributionP(s|k) of thesconfigurations, and thus emulates synapses and biases of the BM, scaled to the atomic limit. Hence, changes inkresult in changes to the BM parameters (Tables1and2) and thus could be used to implement a learning rule in a regime where thek-state lifetime is nearly infinite. One possibility to input complex signals into this system would be to impose multiplexed a.c. signals on the d.c. gate and implement an external computer to modify the values ofkaccording to an alternative training algorithm, which still needs to be designed, specifically for discrete weights31. As a step towards autonomous behaviour, it has been shown that spike-timing dependent neural dynamics can be coupled to synapses and, as such, these synapses can evolve in response to neural stimulation12,13. Indeed, in biological systems, learning is a dynamical process identified with synaptic evolution, namely a distinct time scale in which synapses evolve based on exposure to stimuli. In this way, while neurons change their state on the order of milliseconds, learning, for example in the context of long-term potentiation, occurs over minutes to hours32. Therefore, it is essential to explore materials capable of hosting both neural and synaptic dynamics, which exhibit plasticity, and subsequently exploring new learning models that exploit their coupled network dynamics.
As a step towards this end, we subsequently explore the separation of time scales in our system by quantifying the dynamics of thekconfigurations and their correlation with the steady-state neural distributions and environmental stimuli. We studied thekdynamics in a seven-atom BM (shown experimentally in Fig.
4aand illustrated schematically in Fig.
4b) over sufficiently long time scales (300–2,000 min per distribution) to allow for the spontaneous evolution ofk. We probed the response ofP(k) to changes in the environmental stimuli in the form of small changes in an applied offset voltage (ε=Voff=Vs−Vth, whereVth≈ 400 mV), in a regime where the total voltage is far above the stochastic switching threshold. We performed the measurements by initializing the ensemble into a randomsandkconfiguration before measuringIt(t) (Fig.
4c) at a specifiedVoff. At each environmental condition, measurements were conducted until theP(s,k) converges to a representative and distinct steady-state distribution (Fig.
4d–f, and Supplementary Figs.
9and10). Generally, the time scale for convergence ofP(k|ε) for givenεis approximately 1,000–4,000 times longer than forP(s|k) for givenk. As seen in Fig.
4d–f,P(k|ε) reaches a steady state that is extremely sensitive to changes in the stimulus of the order of 10 mV. WhenVoff= 200 mV,k= (0000) is the most favourable synaptic configuration (P(k= 0000|ε3) = 0.43), while atVoff= 160 mV the favourability moves tok= (0100) (P(0100|ε1) = 0.38). In other words,P(k|ε) autonomously adapts in response to small variations in the gate voltage. To demonstrate that this response is not random, we carried out the following control experiment. We measured at (1)ε1, (2)ε3and again at (1)ε1, while checking the conditional probabilityP(k|ε). The measurements confirm that theP(k|ε1) distribution remains the same before and after the intermediary stimulusε3. Furthermore, it is clear that the evolution ofP(k|ε) is nonlinear inVoff; this is nicely exemplified when examining theε-dependent probability fork= (1000):P(k= 1000|ε1) = 0.09,P(k= 1000|ε2) = 0.33,P(k= 1000|ε3) = 0.20, which is maximum atVoff= 180 mV (ε2). Visualizing such nonlinearity is aided by considering the additional complexity in the joint probability distributionP(s,k|ε) =P(s|k,ε)P(k|ε) (shown in blue in Fig.
4d–f). What is clearly evidenced in Fig.
4d–fis that distinct steady-stateP(k|ε) distributions are correlated with distinct environmental stimuli (also seen in Supplementary Fig.
8). In other words, thekconfigurations self-adapt over a longer time scale, conditioning their state on the input stimulus according to a multi-modal energy landscape defined by the stimuli, analogous to the landscapes of the spins. While we only characterized the BM response to various d.c. stimuli, we observed a strong nonlinear frequency and amplitude dependence in the single neurons’P(s) response (Supplementary Fig.
2). This suggests that the neural and synaptic dynamics exhibit a complex, coupled and rich landscape. As such, multiplexing may potentially be used to encode complex signals in the spectral components of an a.c. signal4and introduced via the tunnelling barrier, if a sufficient learning algorithm is developed.
In contrast to hybrid approaches, here both neurons and synapses are contained in a single material and the physics of the coupling separates the time scales of thesandkdynamics by multiple orders of magnitude. In this situation, we equate neural computation with thesvariables that exhibit fast dynamics and learning with the slower environmentally dependent evolution of the synaptic distribution (P(k|ε)). In this picture, the neural and synaptic dynamics are coupled and therefore standard learning algorithms cannot be used. When the time scales are well separated, the neural activity equilibrates to a stationary distribution and one may be able to develop a learning algorithm that acts on this statistical distribution of neural activity33. Note that learning in this context implies non-trivial, coupled changes to both the synaptic and neural distributions. This is different from current hybrid approaches for on-chip learning where, for example, synaptic strength is conditioned by neural inputs11,12,13.
It remains to be seen if the complex dynamics demonstrated here can be scaled up to a larger number of atoms. The observation of multi-state noise rules out a description dominated by nearest-neighbour interactions and suggests the presence of a complex, spatially varying and time-dependent mean field (Supplementary Figs.
2,4,5and6). To better understand the potential for scaling, it is essential to quantify the interplay between the substrate-mediated interactions, the role of dielectric screening and the external gating field, to clarify how each contributes to machine learning functionality. To probe this experimentally, larger scale cobalt ensembles need to be created, which may be possible with electronic mediated growth mechanisms29. It remains, however, an open challenge to fabricate precise atomic-scale gate structures, as is typically done for dopants in silicon-based devices34, to gate and read-out the BM state. Further work is needed to explore new material systems that might exhibit orbital memory behaviour35up to higher operating temperatures.
ConclusionIn conclusion, we have constructed a network of stochastic orbital memories derived from Co atoms on BP that emulates the behaviour of a Boltzmann machine scaled to the limit of individual atoms. When controllably coupling and gating individual Co atoms, we observed the onset of a tuneable multi-modal landscape that is largely unexpected in model Ising materials. Utilizing the anisotropic behaviour of BP, we were able to create atomic-scale synapses that tuned and stored the weights/biases. We have further shown a separation of neural and synaptic time scales, larger than three orders of magnitude. This allowed us to confirm that the synaptic, memory-bearing atoms autonomously reorganize in response to an input d.c. stimulus. In addition to this self-adaption, we showed that the neural dynamics exhibit rich and complex phase spaces in response to both d.c. and a.c. voltage signals. Using orbital memory is potentially much more energy efficient than approaches that rely on phase changes in materials (Supplementary Information). Furthermore, future developments of learning algorithms that consider the coupled dynamics of both neurons and synapses at their different time scales will allow this system to physically compute using the spins in materials, drastically reducing the computational costs associated with Monte Carlo sampling6,36.
MethodsScanning tunnelling microscopySTM measurements were performed under ultrahigh vacuum (<1 × 10−10mbar) conditions with an Omicron low-temperature STM at a base temperature of 4.4 K, with the voltage applied to the sample. The typical time resolution of these experiments was approximately 1 ms. All STM images were acquired by means of constant-current feedback. AllIt(t) measurements were acquired with the tip at a constant height and the feedback loop turned off. For all measurements, the tip height was stabilized with constant-current feedback on the bare BP, atIt= 20 pA,Vs= −400 mV. Electrochemically etched W tips were used for measurements; each tip was treated in situ by electron bombardment field emission, and was also dipped and characterized on a clean Au(111) surface. BP crystals were purchased from HQ graphene and subsequently stored in vacuum (<1 × 10−8mbar). The crystals were cleaved under ultrahigh vacuum conditions at pressures below 2 × 10−10mbar, and immediately transferred to the microscope for in situ characterization. Cobalt was evaporated directly into the STM chamber withTSTM< 5 K for the entire duration of the dosing procedure. Atomic manipulation of the cobalt atoms was performed by dragging the atoms in constant-current feedback mode with −130 mV <Vs< −100 mV and 6 nA <It< 12 nA (Supplementary Information).
Computing probability distributionsTo identify eachsswitching event in theIt(t) data, an algorithm was used that determines the locations of abrupt changes (steps) in data. The algorithm split the data into segments for which the difference between the residual error and the mean of the data in the segment is minimal37,38. Where necessary, the data was pre-processed by smoothing and corrected for thez-drift between tip and sample. After identifying all the switches in the data, the (local) mean of all the segments (Imean,i) was calculated between switching events. Discrete maxima in the histogram of the mean and/or raw current signal were used to identify a target current (Is,i) for eachsconfiguration; individual segments inIt(t) were assigned tosconfigurations based on the smallest value for |Imean,i−Is,i|. To computeP(s), the histogram of the discretized data was computed and the total values for eachsconfiguration were normalized to the total length of the measurement. To acquire distributionsP(k), eachkswitch was identified manually. For bothP(s) andP(k), all data was acquired with an acquisition time that was a minimum of two times longer than the convergence time.
Data availabilityThe data from this work can be obtained from the corresponding author upon reasonable request.
ReferencesStrukov, D. B., Snider, G. S., Stewart, D. R. & Williams, R. S. The missing memristor found.
Nature453, 80–83 (2008).
CASArticleGoogle ScholarChen, T. et al. Classification with a disordered dopant-atom network in silicon.
Nature577, 341–345 (2020).
CASArticleGoogle ScholarBose, S. K. et al. Evolution of a designless nanoparticle network into reconfigurable Boolean logic.
Nat. Nanotechnol.
10, 1048–1052 (2015).
CASArticleGoogle ScholarTorrejon, J. et al. Neuromorphic computing with nanoscale spintronic oscillators.
Nature547, 428–431 (2017).
CASArticleGoogle ScholarRomera, M. et al. Vowel recognition with four coupled spin-torque nano-oscillators.
Nature563, 230–234 (2018).
CASArticleGoogle ScholarGrollier, J., Querlioz, D. & Stiles, M. D. Spintronic nanodevices for bioinspired computing. proceedings of the IEEE.
Inst. Electr. Electron. Eng.
104, 2024–2039 (2016).
ArticleGoogle ScholarMcMahon, P. L. et al. A fully programmable 100-spin coherent ising machine with all-to-all connections.
Science354, 614–617 (2016).
CASArticleGoogle ScholarPrezioso, M. et al. Training and operation of an integrated neuromorphic network based on metal-oxide memristors.
Nature521, 61–64 (2015).
CASArticleGoogle ScholarBurr, G. W. et al. Neuromorphic computing using non-volatile memory.
Adv. Phys. X2, 89–124 (2017).
Google ScholarBorders, W. A. et al. Integer factorization using stochastic magnetic tunnel junctions.
Nature573, 390–393 (2019).
CASArticleGoogle ScholarFeldmann, J., Youngblood, N., Wright, C. D., Bhaskaran, H. & Pernice, W. H. P. All-optical spiking neurosynaptic networks with self-learning capabilities.
Nature569, 208–214 (2019).
CASArticleGoogle ScholarWang, Z. et al. Fully memristive neural networks for pattern classification with unsupervised learning.
Nat. Electron.
1, 137–145 (2018).
ArticleGoogle ScholarIshii, M. et al. On-Chip Trainable 1.4M 6T2R PCM Synaptic Array with 1.6K Stochastic LIF Neurons for Spiking RBM. InIEEE International Electron Devices Meeting (IEDM)14.2.1–14.2.4 (IEEE, 2019).
Kieferová, M. & Wiebe, N. Tomography and generative training with quantum boltzmann machines.
Phys. Rev. A96, 062327 (2017).
ArticleGoogle ScholarAmin, M. H., Andriyash, E., Rolfe, J., Kulchytskyy, B. & Melko, R. Quantum boltzmann Machine.
Phys. Rev. X8, 021050 (2018).
CASGoogle ScholarKappen, H. J. Learning quantum models from quantum or classical data.
J. Phys. A: Math. Theor.
53, 214001 (2020).
ArticleGoogle ScholarHinton, G. & Sejnowski, T. Optimal perceptual inference. InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition448–453 (IEEE, 1983).
Hertz, J., Krough, A. & Palmer, R. G.
Introduction to the Theory of Neural Computation(CRC Press, 1991).
Edwards, S. F. & Anderson, P. W. Theory of spin glasses. II.
J. Phys. F.
6, 1927 (1976).
ArticleGoogle ScholarEdwards, S. F. & Anderson, P. W. Theory of spin glasses.
J. Phys. F.
5, 965 (1975).
ArticleGoogle ScholarSherrington, D. & Kirkpatrick, S. Solvable model of a Spin-Glass.
Phys. Rev. Lett.
35, 1792 (1975).
ArticleGoogle ScholarHirjibehedin, C. F., Lutz, C. P. & Heinrich, A. J. Spin coupling in engineered atomic structures.
Science312, 1021–1024 (2006).
CASArticleGoogle ScholarKhajetoorians, A. A., Wiebe, J., Chilian, B. & Wiesendanger, R. Realizing all-spin–based logic operations atom by atom.
Science332, 1062–1064 (2011).
CASArticleGoogle ScholarKhajetoorians, A. A. et al. Atom-by-atom engineering and magnetometry of tailored nanomagnets.
Nat. Phys.
8, 497–503 (2012).
CASArticleGoogle ScholarToskovic, R. et al. Atomic spin-chain realization of a model for quantum criticality.
Nat. Phys.
12, 656–660 (2016).
CASArticleGoogle ScholarLoth, S., Baumann, S., Lutz, C. P., Eigler, D. M. & Heinrich, A. J. Bistability in atomic-scale antiferromagnets.
Science335, 196–199 (2012).
CASArticleGoogle ScholarKhajetoorians, A. A. et al. Current-Driven spin dynamics of artificially constructed quantum magnets.
Science339, 55–59 (2013).
CASArticleGoogle ScholarKiraly, B. et al. An orbitally derived single-atom magnetic memory.
Nat. Commun.
9, 3904 (2018).
ArticleGoogle ScholarKiraly, B. et al. Anisotropic Two-Dimensional screening at the surface of black phosphorus.
Phys. Rev. Lett.
123, 216403 (2019).
CASArticleGoogle ScholarPrishchenko, D. A., Mazurenko, V. G., Katsnelson, M. I. & Rudenko, A. N. Coulomb interactions and screening effects in few-layer black phosphorus: a tight-binding consideration beyond the long-wavelength limit.
2D Mater.
4, 025064 (2017).
ArticleGoogle ScholarBaldassi, C., Braunstein, A., Brunel, N. & Zecchina, R. Efficient supervised learning in networks with binary synapses.
Proc. Natl Acad. Sci. USA104, 11079–11084 (2007).
CASArticleGoogle ScholarPurves, D. et al.
Neuroscience(Sinauer Associates, 2019).
Heskes, T. M. & Kappen, B. Learning processes in neural networks.
Phys. Rev. A44, 2718–2726 (1991).
CASArticleGoogle ScholarFuechsle, M. et al. A single-atom transistor.
Nat. Nanotechnol.
7, 242–246 (2012).
CASArticleGoogle ScholarRudenko, A. N., Keil, F. J., Katsnelson, M. I. & Lichtenstein, A. I. Adsorption of cobalt on graphene: electron correlation effects from a quantum chemical perspective.
Phys. Rev. B86, 075422 (2012).
ArticleGoogle ScholarKolmus, A., Katsnelson, M. I., Khajetoorians, A. A. & Kappen, H. J. Atom-by-atom construction of attractors in a tunable finite size spin array.
New J. Phys.
22, 023038 (2020).
CASArticleGoogle ScholarLavielle, M. Using penalized contrasts for the change-point problem.
Signal Process.
85, 1501–1510 (2005).
ArticleGoogle ScholarKillick, R., Fearnhead, P. & Eckley, I. A. Optimal detection of changepoints with a linear computational cost.
J. Am. Stat. Assoc.
107, 1590–1598 (2012).
CASArticleGoogle ScholarDownload referencesAcknowledgementsThis project has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant no. 818399). This research was funded in part by ONR grant no. N00014-17-1-2569. A.A.K. and E.J.K. acknowledge the NWO-VIDI project ‘Manipulating the interplay between superconductivity and chiral magnetism at the single-atom level’ with project no. 680-47-534. B.K. acknowledges NWO-VENI project ‘Controlling magnetism of single atoms on black phosphorus’ with project no. 016.Veni.192.168.
Author informationThese authors contributed equally: Brian Kiraly, Elze J. Knol.
AffiliationsInstitute for Molecules and Materials, Radboud University, Nijmegen, the NetherlandsBrian Kiraly, Elze J. Knol, Werner M. J. van Weerdenburg & Alexander A. KhajetooriansDonders Institute for Neuroscience, Radboud University, Nijmegen, the NetherlandsHilbert J. KappenYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarContributionsB.K. and E.J.K. performed the experiments under the direction and supervision of A.A.K. B.K. and E.J.K. developed the data analysis, while B.K., E.J.K., H.J.K. and A.A.K. participated in the scientific analysis. W.M.J.v.W. developed the a.c. experimental setup. H.J.K. performed the Boltzmann machine modelling. A.A.K. and H.J.K. designed the experiments. The manuscript was written by B.K., E.J.K., H.J.K. and A.A.K.
Corresponding authorCorrespondence toAlexander A. Khajetoorians.
Ethics declarationsCompeting interestsThe authors declare no competing interests.
Additional informationPeer review informationNature Nanotechnologythanks Giuseppe Carleo, Matthew Ellis and the other, anonymous, reviewer(s) for their contribution to the peer review of this work.
Publisher’s noteSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
Supplementary informationSupplementary InformationSupplementary Figs. 1–12, Discussion and Tables 1–3.
Rights and permissionsReprints and PermissionsAbout this articleCite this articleKiraly, B., Knol, E.J., van Weerdenburg, W.M.J.
et al.
An atomic Boltzmann machine capable of self-adaption.
Nat. Nanotechnol.
16,414–420 (2021). https://doi.org/10.1038/s41565-020-00838-4Download citationReceived:04 May 2020Accepted:11 December 2020Published:01 February 2021Issue Date:April 2021DOI:https://doi.org/10.1038/s41565-020-00838-4Share this articleAnyone you share the following link with will be able to read this content:Sorry, a shareable link is not currently available for this article.
Provided by the Springer Nature SharedIt content-sharing initiativeFurther readingIsing machines as hardware solvers of combinatorial optimization problemsNature Reviews Physics(2022)Atomic-scale visualization of chiral charge density wave superlattices and their reversible switchingNature Communications(2022)You have full access to this article via your institution.
AdvertisementExplore contentAbout the journalPublish with usSearchAdvanced searchQuick linksNature Nanotechnology (Nat. Nanotechnol.
)ISSN1748-3395(online)ISSN1748-3387(print)nature.com sitemapDiscover contentPublishing policiesAuthor & Researcher servicesLibraries & institutionsAdvertising & partnershipsCareer developmentRegional websitesLegal & Privacy© 2022 Springer Nature Limited
