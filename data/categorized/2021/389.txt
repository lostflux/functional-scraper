old id = 1982
VQGAN+CLIP — How does it work?. The synthetic imagery (“GAN Art”) scene… | by Alexa Steinbrück | Medium
2021
https://alexasteinbruck.medium.com/vqgan-clip-how-does-it-work-210a5dca5e52

VQGAN+CLIP — How does it work? Alexa Steinbrück · Follow 5 min read · Aug 3, 2021 -- 3 Listen Share Note: This is a very short high-level introduction. If you’re more interested in code and details, check out my newer blogpost “Explaining the code of the popular text-to-image algorithm (VQGAN+CLIP in PyTorch)” ! The synthetic imagery (“GAN Art”) scene has recently seen a kind of productivity explosion: A new kind of neural network architecture capable of generating images from text was quickly popularized through a freely available Google Colab notebook. It enabled thousands of people to create stunning/fantastic/magical images, just by inputting a text prompt. Twitter, Reddit and other forums were flooded by these images, often accompanied with the hashtags #vqgan or #clip The text-to-image paradigm that VQGAN+CLIP popularized certainly opens up new ways to create synthetic media and maybe even democratizes “creativity”, by shifting the skillset from (graphical) execution or algorithmic instruction (programming) to nifty “prompt engineering”.
I see VQGAN+CLIP as another cool tool in the “Creative AI” toolbox. It’s time to look at this tool from a technical standpoint and explain how it works! What is VQGAN+CLIP Who made VQGAN+CLIP How does it work technically What is VQGAN What is CLIP How do VQGAN and CLIP work together What about the training data? Further reading and cool links 1. What is VQGAN+CLIP? VQGAN+CLIP is a neural network architecture that builds upon the revolutionary CLIP architecture published by OpenAI in January 2021.
VQGAN+CLIP is a text-to-image model that generates images of variable size given a set of text prompts (and some other parameters).
There have been other text-to-image models before (e.g. AttentionGAN), but the VQGAN+CLIP architecture brings it on a whole new level: “The crisp, coherent, and high-resolution quality of the images that these tools create differentiate them from AI art tools that have come before (…) These systems are the first ones that actually sort of meet ‘ the promise of text-to-image.
’” ( Vice ) VQGAN+CLIP has launched a new wave of AI-generated art works, as you can follow on Twitter under the hashtags #VQGAN and#CLIP, curated by the Twitter account @images_ai 2. Who made VQGAN+CLIP Around April 2021 Katherine Crowson aka @RiversHaveWings and Ryan Murdoch aka @advadnoun started doing experiments combining the open-source model CLIP (from OpenAI) and various GAN architectures.
Katherine Crowson, artist and mathematician wrote the Google Colab Notebook that combined VQGAN + CLIP. The notebook was shared a thousand times. It was originally in Spanish and has later been translated into English. Earlier, Ryan Murdoch combined BigGAN + CLIP, which was the inspiration for Crowson’s notebook.
3. How does it work technically? VQGAN+CLIP is a combination of two neural network architectures: VQGAN and CLIP. Let’s examine these two individually before we look at them in combination.
4. What is VQGAN? a type of neural network architecture VQGAN = V ector Q uantized G enerative A dversarial N etwork was first proposed in the paper “Taming Transformers” by University Heidelberg (2020) it combines convolutional neural networks (traditionally used for images) with Transformers (traditionally used for language) it’s great for high-resolution images Although VQGAN involves Transformers the models are not trained with text, but pure image data. They just apply the Transformer architecture that was previously used for text to images, which is an important innovation.
5. What is CLIP? a model trained to determine which caption from a set of captions best fits with a given image CLIP = C ontrastive L anguage– I mage P re-training it also uses Transformers proposed by OpenAI in Januar 2021 Paper: “Learning transferable visual models from natural language supervision” Git Repository: https://github.com/openai/CLIP Contrary to VQGAN, CLIP is not a generative model. CLIP is “just” trained to represent both text and images very well.
The revolutionary thing about CLIP is that it is capable of zero-shot learning.
That means that it performs exceptionally well on previously unseen datasets — Often better than models that have been trained exclusively on a particular dataset! Funfact: OpenAI published DALLE (remember the avocado chairs?) at the same time as CLIP. DALLE is an text-to-image model like VQGAN+CLIP. CLIP was open sourced completely, whereas DALLE wasn’t.
“The weights for DALL-E haven’t even been publicly released yet, so you can see this CLIP work as somewhat of a hacker’s attempt at reproducing the promise of DALL-E.” ( Source ) 6. How do VQGAN and CLIP work together In one sentence: CLIP guides VQGAN towards an image that is the best match to a given text.
Using the terminology introduced in Katherine Crowson’s notebook, CLIP is the “Perceptor” and VQGAN is the “Generator”.
“CLIP is a model that was originally intended for doing things like searching for the best match to a description like “a dog playing the violin” among a number of images. By pairing a network that can produce images (a “generator” of some sort) with CLIP, it is possible to tweak the generator’s input to try to match a description.” (@advanoun) It makes sense to look at the inputs and outputs of both models respectively: VQGAN: Like all GANs VQGAN takes in a noise vector, and outputs a (realistic) image.
CLIP on the other hand takes in: - (a) an image, and outputs the image features; or - (b) a text, and outputs text features.
The similarity between image and text can be represented by the cosine similarity of the learnt feature vectors.
By leveraging CLIPs capacities as a “steering wheel”, we can use CLIP to guide a search through VQGAN’s latent space to find images that match a text prompt very well according to CLIP.
Sidenote: Difference to “normal” GANs: Eventhough both VQGAN and CLIP models are pretrained when you use them in VQGAN, you basically train it (again) for every prompt you give to it. That is different to “normal” GANs where you train it one time (or you use a pretrained model) and then you just do inference in order to generate an image.
7. What about the training data? In the case of VQGAN+CLIP we deal with 2 models: VQGAN is trained on a mostly canonical dataset like ImageNet or COCO (this depends on the concrete model you use, of course. VQGAN is just the architecture). CLIP on the other hand was trained on a vast (and unknown) dataset of random internet material. Which makes it so exciting but also slightly scary and unpredictable.
8. Further reading and cool links Original Google Colab notebook by Katherine Crowson Google Colab notebook translated into English A wonderful VQGAN-CLIP-explainer with a focus on how VQGAN works, by Lj Miranda Youtube video that explains the CLIP paper: https://www.youtube.com/watch?v=T9XSU0pKX2E Nice in depth story by Charlie Snell about generative models based on CLIP: https://ml.berkeley.edu/blog/posts/clip-art/ Vice Article “AI-Generated Art Scene Explodes as Hackers Create Groundbreaking New Tools” (July 7th 2021) Abc.net: Paintings of Australia: https://www.abc.net.au/news/science/2021-07-15/ai-art-tool-makes-paintings-of-australia/100288386 Tutorial in Spanish by Jakeukalane Milegum Firisse : https://tuscriaturas.miraheze.org/w/index.php?title=Ayuda:Generar_im%C3%A1genes_con_VQGAN%2BCLIP English translation of the tutorial: https://tuscriaturas.miraheze.org/w/index.php?title=Ayuda:Generar_im%C3%A1genes_con_VQGAN%2BCLIP/English Nice technical article explaining BigSleep (BigGAN + CLIP): https://wandb.ai/gudgud96/big-sleep-test/reports/Image-Generation-Based-on-Abstract-Concepts-Using-CLIP-BigGAN--Vmlldzo1MjA2MTE -- -- 3 Follow Written by Alexa Steinbrück 195 Followers A mix of Frontend Development, Machine Learning, Musings about Creative AI and more Follow More from Alexa Steinbrück Alexa Steinbrück How to configure your Raspberry Pi Zero to play videos in a loop (= cheap and flexible solution for… This tutorial contains 2 sets of instructions depending on the type of Raspberry Pi Zero you have: 6 min read · Feb 27, 2022 -- Alexa Steinbrück Can ChatGPT do image recognition? This is a quick experiment to test if ChatGPT can do a computer vision task such as image recognition.
3 min read · Jan 20 -- 4 Alexa Steinbrück Let’s hope “The AI Dilemma” never gets turned into a Netflix series The new campaign by the “Center of Humane Technology” is dripping with sensationalist X-risk AGI hype and pseudo-science. That’s not how we… 11 min read · Jun 28 -- Alexa Steinbrück 10 useful things about Wikidata & SPARQL that I wish I knew earlier Wikidata is the nerdy cousin of Wikipedia. It’s a machine-readable user-editable graph database born in 2012. SPARQL is a query language… 5 min read · Jan 9, 2022 -- 1 Recommended from Medium Onkar Mishra Stable Diffusion Explained How does Stable diffusion work? Explaining the tech behind text to image generation.
6 min read · Jun 8 -- 3 Federico Bianchi in Towards Data Science Teaching CLIP Some Fashion Training FashionCLIP, a domain-specific CLIP model for Fashion 10 min read · Mar 7 -- 1 Lists Predictive Modeling w/ Python · The New Chatbots: ChatGPT, Bard, and Beyond · Practical Guides to Machine Learning · Natural Language Processing · Maximilian Vogel in MLearning.ai The 10 Best Free Prompt Engineering Courses & Resources for ChatGPT, Midjourney & Co.
A well-crafted prompt is magic.
8 min read · Sep 6 -- 10 Shashank Vats in 𝐀𝐈 𝐦𝐨𝐧𝐤𝐬.𝐢𝐨 A Guide to Fine-Tuning CLIP Models with Custom Data Artificial intelligence and machine learning have come a long way in recent years, with advances in the field allowing researchers and… 5 min read · Jun 1 -- 2 Robert John How to build a Text to Image App with free AI model in 30 lines of code Generate amazing Image with AI, a step by step guide to building a Test-to-Image App 6 min read · May 28 -- Shyam Patel Introduction to Diffusion Models and IMAGEN: The Magic Behind Text-to-Image Generation Introduction to Diffusion Model 4 min read · Sep 26 -- Help Status About Careers Blog Privacy Terms Text to speech Teams
