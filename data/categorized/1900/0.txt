old id = 1389
A Markov model of the Indus script - PMC
1900
http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2721819

The new PMC design is here!Learn moreabout navigating our updated article layout. ThePMC legacy viewwill also be available for a limited time.
An official website of the United States governmentThe .gov means it’s official.
Federal government websites often end in .gov or .mil. Before sharing sensitive information, make sure you’re on a federal government site.
The site is secure.
Thehttps://ensures that you are connecting to the official website and that any information you provide is encrypted and transmitted securely.
AccountOther FormatsActionsShareRESOURCESA Markov model of the Indus scriptRajesh P. N. RaoaDepartment of Computer Science and Engineering, University of Washington, Seattle, WA 98195;Nisha YadavbDepartment of Astronomy and Astrophysics, Tata Institute of Fundamental Research, Mumbai 400005, India;cCentre for Excellence in Basic Sciences, Mumbai 400098, India;Mayank N. VahiabDepartment of Astronomy and Astrophysics, Tata Institute of Fundamental Research, Mumbai 400005, India;cCentre for Excellence in Basic Sciences, Mumbai 400098, India;Hrishikesh Joglekard14, Dhus Wadi, Laxminiketan, Thakurdwar, Mumbai 400002, India;R. AdhikarieInstitute of Mathematical Sciences, Chennai 600113, India; andIravatham MahadevanfIndus Research Centre, Roja Muthiah Research Library, Chennai 600113, IndiaAuthor contributions: R.P.N.R., N.Y., M.N.V., H.J., and R.A. designed research; R.P.N.R., N.Y., H.J., and R.A. performed research; I.M. contributed new reagents/analytic tools; R.P.N.R., N.Y., M.N.V., H.J., and R.A. analyzed data; and R.P.N.R., M.N.V., and R.A. wrote the paper.
Associated DataAbstractAlthough no historical information exists about the Indus civilization (flourishedca. 2600–1900 B.C.), archaeologists have uncovered about 3,800 short samples of a script that was used throughout the civilization. The script remains undeciphered, despite a large number of attempts and claimed decipherments over the past 80 years. Here, we propose the use of probabilistic models to analyze the structure of the Indus script. The goal is to reveal, through probabilistic analysis, syntactic patterns that could point the way to eventual decipherment. We illustrate the approach using a simple Markov chain model to capture sequential dependencies between signs in the Indus script. The trained model allows new sample texts to be generated, revealing recurring patterns of signs that could potentially form functional subunits of a possible underlying language. The model also provides a quantitative way of testing whether a particular string belongs to the putative language as captured by the Markov model. Application of this test to Indus seals found in Mesopotamia and other sites in West Asia reveals that the script may have been used to express different content in these regions. Finally, we show how missing, ambiguous, or unreadable signs on damaged objects can be filled in with most likely predictions from the model. Taken together, our results indicate that the Indus script exhibits rich synactic structure and the ability to represent diverse content. both of which are suggestive of a linguistic writing system rather than a nonlinguistic symbol system.
The Indus (or Harappan) civilization flourished fromca. 2600 to 1900 B.C. in a vast region spanning what is now Pakistan and northwestern India. Its trade networks stretched to the Persian Gulf and the Middle East. The civilization emerged from the depths of antiquity in the late 19th century, when General Alexander Cunningham (1814–1893) visited a site known as Harappa and published a description (1), including an illustration of a tiny seal with characters in an unknown script. Since then, much has been learned about the Indus civilization through the work of archaeologists (see refs.
2and3for reviews), but the script still remains an enigma.
More than 3,800 inscriptions in the Indus script have been unearthed on stamp seals, sealings, amulets, small tablets, and ceramics (seeFig. 1Afor examples). Although there have been >60 claimed decipherments (4), none of these has been widely accepted by the community. Several obstacles to decipherment have been identified (4), including the lack of any bilinguals, the brevity of the inscriptions (the average inscription is ≈5 signs long), and our almost complete lack of knowledge of the language(s) used in the civilization.
The Indus script and Markov models. (A) Examples of Indus inscriptions on seals and tablets (Upper Left) and 3 square stamp seals (image credit: J. M. Kenoyer, Harappa.com). Note that inscriptions appear reversed on seals. (B) Example of a simple Markov chain model with 3 states. (C) Subset of Indus signs from Mahadevan's list of 417 signs (13).
Given these formidable obstacles to direct decipherment, we propose instead the analysis of the script's syntactical structure using techniques from the fields of statistical pattern analysis and machine learning (5). It is our belief that such an approach could provide valuable insights into the grammatical structure of the script, paving the way for a possible eventual decipherment. As a first step in this endeavor, we present here results obtained from analyzing the sequential structure of the Indus script using a simple type of probabilistic graphical model (6) known as a Markov model (7). Markov models assume that the current “state” (e.g., symbol in a text) depends only on the previous state, an assumption that renders learning and inference over sequences tractable. Although this assumption is a simplification of the complex sequential structure seen in languages, Markov models have proved to be extremely useful in analyzing a range of sequential data, including speech (8) and natural language (9). Here, we apply them to the Indus script. A major goal of this article is to provide an exposition of Markov models for those unfamiliar with them and to illustrate their usefulness in the study of an undeciphered script. We leave the application of more advanced higher-order models and grammar induction methods to other papers (10) and future work.
Markov Models for Analyzing the Indus Script.
A Markov model (also called a Markov chain) (7,11,12) consists of a finite set ofN“states”s1,s2, …,sN(e.g., the states could be the signs in the script) and a set of conditional (or transition) probabilitiesP(si|sj) that determine how likely it is that statesifollows statesj. There is also a set of prior probabilitiesP(si) that denote the probability that statesistarts a sequence.
Fig. 1Bshows an example of a “state diagram” for a Markov model with 3 states labeled A, B, and #, where A and B denote letters in a language, and # denotes the end of a text.
Fig. 1Balso shows the prior probabilitiesP(si) and the transition probabilitiesP(si|sj), picked arbitrarily here for the purposes of illustration. Some example sequences generated by this Markov model are BAAB, ABAB, B, etc. (the terminal sign # is not shown). Texts that are not generated by this Markov model include all texts that contain a repetition of B (…BB…) and all texts that end in A, because these are precluded by the transition probability table. A more realistic example would be a Markov model for English texts involving the 26 letters of the alphabet plus space. In this case, the transition probability table (or matrix) would be of size 27 × 27. In the matrix, we would expect, for example, higher probabilities for the letter “s” to be immediately followed by letters such as “e,” “o,” or “u,” than letters such as “x” or “z,” because of the morphological structure of words in English. A Markov model learned from a corpus of English texts would capture such statistical regularities.
Markov models [and their variants, hidden Markov models (HMMs)] are special cases of a more general class of probabilistic models known as graphical models (6). Graphical models can be used to model complex relationships between states, including higher-order dependencies such as the dependence of a symbol on the pastN-1 symbols [equivalent toN-gram models in language modeling (9)]. In this article, we focus on first-order Markov models. In other work (10), we have comparedN-gram models for Indus texts using the information-theoretic measure of “perplexity” (9). We have found that the bulk of the perplexity in the Indus corpus can be captured by a bigram (n= 2) model (or equivalently, a first-order Markov model), supporting the usefulness of such models in analyzing the Indus script.
We focus in this article on 3 applications of Markov models to analyzing the Indus script: (i) Sampling: We show how new sample texts can be generated by randomly sampling from the prior probability distributionP(si) to obtain a starting sign (sayS1) and then sampling from the transition probabilitiesP(si|S1) to obtain the next signS2, and so on. The string generation process can be terminated by assuming an end of text (EOT) sign that transitions to itself with probability one. (ii) Likelihood computation: Ifxis a string of lengthLand is of the formx1x2…xL, where eachxiis a sign from the list of signs, the likelihood thatxwas generated by a Markov modelMcan be computed as:P(x|M) =P(x1x2…xL|M) =P(x1)P(x2|x1)P(x3|x2)…P(xL|xL-1). The likelihood of a string under a particular model tells us how closely the statistical properties of the given string match those of the original strings used to learn the given model. Thus, if the original strings were generated according to the statistical properties of a particular language, the likelihood is useful in ascertaining whether a given string might have been generated by the same language. (iii) Filling in missing signs: Given a learned Markov modelMand an input stringxwith some known signs and some missing signs, one can estimate the most likely complete stringx* by calculating the most probable explanation (MPE) for the unknown parts of the string (seeMethodsfor details).
The Indus Script.
A prerequisite for understanding the sequential structure of a script is to identify and isolate its basic signs. This task is particularly difficult in the case of an undeciphered script such as the Indus script, because there is considerable variability in the rendering of the signs, making it difficult to ascertain whether 2 signs that look different are stylistic variants of the same sign or 2 independent signs. After an analysis of the positional statistics of variant signs in a corpus of known inscriptions (ca. 1977), Mahadevan arrived at a list of 417 independent signs in his concordance (13) [Parpola used similar methods to estimate a slightly shorter list of 386 signs (14)]. We used this list of 417 signs as the basis for our study.
Fig. 1Cshows a subset of signs from this list.
Barring a few exceptions (see p. 14 in ref.
13), the writing direction is generally accepted to be predominantly from right to left (i.e., left to right in seals and right to left in the impressions). There exists convincing external and internal evidence supporting this hypothesis (e.g., refs.
4,13, and14). We consequently learned the sequential structure of the Indus texts based on a right-to-left reading of their signs, although a Markov model could equally well be learned for the opposite direction.
ResultsMarkov Model of Indus Texts.
The learned Markov model provides several interesting insights into the nature of the Indus script. First, examining the learned prior probabilitiesP(si) provides information about how likely it is that a particular signsistarts a text.
Fig. 2Bshows this probability for the 10 most frequently occurring signs (Fig. 2A) in the corpus of Indus inscriptions.
Learned Markov model for the Indus script. (A) The 10 most frequently occurring Indus signs in the dataset. The numbers within parentheses in the bottom row are the frequencies of occurrence of each sign depicted above in the dataset used to train the model. (B) Prior (starting) probabilitiesP(si) learned from data, shown here for the 10 most frequently occurring Indus signs depicted inA. (C) Matrix of transition probabilitiesP(si|sj) learned from data. A 418 × 418 matrix was learned but to aid visualization, only the 10 × 10 portion corresponding to the 10 most frequent signs is shown.
An examination ofFig. 2Breveals that certain frequently occurring signs such asand(signs numbered 3 and 10 inFig. 2B) are much more likely to start a text than the others. On the other hand, certain signs such as(the most frequent sign in the corpus) andare highly unlikely to start a text (they are, in fact, highly likely to end a text; seeFig. 3CandTable 1). These observations have also been made by others (13–16).
Some characteristics of the Indus script extracted from the learned Markov model. (A) Signs most likely and least likely to start an Indus text. (B) Highly probable pairs (top row) and longer sequences of Indus signs predicted by the learned transition matrix. The numbers above the arrows are the learned probabilities for the transitions indicated by the arrows. (C) Signs most likely and least likely to end an Indus text. (D) Top 10 signs with the highest probability of repeating (sign following itself).
Table 1.
Signs that tend to be followed by or follow each of the top 10 frequently occurring signsNote: The table assumes a right to left reading of the texts. The number below each sign is the value from the transition probability matrix for the corresponding sign preceding (left column) or following (right column) a given sign (center column). Only signs that occur 20 or more times in the dataset are included in this table.
From the learned values forP(si), one can also extract the 10 most likely signs to start a text and the 10 least likely signs to do so (among signs occurring at least 20 times in the dataset) (Fig. 3A). These results suggest that some signs that look similar, such asand, subserve different functions within the script and thus cannot be considered variants of the same sign.
The matrix of transition probabilitiesP(si|sj) learned from data is shown inFig. 2C(only the portion corresponding to the 10 most frequent signs is shown). The learned transition probabilities for Indus sign pairs can be shown to be significant, in the sense that the null hypothesis for independence can be rejected (see ref.
10for details). To interpret the transition probability matrix, consider the entry with the highest value (colored white inFig. 2C). This value (≈0.8) corresponds to the probabilityP(si= 2 |sj= 3) and indicates that the signis followed by the signwith a very high probability. Other highly probable pairs and longer sequences of signs are shown inFig. 3B.
The transition matrix also tells us which signs are most likely to end a text (these are signs that have a high probability of being followed by the EOT symbol).
Fig. 3C(top row) shows 10 signs that occur at least 20 times in the dataset, which have high probabilities (in the range 0.87–0.39) of terminating a text.
Fig. 3C(bottom row) shows 10 frequently occurring signs (occurring at least 20 times in the dataset) that have the least probability of terminating a text.
The diagonal of the transition matrix [representingP(si|si)] tells us how likely it is that a given sign follows itself (i.e., repeats).
Fig. 3Dshows 10 signs with the highest probability of repeating. The signoccurs in only one inscription where it occurs as a pair, whereas the signoccurs in 8 inscriptions, 6 times as a pair. More interestingly, the frequently occurring signoccurs as a pair in 33 of the 58 inscriptions in which it occurs. The presence of such repeating symbols in the Indus texts puts strong constraints on their semantic interpretation, because the interpretation has to remain intelligible when repeated.
The dark regions in the transition matrix inFig. 2Cindicate a transition probability that is near zero. An inspection of the transition matrix reveals that for a given sign, there exists a specific subset of signs that have a high probability of following that sign, with the rest of the entries of the transition matrix being close to zero. This suggests that the Indus texts exhibit an intermediate degree of flexibility between random and rigid symbol order: there is some amount of flexibility in what the next sign can be, but not too much flexibility. This intermediate degree of flexibility (or randomness) of strings is also exhibited by most natural languages. A comparison of the Indus script with other linguistic and nonlinguistic systems using the measure of conditional entropy to quantify this flexibility is given in ref.
17(see also ref.
18). One way in which such structure in the transition matrix could arise is from grammar-like rules that allow only a specific subset of signs to follow any given sign. The existence of such rules makes it more likely that the Indus texts encode linguistic information, rather than being random or rigid juxtapositions of emblems or religious and political symbols (19).
Table 1provides additional support for the hypothesis that sequences of signs in the Indus texts may be governed by specific rules.
Table 1shows that for each of the 10 most frequently occurring signs in the dataset, only a small set of signs has a high probability of following the given sign or being followed by it. The possibility of grammar-like rules is further strengthened by the observation that many of these sets of high probability signs exhibit similarities in visual appearance: for example, among the group of signs with a high probability of following the signare the signs,,,, and, as well asand. Other such examples can be observed inTable 1and in the full transition matrix, hinting at distinct syntactic regularities underlying the composition of Indus sign sequences.
Generating New Sample Indus Texts from the Learned Model.
The Markov model learned from the Indus inscriptions acts as a “generative” model, in that one can sample from it and obtain new sequences of signs that conform to the sequential statistics of the original inscriptions (albeit limited to pairwise statistics).
Fig. 4Aprovides an example of a new text obtained by sampling the learned model, along with the closest matching text in the original corpus. The closest match was computed using the “string edit distance” between strings, which measures the number of additions, deletions, and replacements needed to go from one string to the other. The generated text inFig. 4Ais not identical to the closest matching Indus text but differs from it in 2 interesting ways. First, the symboloccurs as the starting symbol instead of. An examination of the transition matrix reveals that bothandhave a high probability of being followed by the sign. Second, the sample text contains the signinstead ofin the same position. This suggests thatandmay have similar functional roles, given that they occur within similar contexts.
Generating new Indus texts and filling in missing signs. (A) (Upper) New sequence of signs (read right to left) generated by sampling from the learned Markov model. (Lower) Closest matching actual Indus inscription from the corpus used to train the model. (B) Inferring functionally related signs. A sample from the Markov model (top) is compared with 2 closest matching inscriptions in the corpus, highlighting signs that function similarly within inscriptions. (C) Filling in missing signs in a known altered text. (Top) Inscription obtained by replacing 2 signs in a complete inscription with blanks (denoted by ?). (Middle) The MPE output. (Bottom) Closest matching text in the corpus. (D) Filling in an actual incomplete Indus inscription. (Top) An actual Indus inscription (from ref.
14, figure 4.8) with 2 signs missing. (Middle) MPE output. (Bottom) Closest matching complete Indus text in the corpus. (E) Filling in of another actual incomplete inscription from ref.
13. (Left) Text with an unknown number of missing signs (hashed box). (Right) Three complete texts of increasing length predicted by the model. The first and third texts actually exist in the corpus.
Fig. 4B(top row) gives another example of a new generated Indus text and 2 closest matching texts from the Indus dataset of inscriptions. Once again, based on their interchangeability in these and other texts, one may infer that the signs,, andshare similar functional characteristics in terms of where they may appear within texts.
Filling in Incomplete Indus Inscriptions.
Many of the inscribed objects excavated at various Indus sites are damaged, resulting in inscriptions that contain missing or illegible signs. To ascertain whether the model trained on complete texts could be used to fill in the missing portion of incomplete inscriptions, we first generated an artificial dataset of “damaged” inscriptions by taking complete inscriptions from the Indus dataset and obliterating one or more signs.
Fig. 4C(top row) shows an example of one such inscription. The complete inscription (middle row) predicted by the Markov model using the MPE method matched a preexisting Indus inscription (bottom row). A detailed cross-validation study of filling in performance revealed that a first-order Markov model can correctly predicted deleted signs with ≈74% accuracy (10), which is encouraging given that only pairwise statistics were learned. Further improvement in performance can be expected with higher-order models.
Fig. 4D(top row) shows an actual Indus text with missing signs (from ref.
14). The middle row shows the completed text generated by the MPE method, with the closest matching Indus text at the bottom. The closest matching text differs from the generated text in 2 ways: the “modifier”has been inserted and the signis replaced by a visually similar sign. The text shown at the left ofFig. 4Eis another actual Indus inscription with an unknown number of signs missing (from ref.
13). The 3 texts shown at the right are MPE outputs assuming 1, 2, or 3 signs are missing. The first and third MPE texts actually occur in the Indus corpus, whereas the middle text contains the frequently occurring pair. Additional examples of filling in of damaged texts are given insupporting information (SI)Table S1.
Testing the Likelihood of Indus Inscriptions.
The likelihood of a particular sequence of Indus signs with respect to the learned Markov model tells us how likely it is that the sign sequence belongs to the putative language encoded by the Markov model. We found that altering the order of signs in an existing Indus text typically caused the likelihood of the text to drop dramatically (SI TextandFig. S1), supporting the hypothesis that the Indus texts are subject to specific syntactic rules determining the sequencing of signs.
Applying this analysis to Indus texts discovered outside the Indus valley, for example, in Mesopotamia and other sites in West Asia, we found that the likelihoods of most of these inscriptions are extremely low compared with their counterparts found in the Indus valley (Table 2). Indeed, the median value of likelihoods for the West Asian texts is 6.40 × 10−13, which is ≈100,000 times less than the median value of 1.12 × 10−7obtained for a random set of 100 texts of Indus valley origin that were excluded from the training set for comparison purposes. These findings suggest the intriguing possibility that the Indus script may have been used to represent a different language or subject matter by Indus traders conducting business in West Asia or West Asian traders sending goods back to the Indus valley. Such a possibility was earlier suggested by Parpola, who noted that the West Asian texts often contain unusual sign combinations (14). Our results provide a quantitative basis for this possibility. The low likelihoods arise from the fact that many of the West Asian texts inTable 2contain sign combinations such as,,,, andthat never appear in any texts found in the Indus valley, even though the signs themselves occur frequently in other combinations in the Indus valley texts.
Table 2.
Likelihood of West Asian Texts compared with Indus valley textsNote: Only complete and unambiguous West Asian texts from ref.
13are included in this table. Two texts have a likelihood of zero, because they each contain a symbol not occurring in the training dataset used to learn the model. The last row shows for comparison the median likelihood for a randomly selected set of 100 texts originating from within the Indus valley, which were held out and not used for training the Markov model (these 100 texts had the same average length as the West Asian texts).
DiscussionA number of researchers have made observations regarding sequential structure in the Indus script, focusing on frequently occurring pairs, triplets, and other groups of signs (13–15,20). Koskenniemi suggested the use of pairwise frequencies of signs to construct syntax trees and segment texts, with the goal of eventually deriving a formal grammar (20). More recently, Yadav, Vahia, and colleagues (15,16) have performed statistical analyses of the Indus texts, including explicit segmentation of texts based on most frequent pairs, triplets, and quadruplets.
In this article, we provide an investigation of sequential structure in the Indus script based on Markov models. An analysis of the transition matrix learned from a corpus of Indus texts provided important insights into which signs tend to follow particular signs and which signs do not. The transition matrix also provides a probabilistic basis for extracting common sequences and subsequences of signs in the Indus texts. We demonstrated how the learned Markov model can be used to generate new sample texts, revealing groups of signs that tend to function similarly within a text. The approach can also be used to fill in missing portions of illegible and incomplete Indus inscriptions based on the corpus of complete inscriptions. Finally, a comparison of the likelihood of Indus inscriptions discovered in West Asian sites with those from the Indus valley suggests that many of the West Asian inscriptions may represent subject matter different from Indus valley inscriptions.
Our results appear to favor the hypothesis that the Indus script represents a linguistic writing system. Our Markov analysis of sign sequences, although restricted to pairwise statistics, makes it clear that the signs do not occur in a random manner within inscriptions but appear to follow certain rules: (i) some signs have a high probability of occurring at the beginning of inscriptions whereas others almost never occur at the beginning; and (ii) for any particular sign, there are signs that have a high probability of occurring after that sign and other signs that have negligible probability of occurring after the same sign. Furthermore, signs appear to fall into functional classes in terms of their position within an Indus text, where a particular sign can be replaced by another sign in its equivalence class. Such rich syntactic structure is hard to reconcile with a nonlinguistic system. Additionally, our finding that the script may have been versatile enough to represent different subject matter in West Asia argues against the claim that the script merely represents religious or political symbols. Other arguments in favor of the linguistic hypothesis for the Indus script are provided by Parpola (21).
Our study suffers from some shortcomings that could be addressed in future work. First, our first-order Markov model captures only pairwise dependencies between signs, ignoring important longer-range dependencies. Higher-order Markov models (10) and other types of probabilistic graphical models (6) would allow more accurate modeling of such dependencies. A second potential shortcoming is our use of an Indus corpus of texts from 1977 (13). New texts and signs have since been discovered, and new sign lists have been suggested with up to 650 signs (22). However, the types of new material that have been discovered are in the same categories as the texts in the 1977 corpus, namely, more seals, tablets, etc., exhibiting syntactic structure similar to that analyzed in this article. Also, the new signs that have been discovered are still far outnumbered by the most commonly occurring signs in the 1977 corpus, most of which also occur frequently in the newly discovered material. Therefore, we expect the variations in sign frequencies due to the new material to only slightly change the conditional probabilities in the Markov model. Nevertheless, a more complete analysis with all known texts and new sign lists remains an important objective for the future. Additionally, our analysis combined data from different geographical locations. A more detailed site-by-site analysis could shed light on the interesting question of whether there are differences in the sequential patterning of signs across regions.
In summary, the results we have presented strongly suggest the existence of specific rules governing the sequencing of Indus signs in a manner that is indicative of an underlying grammar. A formidable, but perhaps not insurmountable, challenge for the future (also articulated in refs.
14,15, and20) is to apply statistical and machine learning techniques to infer a grammar directly from the corpus of available Indus inscriptions.
MethodsWe applied our Markov model analysis techniques to a subset of Indus texts extracted from Mahadevan's 1977 concordance (13). This dataset, called EBUDS (15), excludes all texts from Mahadevan's concordance containing ambiguous or missing signs and all texts having multiple lines on a single side of an object. In the case of duplicates of a text, only one copy is kept in the dataset. This resulted in a dataset containing 1,548 lines of text, with 7,000 sign occurrences. We used Mahadevan's list of 417 signs plus an additional EOT sign to denote the end of each text. Signs were fed to the model from right to left in each line of text, ending in the EOT sign.
Learning a Markov Model from Data.
The parameters of a Markov model include the prior probabilitiesP(si) and the transition probabilitiesP(si|sj). A simple method for computing these probabilities is counting frequencies, e.g.,P(si) is set equal to the number of times signsibegins a text divided by the total number of texts. This can be shown to be equivalent to maximum likelihood estimation of the parameters (9). However, such an estimate, especially for the transition probabilitiesP(si|sj), can yield poor estimates when the dataset is small, as is the case with the Indus script, because many pairs of signs may not occur in the small dataset at all, even though their actual probabilities may be nonzero. There has been extensive research on “smoothing” techniques, which assign small probabilities to unseen pairs based on various heuristic principles (see chapter 6 in ref.
9for an overview). For the results in this article, we used modified Kneser–Ney smoothing (23) (based on ref.
24), a technique that has been shown to outperform other smoothing techniques on a number of benchmark datasets (23).
Filling in Missing Signs.
Letx=x1x2…xLbe a string of lengthLwhere eachxiis a random variable whose value can be any sign from the list of signs. LetXdenote the set ofxifor which the values are given andYthe set ofxiwith values missing. For a general graphical model M, the MPE for the missing variablesYgiven the “evidence”Xis computed asY* = arg maxYP(Y|X,M). For the learned Markov model,M= (α, π), where αi=P(si) and πij=P(si|sj). For a Markov model, we can compute the MPEY* = arg maxYP(Y|X, α, π) using a version of the “Viterbi algorithm” (25) (see refs.
8and12for algorithmic details).
Supplementary MaterialAcknowledgments.
This work was supported by the Packard Foundation, the Sir Jamsetji Tata Trust, the Simpson Center for the Humanities at the University of Washington (UW), the UW College of Arts and Sciences, the UW College of Engineering and Computer Science and Engineering Department, and the Indus Research Centre, Chennai, India.
FootnotesThe authors declare no conflict of interest.
This article contains supporting information online atwww.pnas.org/cgi/content/full/0906237106/DCSupplementalReferencesOther FormatsActionsShareRESOURCESConnect with NLMNational Library of Medicine8600 Rockville PikeBethesda, MD 20894Web PoliciesFOIAHHS Vulnerability DisclosureHelpAccessibilityCareers
