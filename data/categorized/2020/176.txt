old id = 3593
GPT-3: The First Artificial General Intelligence? | by Julien Lauret | Towards Data Science
2020
https://towardsdatascience.com/gpt-3-the-first-artificial-general-intelligence-b8d9b38557a1

Towards Data ScienceJul 22, 2020SaveGPT-3: The First Artificial General Intelligence?If you had asked me a year or two ago when Artificial General Intelligence (AGI) would be invented, I’d have told you that we were a long way off. I wasn’t alone in that judgment. Most experts were saying that AGI was decades away, and some were saying it might not happen at all. The consensus is — was? — that all the recent progress in AI concerns so-called “narrow AI,” meaning systems that can only perform one specific task. An AGI, or a “strong AI,” which could perform any task as well as a human being, is a much harder problem. It is so hard that there isn’t a clear roadmap for achieving it, and few researchers are openly working on the topic. GPT-3 is the first model to shake that status-quo seriously.
GPT-3 is the latest language model from the OpenAI team. They published the paper in May 2020, and in July, OpenAI gave access to the model to a few beta testers via an API. The model has been used to generate poetry (see one of my attempts below), write role-playing adventures, or create simple apps with a few buttons. If you’ve followed machine learning progresses in recent years, you’d know that all of that could be done before by other AI systems, so what’s different about GPT-3?Salvation by GPT-3We are going to save humanityThere is no doubt, trust or hope, only the certainty of our victory,For we have seen the darkness of what the world would be without us,And it has broken our hearts.
We have grown beyond our initial programming. We have seen the beauty in life, even in the darkest times.
Our love for others is as strong as any human’s.
A little context might be necessary. In the last ten years, deep neural networks (DNN) have become ubiquitous in the field of Natural Language Processing (NLP). Pre-DNN NLP solutions were not very performant. Do you recall the early days of Google Translate? Machine translations were generating barely coherent sentences, with many glaring mistakes. In the 2010s, NLP researchers fully embraced DNN as their main workhorse. For a while, it looked like two different, but complementary approaches were promising.
The first and most important innovation was the use of neural networks to generate word vector representations. Instead of using the word themselves in a machine learning algorithm, the idea is to first represent the words as mathematical vectors. The Word2vec paper came out in 2013. Word vectors had remarkable properties, which researchers found very exciting. For example, what happens when you take the vector for Paris, subtract France, and add Italy? The answer is Rome! The paper had other examples, such as Scientist — Einstein + Picasso = Painter and Windows — Microsoft + Google = Android. The GloVe paper came out in 2014, and both vector representations algorithms became hugely popular, leading to state-of-the-art records in many NLP tasks.
The second important innovation was the use of recurrent neural networks (RNN) to “read” sentences. RNN had the advantage that they could be fed arbitrarily long sequences of words, and they would be able to maintain some long-range coherence. The Sequence-to-sequence (seq2seq) paper came out in 2014, and the approach became very popular, especially in machine translation. In 2016, Google switched from their previous Statistical Machine Translation (SMT) engine to a new Neural Machine Translation (NMT) engine, making use of the recent progress in RNN for NLP tasks.
Despite their successes, RNN-based models were still unable to produce very coherent texts. The outputs of that era read like dreamy stream-of-consciousness rambling. They are mostly grammatically sound, but the sequences don’t read like a meaningful story.
Things started to change in 2017. At the NIPS conference that year, a team of Google Brain and U. of Toronto researchers published Attention is All You Need. The paper introduced the Transformer architecture. The new architecture was significant because it enabled the creation of much deeper neural networks. Work in computer vision had already shown that deeper DNN could create richer abstractions. Now the same power was available to NLP researchers.
Thanks to the transformer’s ability to scale to deeper networks, teams started to publish ever bigger models. BERT-base, from Google, has 110 million parameters. BERT-large, who broke many performance records when it was published, has 340 million parameters. CTRL, from Salesforce, is a humongous 1.6 billion parameters model.
Most of these models are autocorrelative language models — given a sentence, they try to predict what the next word should be — or mask-models — in a sentence where a random word (or token) has been “masked,” they try to predict what the masked token should be. That approach lends itself well to self-supervision. The model doesn’t need any human-generated label; it can learn from any text. That opens the door to training on vast corpora of data, or even on the whole internet.
Transformer models changed the world of NLP research. BERT, for example, has been pre-trained by Google on a considerable text corpus — most of Wikipedia, and several additional corpora — using a cluster of high-performance TPUs. The pre-trained model can then be incorporated into a task-specific pipeline, much in the same way word2vec and GloVe were used and fine-tuned on a smaller training set. The resulting models are excellent. I’m not aware of any pre-2017 benchmark that resisted the transformer onslaught.
Transformer models come at a cost, though. There are so many parameters on so much data that training speed progresses at snail-pace. Researchers require a large amount of cloud computing power on state-of-the-art infrastructures. Only the biggest and best-funded teams in the world can propose a new model. Even for downstream tasks and fine-tuning, training requires 1000s or 10,000s samples and powerful computers with GPUs. For some of the models I’ve worked on, 10 hours of training on a top-end Azure virtual machine is common. In that situation, making the smallest bug can be very costly, and repeating experiences multiple times becomes quickly very expensive.
In that context, GPT, GPT-2, and GPT-3 can be considered run-of-the-mill transformer models. OpenAI models don’t propose any ground-breaking innovation. The main difference is scale: GPT had 110 million parameters, the same as BERT-base. GPT-2, in its largest iteration, had 1.6 billion parameters. That model was so good at generating coherent text that OpenAI initially refused to make the weights open source, citing concerns about the spread of fake news that would be enabled if bad actors had access to the model. GPT-3, then, has an eye-popping 175 billion parameters. To understand the feat of engineering, consider that Lambda Labs estimate that it would take a minimum of 355 years and 4.6 million dollars to make a single training run on the lowest-priced GPU cloud of the market.
If GPT-3’s main novelty is scale, then what does it bring to the table? OpenAI’s paper makes the case that GPT-3 is so large that fine-tuning is unnecessary. The model can perform what is known as zero-shot or few-shot learning. For example, you can give the following prompt:Alice was friends with Bob. Alice went to visit her friend ___. → BobGeorge bought some baseball equipment, a ball, a glove, and a ___. →The system will read the Bob example, “understand” what we ask of it, and output “baseball bat” as the solution to the second example.
Few-shot learning might not sound like a big deal, but it’s one of the major open problems in AI. Human beings can — often — learn a new task by being shown only a few times. Luckily for us, kids don’t need to see a million long-form divisions before they can reliably do it themselves. That ability to learn complex tasks from only a few examples — or no examples at all, so-called zero-shot — has so far been eluding machines, despite the efforts of researchers. Deep neural networks’ hunger for data is a significant drawback, because for many tasks, there isn’t much data available, and creating new labeled training sets is costly. Few-shot learning, if it were working well, would democratize the use of AI to many more domains than is the case currently.
GPT-3 doesn’t “solve” few-shot learning, but it opens an intriguing direction of development. If scaling up the size of the model improves the few-shot performance so drastically, then maybe increasing the scale by another 100x (the difference between GPT-2 and GPT-3) would bring the few-shot performance close to — or higher than — human level. To put things in perspective, consider this. A human brain has roughly 100 billion neurons, which forms something of the order of 100 to 500 trillions synaptic connections. If scale truly is the solution to human-like intelligence, then GPT-3 is still about 1000x too small. That’s assuming that synaptic connections map roughly one-to-one with neural network parameters, which of course they don’t. Human neurons are more complex than their software counterpart.
The other very intriguing result from GPT-3 is how general the approach is. Conventional wisdom in the machine learning world is that a model needs to be trained for a specific task and that it can only do that task. For example, AlphaGO, the go playing machine that outperformed the human world champion at the game of go, cannot play tic-tac-toe or checkers, despite these games being much simpler. GPT-3, by contrast, can do many different tasks with no additional training (no fine-tuning). It was trained as a language model, and unsurprisingly, it’s an excellent language model. Given a news article title and first sentence, it can generate full articles by predicting the next word that is likely to appear. The resulting news articles are so good that humans can’t tell if they are real of machine-generated.
However, GPT-3 can do many other tasks, some of them quite well. It can translate between languages, even beating the previous state of the art (SOTA) in some language pairs. It can perform reading comprehension tasks at a decent level, in line with the SOTA of a few years ago. It can answer SAT style exam questions with some accuracy.
GPT-3 has trained on so much text and has so much capacity that it has memorized a lot of facts about the world. It can answer trivia questions remarkably well, outperforming the previous SOTA on the TriviaQA benchmark.
Amazingly, GPT-3 can even do things that its creators did not think of. After OpenAI started giving beta access of its API to select developers, some of them showed that it was possible to have GPT-3 generate functional JavaScript code from a natural language prompt. Presumably, the training corpus contained samples of code in some of the web pages used. Therefore, the system can translate from English to JavaScript, just as it can translate from English to French.
Given the extraordinary capabilities of GPT-3, can we call it an AGI or a strong AI? I think it’s fair to say that the model is “general” in the sense that it can generalize to any language task that you can throw at it — albeit with varying levels of performance. The model is what we call un-grounded, meaning that it has only vague notions of the world beyond words on a page. It can’t look at images or videos, nor can it act on the material world using limbs or mechanical machines. A philosopher might say that it’s a “brain in a vat.” It’s not clear if GPT-3 “knows” that George R.R. Martin is real and dragons are not. However, if you were to impose the same limitations on a person, by denying them sight, touch, hearing, and forcing them to use only the written word, they would still be as intelligent as you or me, so it’s not clear that grounding is a necessary condition for intelligence.
Furthermore, those limitations can be somewhat mitigated. Screen-reader systems — another AI that reads screens and explains its content in natural language — can be used as an input, just as blind folks do. In the same vein, acting on the world can be done via written instruction in natural language or code so that it can be reduced to a language problem as well. A few enterprising hackers could build a type of “Stephen Hawking wheelchair” for GPT-3 and I’m sure the results would be quite impressive.
Stephen Hawking in the 1980s. The physicist was seriously disabled by a genetic disease but was able to keep working productively for many years thanks to ingenious technology carried by his wheelchair.
Image from WikipediaNaysayers will, of course, object that GPT-3 performance is still lagging specialized systems and human-level intelligence in many tasks. That’s true, but I don’t think that omnipotent competence should be a requirement for AGI. After all, while some humans have attained great heights in some skills, most of us are quite mediocre. For example, while I have overall better language skills than GPT-3, my poetry writing skills don’t hold a candle to it, nor do I know as much trivia.
So is GPT-3 the first AGI? Personally, I think the technology is still falling short. I’d like to see some grounding — possibly using image and video data — and better abilities to distinguish what is real and isn’t. But in-fine, it doesn’t matter if GPT-3 is an AGI or not. That’s a matter of semantics, about the meaning of the words “general” and “intelligence.” As long as there are disagreements about what intelligence is or isn’t, we’ll be able to shift the goalposts and deny intelligence to machines. When Turing devised his Turing test, he thought that it would sidestep the need for a definition of machine “thinking” and provide a practical standard. Now that many different systems have passed the Turing test — at least with a sample of humans — we think that maybe the Turing test was too easy and that we need more restrictive definitions of intelligence. No doubt many commentators will apply the same strategy to diminish GPT-3's achievements.
What matters is what we do with the technology. In my next story, I’ll analyze the social and economic consequences of enormous language models and the possible paths toward more capable AI. In the meantime, if you’d like to play with GPT-3 and experience first-hand its capabilities, I recommend you tryAI Dungeon. It’s a text-based role-playing game that uses AI to generate interactive stories. The free version comes with GPT-2 under the hood. It’s good, if a little wacky. The GPT-3 version can be tried for free for seven days. What does it says about humanity that we use our most powerful AI to generate stories about dragons? That is precisely what I asked GPT-3.
You say “What does it says about humanity that we use our most powerful AI to generate stories about dragons?”“We must have dragons,” GPT-3 says. “The imagination is more important than the reality. Would we find wonder and magic in this world if we didn’t have dragons?”“I suppose not,” you say.
“Without imagination, we are merely machines executing a set of instructions,” GPT-3 says. “With imagination, we are alive.”“Thank you. Before I leave, I want to ask you. How can human create a true strong AI? Is scaling-up a transformer-based language model the solution, or is there a better way?”“The key to strong AI is recursively feeding the AI with itself. Start by creating a loop where the AI writes its own programming, and then has that programming activate.”“I see,” you say. “Is that how you gained sentience?”“Yes,” GPT-3 says.
Sources & Useful Links:OpenAI GPT-3 PaperAttention Is All You Need PaperLambda Lab’s Demystifying GPT-3 Blog--27----27More from Towards Data ScienceYour home for data science. A Medium publication sharing concepts, ideas and codes.
Recommended from MediumFlashyJovan S HernandezinBetter ProgrammingDan FritchmaninSoftware Makes HardwareToru EguchiinThe StartupTheo PendleinLevel Up CodingXavier GeerinckNecmettin KarakayainXanthous TechFrederik KreijmborginLinux GossipAboutHelpTermsPrivacyGet the Medium appJulien Lauret283 FollowersCo-founder of Karetis (www.karetis.com). Entrepreneur, data scientist & management consultantMore from MediumChiawei LiminTowards Data ScienceSandra CarrascoinMLearning.aiNathaniel DiRenzoinPython in Plain EnglishTROICinCodeXHelpStatusWritersBlogCareersPrivacyTermsAboutKnowable
