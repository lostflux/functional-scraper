old id = 1237
International evaluation of an AI system for breast cancer screening | Nature
2020
https://www.nature.com/articles/s41586-019-1799-6

Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript.
AdvertisementInternational evaluation of an AI system for breast cancer screeningNaturevolume577,pages89–94 (2020)Cite this article76kAccesses684Citations3602AltmetricMetricsdetailsSubjectsMatters Arisingto this article was published on 14 October 2020AnAddendumto this article was published on 14 October 2020AbstractScreening mammography aims to identify breast cancer at earlier stages of the disease, when treatment can be more successful1. Despite the existence of screening programmes worldwide, the interpretation of mammograms is affected by high rates of false positives and false negatives2. Here we present an artificial intelligence (AI) system that is capable of surpassing human experts in breast cancer prediction. To assess its performance in the clinical setting, we curated a large representative dataset from the UK and a large enriched dataset from the USA. We show an absolute reduction of 5.7% and 1.2% (USA and UK) in false positives and 9.4% and 2.7% in false negatives. We provide evidence of the ability of the system to generalize from the UK to the USA. In an independent study of six radiologists, the AI system outperformed all of the human readers: the area under the receiver operating characteristic curve (AUC-ROC) for the AI system was greater than the AUC-ROC for the average radiologist by an absolute margin of 11.5%. We ran a simulation in which the AI system participated in the double-reading process that is used in the UK, and found that the AI system maintained non-inferior performance and reduced the workload of the second reader by 88%. This robust assessment of the AI system paves the way for clinical trials to improve the accuracy and efficiency of breast cancer screening.
You have full access to this article via your institution.
MainBreast cancer is the second leading cause of death from cancer in women3, but early detection and treatment can considerably improve outcomes1,4,5. As a consequence, many developed nations have implemented large-scale mammography screening programmes. Major medical and governmental organizations recommend screening for all women starting between the ages of 40 and 506,7,8. In the USA and UK combined, over 42 million exams are performed each year9,10.
Despite the widespread adoption of mammography, interpretation of these images remains challenging. The accuracy achieved by experts in cancer detection varies widely, and the performance of even the best clinicians leaves room for improvement11,12. False positives can lead to patient anxiety13, unnecessary follow-up and invasive diagnostic procedures. Cancers that are missed at screening may not be identified until they are more advanced and less amenable to treatment14.
AI may be uniquely poised to help with this challenge. Studies have demonstrated the ability of AI to meet or exceed the performance of human experts on several tasks of medical-image analysis15,16,17,18,19. As a shortage of mammography professionals threatens the availability and adequacy of breast-screening services around the world20,21,22,23, the scalability of AI could improve access to high-quality care for all.
Computer-aided detection (CAD) software for mammography was introduced in the 1990s, and several assistive tools have been approved for medical use24. Despite early promise25,26, this generation of software failed to improve the performance of readers in real-world settings12,27,28. More recently, the field has seen a renaissance owing to the success of deep learning. A few studies have characterized systems for breast cancer prediction with stand-alone performance that approaches that of human experts29,30. However, the existing work has several limitations. Most studies are based on small, enriched datasets with limited follow-up, and few have compared performance to readers in actual clinical practice—instead relying on laboratory-based simulations of the reading environment. So far there has been little evidence of the ability of AI systems to translate between different screening populations and settings without additional training data31. Critically, the pervasive use of follow-up intervals that are no longer than 12 months29,30,32,33means that more subtle cancers that are not identified until the next screen may be ignored.
In this study, we evaluate the performance of a new AI system for breast cancer prediction using two large, clinically representative datasets from the UK and the USA. We compare the predictions of the system to those made by readers in routine clinical practice and show that performance exceeds that of individual radiologists. These observations are confirmed with an independently conducted reader study. Furthermore, we show how this system might be integrated into screening workflows, and provide evidence that the system can generalize across continents. Figure1shows an overview of the project.
Datasets representative of the UK and US breast cancer screening populations were curated from three screening centres in the UK and one centre in the USA. Outcomes were derived from the biopsy record and longitudinal follow-up. An AI system was trained to identify the presence of breast cancer from a set of screening mammograms, and was evaluated in three primary ways: first, AI predictions were compared with the historical decisions made in clinical practice; second, to evaluate the generalizability across populations, a version of the AI system was developed using only the UK data and retested on the US data; and finally, the performance of the AI system was compared to that of six independent radiologists using a subset of the US test set.
Datasets from cancer screening programmesA deep learning model for identifying breast cancer in screening mammograms was developed and evaluated using two large datasets from the UK and the USA. We report results on test sets that were not used to train or tune the AI system.
The UK test set consisted of screening mammograms that were collected between 2012 and 2015 from 25,856 women at two screening centres in England, where women are screened every three years. It included 785 women who had a biopsy, and 414 women with cancer that was diagnosed within 39 months of imaging. This was a random sample of 10% of all women with screening mammograms at these sites during this time period. The UK cohort resembled the broader screening population in age and disease characteristics (Extended Data Table1a).
The test set from the USA, where women are screened every one to two years, consisted of screening mammograms that were collected between 2001 and 2018 from 3,097 women at one academic medical centre. We included images from all 1,511 women who were biopsied during this time period and a random subset of women who never underwent biopsy (Methods). Among the women who received a biopsy, 686 were diagnosed with cancer within 27 months of imaging.
Breast cancer outcome was determined on the basis of multiple years of follow-up (Fig.
1). We chose the follow-up duration on the basis of the screening interval in the country of origin for each dataset. In a similar manner to previous work34, we augmented each interval with a three-month buffer to account for variability in scheduling and latency of follow-up. Cases that were designated as cancer-positive were accompanied by a biopsy-confirmed diagnosis within the follow-up period. Cases labelled as cancer-negative had at least one follow-up non-cancer screen; cases without this follow-up were excluded from the test set.
Retrospective clinical comparisonWe used biopsy-confirmed breast cancer outcomes to evaluate the predictions of the AI system as well as the original decisions made by readers in the course of clinical practice. Human performance was computed on the basis of the clinician’s decision to recall the patient for further diagnostic investigation. The receiver operating characteristic (ROC) curve of the AI system is shown in Fig.
2.
a, The ROC curve of the AI system on the UK screening data. The AUC is 0.889 (95% CI 0.871, 0.907;n= 25,856 patients). Also shown are the sensitivity and specificity pairs for the human decisions made in clinical practice. Cases were considered positive if they received a biopsy-confirmed diagnosis of cancer within 39 months of screening. The consensus decision represents the standard of care in the UK, and will involve input from between two and three expert readers. The inset shows a magnification of the grey shaded region. AI system operating points were selected on a separate validation dataset: point i was intended to match the sensitivity and exceed the specificity of the first reader; points ii and iii were selected to attain non-inferiority for both the sensitivity and specificity of the second reader and consensus opinion, respectively.
b, The ROC curve of the AI system on the US screening data. When trained on both datasets (solid curve), the AUC is 0.8107 (95% CI 0.791, 0.831;n= 3,097 patients). When trained on only the UK dataset (dotted curve), the AUC is 0.757 (95% CI 0.732, 0.780). Also shown are the sensitivity and specificity achieved by radiologists in clinical practice using BI-RADS35. Cases were considered positive if they received a biopsy-confirmed diagnosis of cancer within 27 months of screening. AI system operating points were chosen, using a separate validation dataset, to exceed the sensitivity and specificity of the average reader. Negative cases were upweighted to account for the sampling protocol (seeMethodssection ‘Inverse probability weighting’). Extended Data Figure1shows an unweighted analysis. See Extended Data Table2afor statistical comparisons of sensitivity and specificity.
In the UK, each mammogram is interpreted by two readers, and in cases of disagreement, an arbitration process may invoke a third opinion. These interpretations occur serially, such that each reader has access to the opinions of previous readers. The records of these decisions yield three benchmarks of human performance for cancer prediction.
Compared to the first reader, the AI system demonstrated a statistically significant improvement in absolute specificity of 1.2% (95% confidence interval (CI) 0.29%, 2.1%;P= 0.0096 for superiority) and an improvement in absolute sensitivity of 2.7% (95% CI −3%, 8.5%;P= 0.004 for non-inferiority at a pre-specified 5% margin; Extended Data Table2a).
Compared to the second reader, the AI system showed non-inferiority (at a 5% margin) for both specificity (P< 0.001) and sensitivity (P= 0.02). Likewise, the AI system showed non-inferiority (at a 5% margin) to the consensus judgment for specificity (P< 0.001) and sensitivity (P= 0.0039).
In the standard screening protocol in the USA, each mammogram is interpreted by a single radiologist. We used the BI-RADS35score that was assigned to each case in the original screening context as a proxy for human cancer prediction (seeMethodssection ‘Interpreting clinical reads’). Compared to the typical reader, the AI system demonstrated statistically significant improvements in absolute specificity of 5.7% (95% CI 2.6%, 8.6%;P< 0.001) and in absolute sensitivity of 9.4% (95% CI 4.5%, 13.9%;P< 0.001; Extended Data Table2a).
Generalization across populationsTo evaluate the ability of the AI system to generalize across populations and screening settings, we trained the same architecture using only the UK dataset and applied it to the US test set (Fig.
2b). Even without exposure to the US training data, the ROC curve of the AI system encompasses the point that indicates the average performance of US radiologists. Again, the AI system showed improved specificity (+3.5%,P= 0.0212) and sensitivity (+8.1%,P= 0.0006; Extended Data Table2b) compared with radiologists.
Comparison with a reader studyIn a reader study that was conducted by an external clinical research organization, six US-board-certified radiologists who were compliant with the requirements of the Mammography Quality Standards Act (MQSA) interpreted 500 mammograms that were randomly sampled from the US test set. Where data were available, readers were equipped with contextual information typically available in the clinical setting, including the patient’s age, breast cancer history, and previous screening mammograms.
Among the 500 cases selected for this study, 125 had biopsy-proven cancer within 27 months, 125 had a negative biopsy within 27 months and 250 were not biopsied (Extended Data Table3). These proportions were chosen to increase the difficulty of the screening task and increase statistical power. (Such enrichment is typical in observer studies36.)Readers rated each case using the forced BI-RADS35scale, and BI-RADS scores were compared to ground-truth outcomes to fit an ROC curve for each reader. The scores of the AI system were treated in the same manner (Fig.
3).
a, Six readers rated each case (n= 465) using the six-point BI-RADS scale. A fitted ROC curve for each of the readers is compared to the ROC curve of the AI system (seeMethodssection ‘Statistical analysis’). For reference, a non-parametric ROC curve is presented in tandem. Cases were considered positive (n= 113) if they received a pathology-confirmed diagnosis of cancer within 27 months of the time of screening. Note that this sample of cases was enriched for patients who received a negative biopsy result (n= 119), making this a more-challenging population for screening. The mean reader AUC was 0.625 (s.d. 0.032), whereas the AUC for the AI system was 0.740 (95% CI 0.696, 0.794). The AI system exceeded human performance by a significant margin (ΔAUC = +0.115, 95% CI 0.055, 0.175;P= 0.0002 by two-sided ORH method (seeMethodssection ‘Statistical analysis’)). For results using a 12-month interval, see Extended Data Fig.
2.
b, Pooled results from all six readers froma.
c, Pooled results (n= 408) from all 6 readers using a 12-month interval for cancer definition. Cases were considered positive (n= 56) if they received a pathology-confirmed cancer diagnosis within one year (Extended Data Table3).
The AI system exceeded the average performance of radiologists by a significant margin (change in area under curve (ΔAUC) = +0.115, 95% CI 0.055, 0.175;P= 0.0002). Similar results were observed when a follow-up period of one year was used instead of 27 months (Fig.
3c, Extended Data Fig.
2).
In addition to producing a classification decision for the entire case, the AI system was designed to highlight specific areas of suspicion for malignancy. Likewise, the readers in our study supplied rectangular region-of-interest (ROI) annotations surrounding concerning findings.
We used multi-localization receiver operating characteristic (mLROC) analysis37to compare the ability of the readers and the AI system to identify malignant lesions within each case (seeMethodssection ‘Localization analysis’).
We summarized each mLROC plot by computing the partial area under the curve (pAUC) in the false-positive fraction interval from 0 to 0.138(Extended Data Fig.
3). The AI system exceeded human performance by a significant margin (ΔpAUC = +0.0192, 95% CI 0.0086, 0.0298;P= 0.0004).
Potential clinical applicationsThe classifications made by the AI system could be used to reduce the workload involved in the double-reading process that is used in the UK, while preserving the standard of care. We simulated this scenario by omitting the second reader and any ensuing arbitration when the decision of the AI system agreed with that of the first reader. In these cases, the opinion of the first reader was treated as final. In cases of disagreement, the second and consensus opinions were invoked as usual. This combination of human and machine results in performance equivalent to that of the traditional double-reading process, but saves 88% of the effort of the second reader (Extended Data Table4a).
The AI system could also be used to provide automated, immediate feedback in the screening setting.
To identify normal cases with high confidence, we used a very-low decision threshold. For the UK data, we achieved a negative predictive value (NPV) of 99.99% while retaining a specificity of 41.15%. Similarly, for the US data, we achieved a NPV of 99.90% while retaining a specificity of 34.79%. These data suggest that it may be feasible to dismiss 35–41% of normal cases if we allow for one cancer in every 1,000–10,000 negative predictions (NPV 99.90–99.99% in USA–UK). By comparison, consensus double reading in our UK dataset included one cancer in every 182 cases that were deemed normal.
To identify cancer cases with high confidence, we used a very-high decision threshold. For the UK data, we achieved a positive predictive value (PPV) of 85.6% while retaining a sensitivity of 41.2%. Similarly, for the US data, we achieved a PPV of 82.4% while retaining a sensitivity of 29.8%. These data suggest that it may be feasible to rapidly prioritize 30–40% of cancer cases, with approximately five out of six follow-ups leading to a diagnosis of cancer. By comparison, in our study only 22.8% of UK cases that were recalled by consensus double reading and 4.9% of US cases that were recalled by single reading were ultimately diagnosed with cancer.
Performance breakdownComparing the errors of the AI system with errors from clinical reads revealed many cases in which the AI system correctly identified cancer whereas the reader did not, and vice versa (Supplementary Table1). Most of the cases in which only the AI system identified cancer were invasive (Extended Data Table5). On the other hand, cases in which only the reader identified cancer were split more evenly between in situ and invasive. Further breakdowns by invasive cancer size, grade and molecular markers show no clear biases (Supplementary Table2).
We also considered the disagreement between the AI system and the six radiologists that participated in the US reader study. Figure4ashows a sample cancer case that was missed by all six radiologists, but correctly identified by the AI system. Figure4bshows a sample cancer case that was caught by all six radiologists, but missed by the AI system. Although we were unable to determine clear patterns among these instances, the presence of such edge cases suggests potentially complementary roles for the AI system and human readers in reaching accurate conclusions.
a, A sample cancer case that was missed by all six readers in the US reader study, but correctly identified by the AI system. The malignancy, outlined in yellow, is a small, irregular mass with associated microcalcifications in the lower inner right breast.
b, A sample cancer case that was caught by all six readers in the US reader study, but missed by the AI system. The malignancy is a dense mass in the lower inner right breast. Left, mediolateral oblique view; right, craniocaudal view.
We compared the performance of the 20 individual readers best represented in the UK clinical dataset with that of the AI system (Supplementary Table3). The results of this analysis suggest that the aggregate comparison presented above is not unduly influenced by any particular readers. Breakdowns by cancer type, grade and lesion size suggest no apparent difference in the distribution of cancers detected by the AI system and human readers (Extended Data Table6a).
On the US test set, a breakdown by cancer type (Extended Data Table6b) shows that the sensitivity advantage of the AI system is concentrated on the identification of invasive cancers (for example, invasive lobular or ductal carcinoma) rather than in situ cancer (for example, ductal carcinoma in situ). A breakdown by BI-RADS35breast density category shows that performance gains apply equally across the spectrum of breast tissue types that is represented in this dataset (Extended Data Table6c).
DiscussionIn this study we present an AI system that outperforms radiologists on a clinically relevant task of breast cancer identification. These results held across two large datasets that are representative of different screening populations and practices.
In the UK, the AI system showed specificity superior to that of the first reader. Sensitivity at the same operating point was non-inferior. Consensus double reading has been shown to improve performance compared to single reading39, and represents the current standard of care in the UK and many European countries40. Our system did not outperform this benchmark, but was statistically non-inferior to the second reader and consensus opinion.
In the USA, the AI system exhibited specificity and sensitivity superior to that of radiologists practising in an academic medical centre. This trend was confirmed in an externally conducted reader study, which showed that the scores of the AI system stratified cases better than the BI-RADS ratings (the standard scale for mammography assessment in the USA) that were assigned by each of the six readers.
Notably, the human readers (both in the clinic and our reader study) had access to patient history and previous mammograms when making screening decisions. The US clinical readers may have also had access to breast tomosynthesis images. By contrast, the AI system only processed the most recent mammogram.
These comparisons are not without limitations. Although the UK dataset mirrored the nationwide screening population in age and cancer prevalence (Extended Data Table1a), the same cannot be said of the US dataset, which was drawn from a single screening centre and enriched for cancer cases.
By chance, the vast majority of images used in this study were acquired on devices made by Hologic. Future research should assess the performance of the AI system across a variety of manufacturers in a more systematic way.
In our reader study, all of the radiologists were eligible to interpret screening mammograms in the USA, but did not uniformly receive fellowship training in breast imaging. It is possible that a higher benchmark for performance could have been obtained with readers who were more specialized41.
To obtain high-quality ground-truth labels, we used extended follow-up intervals that were chosen to encompass a subsequent round of screening in each country. Although there is some precedent in clinical trials34and targeted cohort studies42, this step is not usually taken during systematic evaluation of AI systems for breast cancer detection.
In retrospective datasets with shorter follow-up intervals, outcome labels tend to be skewed in favour of readers. As they are gatekeepers for biopsy, asymptomatic cases will only receive a cancer diagnosis if a mammogram raises the suspicions of a reader. A longer follow-up interval decouples the ground-truth labels from reader opinions (Extended Data Fig.
4) and includes cancers that may have been initially missed by human eyes.
The use of an extended interval makes cancer prediction a more challenging task. Cancers that are diagnosed years later may include new growths for which there could be no mammographic evidence in the original images. Consequently, the sensitivity values presented here are lower than what has been reported for 12-month intervals2(Extended Data Fig.
5).
We present early evidence of the ability of the AI system to generalize across populations and screening protocols. We retrained the system using exclusively UK data, and then measured performance on unseen US data. In this context, the system continued to outperform radiologists, albeit by a smaller margin. This suggests that in future clinical deployments, the system might offer strong baseline performance, but could benefit from fine-tuning with local data.
The optimal use of the AI system within clinical workflows remains to be determined. The specificity advantage exhibited by the system suggests that it could help to reduce recall rates and unnecessary biopsies. The improvement in sensitivity exhibited in the US data shows that the AI system may be capable of detecting cancers earlier than the standard of care. An analysis of the localization performance of the AI system suggests it holds early promise for flagging suspicious regions for review by experts. Notably, the additional cancers identified by the AI system tended to be invasive rather than in situ disease.
Beyond improving reader performance, the technology described here may have a number of other clinical applications. Through simulation, we suggest how the system could obviate the need for double reading in 88% of UK screening cases, while maintaining a similar level of accuracy to the standard protocol. We also explore how high-confidence operating points can be used to triage high-risk cases and dismiss low-risk cases. These analyses highlight the potential of this technology to deliver screening results in a sustainable manner despite workforce shortages in countries such as the UK43. Prospective clinical studies will be required to understand the full extent to which this technology can benefit patient care.
MethodsEthical approvalUse of the UK dataset for research collaborations by both commercial and non-commercial organizations received ethical approval (REC reference14/SC/0258). The US data were fully de-identified and released only after an Institutional Review Board approval (STU00206925).
The UK datasetThe UK dataset was collected from three breast screening sites in the UK National Health Service Breast Screening Programme (NHSBSP). The NHSBSP invites women aged between 50 and 70 who are registered with a general practitioner (GP) for mammographic screening every three years. Women who are not registered with a GP, or who are older than 70, can self-refer to the screening programme. In the UK, the screening programme uses double reading: each mammogram is read by two radiologists, who are asked to decide whether to recall the woman for additional follow-up. When there is disagreement, an arbitration process takes place.
The data were initially compiled by OPTIMAM (Cancer Research UK) between 2010 and 2018, from St George’s Hospital (London), Jarvis Breast Centre (Guildford) and Addenbrooke’s Hospital (Cambridge). The collected data included screening and follow-up mammograms (comprising mediolateral oblique and craniocaudal views of the left and right breasts), all radiologist opinions (including the arbitration result, if applicable) and the metadata associated with follow-up treatment.
The mammograms and associated metadata of 137,291 women were considered for inclusion in the study. Of these, 123,964 women had screening images and uncorrupted metadata. Exams that were recalled for reasons other than radiographic evidence of malignancy, or episodes that were not part of routine screening, were excluded. In total, 121,850 women had at least one eligible exam. Women who were below the age of 47 at the time of the screen were excluded from validation and test sets, leaving 121,455 women. Finally, women for whom there was no exam with sufficient follow-up were excluded from validation and test sets. This last step resulted in the exclusion of 5,990 of 31,766 test-set cases (19%); see Supplementary Fig.
1.
The test set is a random sample of 10% of all women who were screened at two sites (St George’s Hospital and Jarvis Breast Centre) between 2012 and 2015. Insufficient data were provided to apply the sampling procedure to the third site. In assembling the test set, we randomly selected a single eligible screening mammogram from the record of each woman. For women with a positive biopsy, eligible mammograms were those conducted in the 39 months before the date of biopsy. For women who never had a positive biopsy, eligible mammograms were accompanied by a non-suspicious mammogram at least 21 months later.
The final test set consisted of 25,856 women (see Supplementary Fig.
1). When compared to the UK national breast cancer screening service, we observed a very similar distribution of cancer prevalence, age and, cancer type (see Extended Data Table1a). Digital mammograms were acquired predominantly on devices manufactured by Hologic (95%), followed by General Electric (4%) and Siemens (1%).
The US datasetThe US dataset was collected from Northwestern Memorial Hospital (Chicago) between 2001 and 2018. In the USA, each screening mammogram is typically read by a single radiologist, and screens are conducted annually or biannually. The breast radiologists at this hospital receive fellowship training and only interpret breast-imaging studies. Their experience levels ranged from 1 to 30 years. The American College of Radiology (ACR) recommends that women start routine screening at the age of 40; other organizations, including the United States Preventive Services Task Force (USPSTF), recommend that screening begins at the age of 50 for women with an average risk of breast cancer6,7,8.
The US dataset included records from all women that underwent a breast biopsy between 2001 and 2018. It also included a random sample of approximately 5% of all women who participated in screening, but were never biopsied. This heuristic was used in order to capture all cancer cases (to enhance statistical power) and to curate a rich set of benign findings on which to train and test the AI system. The data-processing steps involved in constructing the dataset are summarized in Supplementary Fig.
2.
Among women with a completed mammogram order, we collected records from all women with a pathology report that contained the term ‘breast’. Among women that lacked such a pathology report, those whose records bore an International Classification of Diseases (ICD) code indicative of breast cancer were excluded. Approximately 5% of this unbiopsied negative population was sampled. After de-identification and transfer, women were excluded if their metadata were unavailable or corrupted. The women in the dataset were split randomly among train (55%), validation (15%) and test (30%) sets. For testing, a single case was chosen for each woman, following a similar procedure as for the UK dataset. In women who underwent biopsy, we randomly chose a case from the 27 months preceding the date of biopsy. For women who did not undergo biopsy, one screening mammogram was randomly chosen from among those with a follow-up event at least 21 months later.
Cases were considered complete if they possessed the four standard screening views (mediolateral oblique and craniocaudal views of the left and right breasts), acquired for screening intent. Again, the vast majority of the studies were acquired using Hologic (including Lorad-branded) devices (99%); the other manufacturers (Siemens and General Electric) together constituted less than 1% of studies.
The radiology reports associated with cases in the test set were used to flag and exclude cases that involved breast implants or were recalled for technical reasons. To compare the AI system against the clinical reads performed at this site, we employed clinicians to manually extract BI-RADS scores from the original radiology reports. There were some cases for which the original radiology report could not be located, even if a subsequent cancer diagnosis was confirmed by biopsy. This might have happened, for example, if the screening case was imported from an outside institution. Such cases were excluded from the clinical reader comparison.
Randomization and blindingPatients were randomized into training, validation, and test sets by applying a hash function to the de-identified medical record number. Set assignment was based on the value of the resulting integer modulo 100. For the UK data, values of 0–9 were reserved for the test set. For the US data, values of 0–29 were reserved for the test set. Test set sizes were chosen to produce, in expectation, a sufficient number of positives to power statistical comparisons on the metric of sensitivity.
The US and UK test sets were held back from AI system development, which only took place on the training and validation sets. Investigators did not access test set data until models, hyperparameters, and operating point thresholds were finalized. None of the readers who interpreted the images had knowledge of any aspect of the AI system.
Inverse probability weightingThe US test set includes images from all biopsied women, but only a random subset of women who never underwent biopsy. This enrichment allowed us to accrue more positives in light of the low baseline prevalence of breast cancer, but led to underrepresentation of normal cases. We accounted for this sampling process by using inverse probability weighting to obtain unbiased estimates of human and AI system performance in the screening population44,45.
We acquired images from 7,522 of the 143,238 women who underwent mammography screening but had no cancer diagnosis or biopsy record. Accordingly, we upweighted cases from women who never underwent biopsy by a factor of 19.04. Further sampling occurred when selecting one case per patient: to enrich for difficult cases, we preferentially chose cases from the timeframe preceding a biopsy (if one occurred). Although this sampling increases the diversity of benign findings, it again shifts the distribution from what would be observed in a typical screening interval. To better reflect the prevalence that results when negative cases are randomly selected, we estimated additional factors by Monte Carlo simulation. Choosing one case per patient with our preferential sampling mechanism yielded 872 cases that were biopsied within 27 months, and 1,662 cases that were not (Supplementary Fig.
2). However, 100 trials of pure random sampling yielded on average 557.54 and 2,056.46 cases, respectively. Accordingly, cases associated with negative biopsies were downweighted by 557.54/872 = 0.64. Cases that were not biopsied were upweighted by another 2,056.46/1,662 = 1.24, leading to a final weight of 19.04 × 1.24 = 23.61.Cancer-positive cases carried a weight of 1.0. The final sample weights were used in sensitivity, specificity and ROC calculations.
Histopathological outcomesIn the UK dataset, benign and malignant classifications (given directly in the metadata) followed NHSBSP definitions46. To derive the outcome labels for the US dataset, pathology reports were reviewed by US-board-certified pathologists and categorized according to the findings they contained. An effort was made to harmonize this categorization with UK definitions. Malignant pathologies included ductal carcinoma in situ, microinvasive carcinoma, invasive ductal carcinoma, invasive lobular carcinoma, special-type invasive carcinoma (including tubular, mucinous and cribriform carcinomas), intraductal papillary carcinoma, non-primary breast cancers (including lymphoma and phyllodes) and inflammatory carcinoma. Women who received a biopsy that found any of these malignant pathologies were considered to have a diagnosis of cancer.
Benign pathologies included lobular carcinoma in situ, radial scar, columnar cell changes, atypical lobular hyperplasia, atypical ductal hyperplasia, cyst, sclerosing adenosis, fibroadenoma, papilloma, periductal mastitis and usual ductal hyperplasia. None of these findings were considered to be cancerous.
Interpreting clinical readsIn the UK screening setting, readers categorize mammograms from asymptomatic women as normal or abnormal, with a third option for technical recall owing to inadequate image quality. An abnormal result at the conclusion of the double-reading process results in further diagnostic assessment. We treat mammograms deemed abnormal as a prediction of malignancy. Cases in which the consensus judgment recalled the patient for technical reasons were excluded from analysis, as the images were presumed to be incomplete or unreliable. Cases in which any single reader recommended technical recall were excluded from the corresponding reader comparison.
In the US screening setting, radiologists attach a BI-RADS35score to each mammogram. A score of 0 is deemed ‘incomplete’, and will later be refined on the basis of follow-up imaging or repeat mammography to address technical issues. For computation of sensitivity and specificity, we dichotomized the BI-RADS assessments in line with previous work34. Scores of 0, 4 and 5 were treated as positive predictions if the recommendation was based on mammographic findings, not on technical grounds or patient symptoms alone. Cases of technical recall were excluded from analysis, as the images were presumed to be incomplete or unreliable. BI-RADS scores were manually extracted from the free-text radiology reports. Cases for which the BI-RADS score was unavailable were excluded from the reader comparison.
In both datasets, the original readers had access to contextual information that is normally available in clinical practice. This includes the patient’s family history of cancer, prior screening and diagnostic imaging, and radiology or pathology notes from past examinations. By contrast, only the age of the patient was made available to the AI system.
Overview of the AI systemThe AI system consisted of an ensemble of three deep learning models, each operating on a different level of analysis (individual lesions, individual breasts and the full case). Each model produces a cancer risk score between 0 and 1 for the entire mammography case. The final prediction of the system was the mean of the predictions from the three independent models. A detailed description of the AI system is available in theSupplementary Methodsand Supplementary Fig.
3.
Selection of operating pointsThe AI system natively produces a continuous score that represents the likelihood of cancer being present. To support comparisons with the predictions of human readers, we thresholded this score to produce analogous binary screening decisions. For each clinical benchmark, we used the validation set to choose a distinct operating point; this amounts to a score threshold that separates positive and negative decisions. To better simulate prospective deployment, the test sets were never used in selecting operating points.
The UK dataset contains three clinical benchmarks—the first reader, second reader and consensus. This last decision is the outcome of the double-reading process and represents the standard of care in the UK. For the first reader, we chose an operating point aimed at demonstrating statistical superiority in specificity and non-inferiority for sensitivity. For the second reader and consensus reader, we chose an operating point aimed at demonstrating statistical non-inferiority for both sensitivity and specificity.
The US dataset contains a single operating point for comparison, which corresponds to the radiologist using the BI-RADS rubric for evaluation. In this case, we used the validation set to choose an operating point aimed at achieving superiority for both sensitivity and specificity.
Reader studyFor the reader study, six US-board-certified radiologists interpreted a sample of 500 cases from 500 women in the test set. All radiologists were compliant with MQSA requirements for interpreting mammography and had an average of 10 years of clinical experience (Extended Data Table7b). Two of them were fellowship-trained in breast imaging. The sample of cases was stratified to contain 50% normal cases, 25% biopsy-confirmed negative cases and 25% biopsy-confirmed positive cases. A detailed description of the case composition of the reader study can be found in Extended Data Table3. Readers were not informed of the enrichment levels in the dataset.
Readers recorded their assessments on a 21CFR11-compliant electronic case report form within the Ambra Health (New York, NY) viewer v3.18.7.0R. They interpreted the images using 5MP MSQA-compliant displays. Each reader interpreted the cases in a unique randomized order.
For each study, readers were asked to first report a BI-RADS355th edition score using the values 0, 1 and 2, as if they were interpreting the screening mammogram in routine practice. They were then asked to render a forced diagnostic BI-RADS score using the values 1, 2, 3, 4A, 4B, 4C or 5. Readers also gave a finer-grained score between 0 and 100 that was indicative of their suspicion that the case contains a malignancy.
In addition to the four standard mammographic screening images, clinical context was provided to better simulate the screening setting. Readers were presented with the preamble of the de-identified radiology report that was produced by the radiologist who originally interpreted the study. This contained information such as the age of the patient and their family history of cancer. The information was manually reviewed to ensure that no impression or findings were included.
Where possible (in 43% of cases), previous imaging was made available to the readers. Readers could review up to four sets of previous screening exams that were acquired between 1 and 4 years earlier, accompanied by de-identified radiologist reports. If prior imaging was available, the study was read twice by each reader—first without the prior information, and then immediately after, with the prior information present. The system ensured that readers could not update their initial assessment after the prior information was presented. For cases for which previous exams were available, the final reader assessment (given after having reviewed the prior exams) was used for the analysis.
Cases in which at least half of the readers indicated concerns with image quality were excluded from the analysis. Cases in which breast implants were noted were also excluded. The final analysis was performed on the remaining 465 cases.
Localization analysisFor this purpose, we considered all screening exams from the reader study for which cancer developed within 12 months. See Extended Data Table3for a detailed description of how the dataset was constructed. To collect ground-truth localizations, two board-certified radiologists inspected each case, using follow-up data to identify the location of malignant lesions. Instances of disagreement were resolved by one radiologist with fellowship training in breast imaging. To identify the precise location of the cancerous tissue, radiologists consulted subsequent diagnostic mammograms, radiology reports, biopsy notes, pathology reports and post-biopsy mammograms. Rectangular bounding boxes were drawn around the locations of subsequent positive biopsies in all views in which the finding was visible. In cases in which no mammographic finding was visible, the location where the lesion later appeared was highlighted. Of the 56 cancers considered for analysis, location information could be obtained with confidence in 53 cases; three cases were excluded owing to ambiguity in the index examination and the absence of follow-up images. On average, there were 2.018 ground-truth regions per cancer-positive case.
In the reader study, readers supplied rectangular ROI annotations surrounding suspicious findings in all cases to which they assigned a BI-RADS score of 3 or higher. A limit of six ROIs per case was enforced. On average, the readers supplied 2.04 annotations per suspicious case. In addition to an overall cancer likelihood score, the AI system produces a ranked list of rectangular bounding boxes for each case. To conduct a fair comparison, we allowed only the top two bounding boxes from the AI system to match the number of ROIs produced by the readers.
To compare the localization performance of the AI system with that of the readers, we used a method inspired by location receiver operating characteristic (LROC) analysis37. LROC analysis differs from traditional ROC analysis in that the ordinate is a sensitivity measure that factors in localization accuracy. Although LROC analysis traditionally involves a single finding per case37,47, we permitted multiple unranked findings to match the format of our data. We use the term multi-localization ROC analysis (mLROC) to describe our approach. For each threshold, a cancer case was considered a true positive if its case-wide score exceeded this threshold and at least one culprit area was correctly localized in any of the four mammogram views. Correct localization required an intersection-over-union (IoU) of 0.1 with the ground-truth ROI. False positives were defined as usual.
CAD systems are often evaluated on the basis of whether the centre of their marking falls within the boundary of a ground-truth annotation48. This is potentially problematic as it does not properly penalize predicted bounding boxes that are so large as to be non-specific, but whose centre nevertheless happens to fall within the target region. Similarly, large ground-truth annotations associated with diffuse findings might be overly generous to the CAD system. We prefer the IoU metric because it balances these considerations. We chose a threshold of 0.1 to account for the fact that indistinct margins on mammography findings lead to ROI annotations of vastly different sizes depending on subjective factors of the annotator (see Supplementary Fig.
4). Similar work in three-dimensional chest computed tomography18used any pixel overlap to qualify for correct localization. Likewise, an FDA-approved software device for the detection of wrist fractures reports statistics in which true positives require at least one pixel of overlap49. An IoU value of 0.1 is strict by these standards.
Statistical analysisTo evaluate the stand-alone performance of the AI system, the AUC-ROC was estimated using the normalized Wilcoxon (Mann–Whitney)Ustatistic50. This is the standard non-parametric method used by most modern software libraries. For the UK dataset, non-parametric confidence intervals on the AUC were computed with DeLong’s method51,52. For the US dataset, in which each sample carried a scalar weight, the bootstrap was used with 1,000 replications.
For both datasets, we compared the sensitivity and specificity of the readers with that of a thresholded score from the AI system. For the UK dataset, we knew the pseudo-identity of each reader, so statistics were adjusted for the clustered nature of the data using Obuchowski’s method for paired binomial proportions53,54. Confidence intervals on the difference are Wald intervals55and a Wald test was used for non-inferiority56. Both used the Obuchowski variance estimate.
For the US dataset, in which each sample carried a scalar inverse probability weight45, we used resampling methods57to compare the sensitivity and specificity of the AI system with those of the pool of radiologists. Confidence intervals on the difference were generated with the bootstrap method with 1,000 replications. APvalue on the difference was generated through the use of a permutation test58. In each of 10,000 trials, the reader and AI system scores were randomly interchanged for each case, yielding a reader–AI system difference sampled from the null distribution. A two-sidedPvalue was computed by comparing the observed statistic to the empirical quantiles of the randomization distribution.
In the reader study, each reader graded each case using a forced BI-RADS protocol (a score of 0 was not permitted), and the resulting values were treated as a 6-point index of suspicion for malignancy. Scores of 1 and 2 were collapsed into the lowest category of suspicion; scores 3, 4a, 4b, 4c and 5 were treated independently as increasing levels of suspicion. Because none of the BI-RADS operating points reached the high-sensitivity regime (see Fig.
3), to avoid bias from non-parametric analysis59we fitted parametric ROC curves to the data using the proper binormal model60. This issue was not alleviated by using the readers’ ratings for their suspicion of malignancy, which showed very strong correspondence with the BI-RADS scores (Supplementary Fig.
5). As BI-RADS is used in actual screening practice, we chose to focus on these scores for their superior clinical relevance. In a similar fashion, we fitted a parametric ROC curve to discretized AI system scores on the same data.
The performance of the AI system was compared to that of the panel of radiologists using methods for the analysis of multi-reader multi-case (MRMC) studies that are standard in the radiology community61. More specifically, we compared the AUC-ROC and pAUC-mLROC for the AI system to those of the average radiologist using the ORH procedure62,63. Originally formulated for the comparison of multiple imaging modalities, this analysis has been adapted to the setting in which the population of radiologists operate on a single modality and interest lies in comparing their performance to that of a stand-alone algorithm61. The jackknife method was used to estimate the covariance terms in the model. Computation ofPvalues and confidence intervals was conducted in Python using the numpy and scipy packages, and benchmarked against a reference implementation in the RJafroc library for the R computing language (https://cran.r-project.org/web/packages/RJafroc/index.html).
Our primary comparisons numbered seven in total: sensitivity and specificity for the UK first reader; sensitivity and specificity for the US clinical radiologist; sensitivity and specificity for the US clinical radiologist against a model trained using only UK data; and the AUC-ROC in the reader study. For comparisons with the clinical reads, the choice of superiority or non-inferiority was based on what seemed attainable from simulations conducted on the validation set. For non-inferiority comparisons, a 5% absolute margin was pre-specified before the test set was inspected. We used a statistical significance threshold of 0.05. All sevenPvalues survived correction for multiple comparisons using the Holm–Bonferroni method64.
Reporting summaryFurther information on research design is available in theNature Research Reporting Summarylinked to this paper.
Data availabilityThe dataset from Northwestern Medicine was used under license for the current study, and is not publicly available. Applications for access to the OPTIMAM database can be made athttps://medphys.royalsurrey.nhs.uk/omidb/getting-access/.
Code availabilityThe code used for training the models has a large number of dependencies on internal tooling, infrastructure and hardware, and its release is therefore not feasible. However, all experiments and implementation details are described in sufficient detail in theSupplementary Methodssection to support replication with non-proprietary libraries. Several major components of our work are available in open source repositories: Tensorflow (https://www.tensorflow.org); Tensorflow Object Detection API (https://github.com/tensorflow/models/tree/master/research/object_detection).
ReferencesTabár, L. et al. Swedish two-county trial: impact of mammographic screening on breast cancer mortality during 3 decades.
Radiology260, 658–663 (2011).
PubMedArticleGoogle ScholarLehman, C. D. et al. National performance benchmarks for modern screening digital mammography: update from the Breast Cancer Surveillance Consortium.
Radiology283, 49–58 (2017).
ADSPubMedArticleGoogle ScholarBray, F. et al. Global cancer statistics 2018: GLOBOCAN estimates of incidence and mortality worldwide for 36 cancers in 185 countries.
CA Cancer J. Clin.
68, 394–424 (2018).
ArticlePubMedGoogle ScholarThe Canadian Task Force on Preventive Health Care. Recommendations on screening for breast cancer in average-risk women aged 40–74 years.
CMAJ183, 1991–2001 (2011).
PubMed CentralArticleGoogle ScholarMarmot, M. G. et al. The benefits and harms of breast cancer screening: an independent review.
Br. J. Cancer108, 2205–2240 (2013).
CASPubMedPubMed CentralArticleGoogle ScholarLee, C. H. et al. Breast cancer screening with imaging: recommendations from the Society of Breast Imaging and the ACR on the use of mammography, breast MRI, breast ultrasound, and other technologies for the detection of clinically occult breast cancer.
J. Am. Coll. Radiol.
7, 18–27 (2010).
PubMedArticleGoogle ScholarOeffinger, K. C. et al. Breast cancer screening for women at average risk: 2015 guideline update from the American CancerSociety. J. Am. Med. Assoc.
314, 1599–1614 (2015).
CASArticleGoogle ScholarSiu, A. L. Screening for breast cancer: U.S. Preventive Services Task Force recommendation statement.
Ann. Intern. Med.
164, 279–296 (2016).
PubMedArticleGoogle ScholarCenter for Devices & Radiological Health.
MQSA National Statistics(US Food and Drug Administration, 2019; accessed 16 July 2019);http://www.fda.gov/radiation-emitting-products/mqsa-insights/mqsa-national-statisticsCancer Research UK.
Breast Screening(CRUK, 2017; accessed 26 July 2019);https://www.cancerresearchuk.org/about-cancer/breast-cancer/screening/breast-screeningElmore, J. G. et al. Variability in interpretive performance at screening mammography and radiologists’ characteristics associated with accuracy.
Radiology253, 641–651 (2009).
PubMedPubMed CentralArticleGoogle ScholarLehman, C. D. et al. Diagnostic accuracy of digital screening mammography with and without computer-aided detection.
JAMA Intern. Med.
175, 1828–1837 (2015).
PubMedPubMed CentralArticleGoogle ScholarTosteson, A. N. A. et al. Consequences of false-positive screening mammograms.
JAMA Intern. Med.
174, 954–961 (2014).
PubMedPubMed CentralArticleGoogle ScholarHoussami, N. & Hunter, K. The epidemiology, radiology and biological characteristics of interval breast cancers in population mammography screening.
NPJ Breast Cancer3, 12 (2017).
PubMedPubMed CentralArticleGoogle ScholarGulshan, V. et al. Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs.
J. Am. Med. Assoc.
316, 2402–2410 (2016).
ArticleGoogle ScholarEsteva, A. et al. Dermatologist-level classification of skin cancer with deep neural networks.
Nature542, 115–118 (2017).
ADSCASPubMedPubMed CentralArticleGoogle ScholarDe Fauw, J. et al. Clinically applicable deep learning for diagnosis and referral in retinal disease.
Nat. Med.
24, 1342–1350 (2018).
PubMedArticleCASGoogle ScholarArdila, D. et al. End-to-end lung cancer screening with three-dimensional deep learning on low-dose chest computed tomography.
Nat. Med.
25, 954–961 (2019).
CASPubMedArticleGoogle ScholarTopol, E. J. High-performance medicine: the convergence of human and artificial intelligence.
Nat. Med.
25, 44–56 (2019).
CASPubMedArticleGoogle ScholarMoran, S. & Warren-Forward, H. The Australian BreastScreen workforce: a snapshot.
Radiographer59, 26–30 (2012).
ArticleGoogle ScholarWing, P. & Langelier, M. H. Workforce shortages in breast imaging: impact on mammography utilization.
AJR Am. J. Roentgenol.
192, 370–378 (2009).
PubMedArticleGoogle ScholarRimmer, A. Radiologist shortage leaves patient care at risk, warns royal college.
BMJ359, j4683 (2017).
PubMedArticleGoogle ScholarNakajima, Y., Yamada, K., Imamura, K. & Kobayashi, K. Radiologist supply and workload: international comparison.
Radiat. Med.
26, 455–465 (2008).
PubMedArticleGoogle ScholarRao, V. M. et al. How widely is computer-aided detection used in screening and diagnostic mammography?J. Am. Coll. Radiol.
7, 802–805 (2010).
PubMedArticleGoogle ScholarGilbert, F. J. et al. Single reading with computer-aided detection for screening mammography.
N. Engl. J. Med.
359, 1675–1684 (2008).
CASPubMedArticleGoogle ScholarGiger, M. L., Chan, H.-P. & Boone, J. Anniversary paper: history and status of CAD and quantitative image analysis: the role ofMedical Physicsand AAPM.
Med. Phys.
35, 5799–5820 (2008).
PubMedPubMed CentralArticleGoogle ScholarFenton, J. J. et al. Influence of computer-aided detection on performance of screening mammography.
N. Engl. J. Med.
356, 1399–1409 (2007).
CASPubMedPubMed CentralArticleGoogle ScholarKohli, A. & Jha, S. Why CAD failed in mammography.
J. Am. Coll. Radiol.
15, 535–537 (2018).
PubMedArticleGoogle ScholarRodriguez-Ruiz, A. et al. Stand-alone artificial intelligence for breast cancer detection in mammography: comparison with 101 radiologists.
J. Natl. Cancer Inst.
111, 916–922 (2019).
PubMedPubMed CentralArticleGoogle ScholarWu, N. et al. Deep neural networks improve radiologists’ performance in breast cancer screening.
IEEE Trans. Med. Imaginghttps://doi.org/10.1109/TMI.2019.2945514(2019).
Zech, J. R. et al. Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: a cross-sectional study.
PLoS Med.
15, e1002683 (2018).
PubMedPubMed CentralArticleGoogle ScholarBecker, A. S. et al. Deep learning in mammography: diagnostic accuracy of a multipurpose image analysis software in the detection of breast cancer.
Invest. Radiol.
52, 434–440 (2017).
PubMedArticleGoogle ScholarRibli, D., Horváth, A., Unger, Z., Pollner, P. & Csabai, I. Detecting and classifying lesions in mammograms with deep learning.
Sci. Rep.
8, 4165 (2018).
ADSPubMedPubMed CentralArticleCASGoogle ScholarPisano, E. D. et al. Diagnostic performance of digital versus film mammography for breast-cancer screening.
N. Engl. J. Med.
353, 1773–1783 (2005).
CASPubMedArticleGoogle ScholarD’Orsi, C. J. et al.
ACR BI-RADS Atlas: Breast Imaging Reporting and Data System(American College of Radiology, 2013).
Gallas, B. D. et al. Evaluating imaging and computer-aided detection and diagnosis devices at the FDA.
Acad. Radiol.
19, 463–477 (2012).
PubMedPubMed CentralArticleGoogle ScholarSwensson, R. G. Unified measurement of observer performance in detecting and localizing target objects on images.
Med. Phys.
23, 1709–1725 (1996).
CASPubMedArticleGoogle ScholarSamulski, M. et al. Using computer-aided detection in mammography as a decision support.
Eur. Radiol.
20, 2323–2330 (2010).
PubMedPubMed CentralArticleGoogle ScholarBrown, J., Bryan, S. & Warren, R. Mammography screening: an incremental cost effectiveness analysis of double versus single reading of mammograms.
BMJ312, 809–812 (1996).
CASPubMedPubMed CentralArticleGoogle ScholarGiordano, L. et al. Mammographic screening programmes in Europe: organization, coverage and participation.
J. Med. Screen.
19, 72–82 (2012).
PubMedArticleGoogle ScholarSickles, E. A., Wolverton, D. E. & Dee, K. E. Performance parameters for screening and diagnostic mammography: specialist and general radiologists.
Radiology224, 861–869 (2002).
PubMedArticleGoogle ScholarIkeda, D. M., Birdwell, R. L., O’Shaughnessy, K. F., Sickles, E. A. & Brenner, R. J. Computer-aided detection output on 172 subtle findings on normal mammograms previously obtained in women with breast cancer detected at follow-up screening mammography.
Radiology230, 811–819 (2004).
PubMedArticleGoogle ScholarRoyal College of Radiologists.
The Breast Imaging and Diagnostic Workforce in the United Kingdom(RCR, 2016; accessed 22 July 2019);https://www.rcr.ac.uk/publication/breast-imaging-and-diagnostic-workforce-united-kingdomPinsky, P. F. & Gallas, B. Enriched designs for assessing discriminatory performance—analysis of bias and variance.
Stat. Med.
31, 501–515 (2012).
MathSciNetPubMedArticleGoogle ScholarMansournia, M. A. & Altman, D. G. Inverse probability weighting.
BMJ352, i189 (2016).
PubMedArticleGoogle ScholarEllis, I. O. et al.
Pathology Reporting of Breast Disease in Surgical Excision Specimens Incorporating the Dataset for Histological Reporting of Breast Cancer, June 2016(Royal College of Pathologists, accessed 22 July 2019);https://www.rcpath.org/resourceLibrary/g148-breastdataset-hires-jun16-pdf.htmlChakraborty, D. P. & Yoon, H.-J. Operating characteristics predicted by models for diagnostic tasks involving lesion localization.
Med. Phys.
35, 435–445 (2008).
CASPubMedArticleGoogle ScholarEllis, R. L., Meade, A. A., Mathiason, M. A., Willison, K. M. & Logan-Young, W. Evaluation of computer-aided detection systems in the detection of small invasive breast carcinoma.
Radiology245, 88–94 (2007).
PubMedArticleGoogle ScholarUS Food and Drug Administration.
Evaluation of Automatic Class III Designation for OsteoDetect(FDA, 2018; accessed 2 October 2019);https://www.accessdata.fda.gov/cdrh_docs/reviews/DEN180005.pdfHanley, J. A. & McNeil, B. J. The meaning and use of the area under a receiver operating characteristic (ROC) curve.
Radiology143, 29–36 (1982).
CASPubMedArticleGoogle ScholarDeLong, E. R., DeLong, D. M. & Clarke-Pearson, D. L. Comparing the areas under two or more correlated receiver operating characteristic curves: a nonparametric approach.
Biometrics44, 837–845 (1988).
CASMATHArticlePubMedGoogle ScholarGengsheng Qin, & Hotilovac, L. Comparison of non-parametric confidence intervals for the area under the ROC curve of a continuous-scale diagnostic test.
Stat. Methods Med. Res.
17, 207–221 (2008).
MathSciNetMATHArticleGoogle ScholarObuchowski, N. A. On the comparison of correlated proportions for clustered data.
Stat. Med.
17, 1495–1507 (1998).
CASPubMedArticleGoogle ScholarYang, Z., Sun, X. & Hardin, J. W. A note on the tests for clustered matched-pair binary data.
Biom. J.
52, 638–652 (2010).
MathSciNetPubMedMATHArticleGoogle ScholarFagerland, M. W., Lydersen, S. & Laake, P. Recommended tests and confidence intervals for paired binomial proportions.
Stat. Med.
33, 2850–2875 (2014).
MathSciNetPubMedArticleGoogle ScholarLiu, J.-P., Hsueh, H.-M., Hsieh, E. & Chen, J. J. Tests for equivalence or non-inferiority for paired binary data.
Stat. Med.
21, 231–245 (2002).
PubMedArticleGoogle ScholarEfron, B. & Tibshirani, R. J.
An Introduction to the Bootstrap(Springer, 1993).
Chihara, L. M., Hesterberg, T. C. & Dobrow, R. P.
Mathematical Statistics with Resampling and R & Probability with Applications and R Set(Wiley, 2014).
Gur, D., Bandos, A. I. & Rockette, H. E. Comparing areas under receiver operating characteristic curves: potential impact of the “last” experimentally measured operating point.
Radiology247, 12–15 (2008).
PubMedArticleGoogle ScholarMetz, C. E. & Pan, X. “Proper” binormal ROC curves: theory and maximum-likelihood estimation.
J. Math. Psychol.
43, 1–33 (1999).
MathSciNetCASPubMedMATHArticleGoogle ScholarChakraborty, D. P.
Observer Performance Methods for Diagnostic Imaging: Foundations, Modeling, and Applications with R-Based Examples(CRC, 2017).
Obuchowski, N. A. & Rockette, H. E. Hypothesis testing of diagnostic accuracy for multiple readers and multiple tests an anova approach with dependent observations.
Commun. Stat. Simul. Comput.
24, 285–308 (1995).
MATHArticleGoogle ScholarHillis, S. L. A comparison of denominator degrees of freedom methods for multiple observer ROC analysis.
Stat. Med.
26, 596–619 (2007).
MathSciNetPubMedPubMed CentralArticleGoogle ScholarAickin, M. & Gensler, H. Adjusting for multiple testing when reporting research results: the Bonferroni vs Holm methods.
Am. J. Public Health86, 726–728 (1996).
CASPubMedPubMed CentralArticleGoogle ScholarNHS Digital.
Breast Screening Programme(NHS, accessed 17 July 2019);https://digital.nhs.uk/data-and-information/publications/statistical/breast-screening-programmeDownload referencesAcknowledgementsWe would like to acknowledge multiple contributors to this international project: Cancer Research UK, the OPTIMAM project team and staff at the Royal Surrey County Hospital who developed the UK mammography imaging database; S. Tymms and S. Steer for providing patient perspectives; R. Wilson for providing a clinical perspective; all members of the Etemadi Research Group for their efforts in data aggregation and de-identification; and members of the Northwestern Medicine leadership, without whom this work would not have been possible (M. Schumacher, C. Christensen, D. King and C. Hogue). We also thank everyone at NMIT for their efforts, including M. Lombardi, D. Fridi, P. Lendman, B. Slavicek, S. Xinos, B. Milfajt and others; V. Cornelius, who provided advice on statistical planning; R. West and T. Saensuksopa for assistance with data visualization; A. Eslami and O. Ronneberger for expertise in machine learning; H. Forbes and C. Zaleski for assistance with project management; J. Wong and F. Tan for coordinating labelling resources; R. Ahmed, R. Pilgrim, A. Phalen and M. Bawn for work on partnership formation; R. Eng, V. Dhir and R. Shah for data annotation and interpretation; C. Chen for critically reading the manuscript; D. Ardila for infrastructure development; C. Hughes and D. Moitinho de Almeida for early engineering work; and J. Yoshimi, X. Ji, W. Chen, T. Daly, H. Doan, E. Lindley and Q. Duong for development of the labelling infrastructure. A.D. and F.J.G. receive funding from the National Institute for Health Research (Senior Investigator award). Infrastructure support for this research was provided by the NIHR Imperial Biomedical Research Centre (BRC). The views expressed are those of the authors and not necessarily those of the NIHR or the Department of Health and Social Care.
Author informationThese authors contributed equally: Scott Mayer McKinney, Marcin T. Sieniek, Varun Godbole, Jonathan GodwinThese authors jointly supervised this work: Jeffrey De Fauw, Shravya ShettyAffiliationsGoogle Health, Palo Alto, CA, USAScott Mayer McKinney, Marcin Sieniek, Varun Godbole, Greg S. Corrado, Hormuz Mostofi, Lily Peng, Daniel Tse & Shravya ShettyDeepMind, London, UKJonathan Godwin, Natasha Antropova, Trevor Back, Mary Chesus, Demis Hassabis, Joseph R. Ledsam, Bernardino Romera-Paredes, Mustafa Suleyman & Jeffrey De FauwDepartment of Surgery and Cancer, Imperial College London, London, UKHutan Ashrafian & Ara DarziInstitute of Global Health Innovation, Imperial College London, London, UKHutan Ashrafian & Ara DarziCancer Research UK Imperial Centre, Imperial College London, London, UKAra DarziNorthwestern Medicine, Chicago, IL, USAMozziyar Etemadi, Florencia Garcia-Vicente & David MelnickDepartment of Radiology, Cambridge Biomedical Research Centre, University of Cambridge, Cambridge, UKFiona J. GilbertRoyal Surrey County Hospital, Guildford, UKMark Halling-Brown & Kenneth C. YoungVerily Life Sciences, South San Francisco, CA, USASunny JansenGoogle Health, London, UKAlan Karthikesalingam, Christopher J. Kelly & Dominic KingStanford Health Care and Palo Alto Veterans Affairs, Palo Alto, CA, USAJoshua Jay ReicherThe Royal Marsden Hospital, London, UKRichard SidebottomThirlestaine Breast Centre, Cheltenham, UKRichard SidebottomYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarYou can also search for this author inPubMedGoogle ScholarContributionsA.K., A.D., D.H., D.K., H.M., G.C.C., J.D.F., J.R.L., K.C.Y., L.P., M.H.-B., M. Sieniek, M. Suleyman, R.S., S.M.M., S.S. and T.B. contributed to the conception of the study; A.K., B.R.-P., C.J.K., D.H., D.T., F.J.G., J.D.F., J.R.L., K.C.Y., L.P., M.H.-B., M.C., M.E., M. Sieniek, M. Suleyman, N.A., R.S., S.J., S.M.M., S.S., T.B. and V.G. contributed to study design; D.M., D.T., F.G.-V., G.C.C., H.M., J.D.F., J.G., K.C.Y., L.P., M.H.-B., M.C., M.E., M. Sieniek, S.M.M., S.S. and V.G. contributed to acquisition of the data; A.K., A.D., B.R.-P., C.J.K., F.J.G., H.A., J.D.F., J.G., J.J.R., M. Suleyman, N.A., R.S., S.J., S.M.M., S.S. and V.G. contributed to analysis and interpretation of the data; A.K., C.J.K., D.T., F.J.G., J.D.F., J.G., J.J.R., M. Sieniek, N.A., R.S., S.J., S.M.M., S.S. and V.G. contributed to drafting and revising the manuscript.
Corresponding authorsCorrespondence toScott Mayer McKinney,Daniel TseorShravya Shetty.
Ethics declarationsCompeting interestsThis study was funded by Google LLC and/or a subsidiary thereof (‘Google’). S.M.M., M. Sieniek, V.G., J.G., N.A., T.B., M.C., G.C.C., D.H., S.J., A.K., C.J.K., D.K., J.R.L., H.M., B.R.-P., L.P., M. Suleyman, D.T., J.D.F. and S.S. are employees of Google and own stock as part of the standard compensation package. J.J.R., R.S., F.J.G. and A.D. are paid consultants of Google. M.E., F.G.-V., D.M., K.C.Y. and M.H.-B received funding from Google to support the research collaboration.
Additional informationPublisher’s noteSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
Extended data figures and tablesExtended Data Fig. 1 Unweighted evaluation of breast cancer prediction on the US test set.
In contrast to in Fig.
2b, the sensitivity and specificity were computed without the use of inverse probability weights to account for the spectrum enrichment of the study population. Because hard negatives are overrepresented, the specificity of both the AI system and the human readers is reduced. The unweighted human sensitivity and specificity are 48.10% (n= 553) and 69.65% (n= 2,185), respectively.
Extended Data Fig. 2 Performance of the AI system in breast cancer prediction compared to six independent readers, with a 12-month follow-up interval for cancer-positive status.
Whereas the mean reader AUC was 0.750 (s.d. 0.049), the AI system achieved an AUC of 0.871 (95% CI 0.785, 0.919). The AI system exceeded human performance by a significant margin (ΔAUC = +0.121, 95% CI 0.070, 0.173;P= 0.0018 by two-sided ORH method). In this analysis, there were 56 positives of 408 total cases; see Extended Data Table3. Note that this sample of cases was enriched for patients who had received a negative biopsy result (n= 119), making it a more challenging population for screening. As these external readers were not gatekeepers for follow-up and eventual cancer diagnosis, there was no bias in favour of reader performance at this shorter time horizon. See Fig.
3afor a comparison with a time interval that was chosen to encompass a subsequent screening exam.
Extended Data Fig. 3 Localization (mLROC) analysis.
Similar to Extended Data Fig.
2, but true positives require localization of a malignancy in any of the four mammogram views (seeMethodssection ‘Localization analysis’). Here, the cancer interval was 12 months (n= 53 positives of 405 cases; see Extended Data Table3). The dotted line indicates a false-positive rate of 10%, which was used as the right-hand boundary for the pAUC calculation. The mean reader pAUC was 0.029 (s.d. 0.005), whereas that of the AI system was 0.048 (95% CI 0.035, 0.061). The AI system exceeded human performance by a significant margin (ΔpAUC = +0.0192, 95% CI 0.0086, 0.0298;P= 0.0004 by two-sided ORH method).
Extended Data Fig. 4 Evidence for the gatekeeper effect in retrospective datasets.
a,b, Graphs show the change in observed reader sensitivity in the UK (a) and the USA (b) as the cancer follow-up interval is extended. At short intervals, measured reader sensitivity is extremely high, owing to the fact that biopsies are only triggered based on radiological suspicion. As the time interval is extended, the task becomes more difficult and measured sensitivity declines. Part of this decline stems from the development of new cancers that were impossible to detect at the initial screening. However, steeper drops occur when the follow-up window encompasses the screening interval (36 months in the UK; 12 and 24 months in the USA). This is suggestive of what happens to reader metrics when gatekeeper bias is mitigated by another screening examination. In both graphs, the number of positives grows as the follow-up interval is extended. In the UK dataset (a), it increases fromn= 259 within 3 months ton= 402 within 39 months. In the US dataset (b), it increases fromn= 221 withinn =3 months to 553 within 39 months.
Extended Data Fig. 5 Quantitative evaluation of reader and AI system performance with a 12-month follow-up interval for ground-truth cancer-positive status.
Because a 12-month follow-up interval is unlikely to encompass a subsequent screening exam in either country, reader–model comparisons on retrospective clinical data may be skewed by the gatekeeper effect (Extended Data Fig.
4). See Fig.
2for comparison with longer time intervals.
a, Performance of the AI system on UK data. This plot was derived from a total of 25,717 eligible examples, including 274 positives. The AI system achieved an AUC of 0.966 (95% CI 0.954, 0.977).
b, Performance of the AI system on US data. This plot was derived from a total of 2,770 eligible examples, including 359 positives. The AI system achieved an AUC of 0.883 (95% CI 0.859, 0.903).
c, Reader performance. When computing reader metrics, we excluded cases for which the reader recommended repeat mammography to address technical issues. In the US data, the performance of radiologists could only be assessed on the subset of cases for which a BI-RADS grade was available.
Supplementary informationSupplementary InformationSupplementary Methods describes development and training of the AI system in greater detail. Supplementary Tables 1-3 and Supplementary Figures 1-5 contain additional results, alternative analyses, and methodological diagrams.
Reporting SummaryRights and permissionsReprints and PermissionsAbout this articleCite this articleMcKinney, S.M., Sieniek, M., Godbole, V.
et al.
International evaluation of an AI system for breast cancer screening.
Nature577,89–94 (2020). https://doi.org/10.1038/s41586-019-1799-6Download citationReceived:27 July 2019Accepted:05 November 2019Published:01 January 2020Issue Date:02 January 2020DOI:https://doi.org/10.1038/s41586-019-1799-6Share this articleAnyone you share the following link with will be able to read this content:Sorry, a shareable link is not currently available for this article.
Provided by the Springer Nature SharedIt content-sharing initiativeFurther readingPhysicians’ preferences and willingness to pay for artificial intelligence-based assistance tools: a discrete choice experiment among german radiologistsBMC Health Services Research(2022)Mammographically occult breast cancers detected with AI-based diagnosis supporting software: clinical and histopathologic characteristicsInsights into Imaging(2022)Artificial intelligence in mammographic phenotyping of breast cancer risk: a narrative reviewBreast Cancer Research(2022)Challenging presumed technological superiority when working with (artificial) colleaguesScientific Reports(2022)Incorporating uncertainty in learning to defer algorithms for safe computer-aided diagnosisScientific Reports(2022)CommentsBy submitting a comment you agree to abide by ourTermsandCommunity Guidelines. If you find something abusive or that does not comply with our terms or guidelines please flag it as inappropriate.
You have full access to this article via your institution.
Associated ContentAI shows promise for breast cancer screeningAI for breast-cancer screeningReply to: Transparency and reproducibility in artificial intelligenceAdvertisementExplore contentAbout the journalPublish with usSearchAdvanced searchQuick linksNature (Nature)ISSN1476-4687(online)ISSN0028-0836(print)nature.com sitemapDiscover contentPublishing policiesAuthor & Researcher servicesLibraries & institutionsAdvertising & partnershipsCareer developmentRegional websitesLegal & Privacy© 2022 Springer Nature LimitedSign up for theNature Briefingnewsletter — what matters in science, free to your inbox daily.
