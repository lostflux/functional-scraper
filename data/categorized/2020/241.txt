old id = 2219
A New Brain-Inspired Learning Method for AI Saves Memory and Energy
2020
https://singularityhub.com/2020/07/27/a-new-brain-inspired-learning-method-for-ai-saves-memory-and-energy

Topics AI Biotech Computing Space Energy Future Tech Robotics Science Experts Featured Experts Perspectives Books Events Videos Latest Series Interviews About Singularity About Programs Membership Experts Community Careers Subscribe Welcome! Search.
News and Insights from Singularity Group Search Subscribe to our newsletter Singularity Group Singularity Community Facebook Instagram Twitter Youtube.
st0{fill:#FFFFFF;} .st1{fill:url(#SVGID_1_);} Singularity Hub News and Insights from Singularity Group Topics Experts Events Videos Search singularity group singularity community Facebook Instagram Twitter Youtube.
st0{fill:#FFFFFF;} .st1{fill:url(#SVGID_1_);} Singularity Hub News and Insights from Singularity Group Topics Experts Events Videos Search A New Brain-Inspired Learning Method for AI Saves Memory and Energy By Despite the frequent analogies, today’s AI operates on very different principles to the human brain. Now researchers have proposed a new learning method more closely tied to biology, which they think could help us approach the brain’s unrivaled efficiency.
Modern deep learning is at the very least biologically-inspired, encoding information in the strength of connections between large networks of individual computing units known as neurons. Probably the biggest difference, though, is the way these neurons communicate with each other.
Artificial neural networks are organized into layers, with each neuron typically connected to every neuron in the next layer. Information passes between layers in a highly synchronized fashion as numbers falling in a range that determines the strength of the connection between pairs of neurons.
Biological neurons, on the other hand, communicate by firing off electrical impulses known as spikes, and each neuron does so on its own schedule. Connections are not neatly divided into layers and feature many feedback loops that means the output of a neuron often ends up impacting its input somewhere down the line.
This spike-based approach is vastly more energy efficient, which is why training the most powerful AI requires kilowatts of electricity while the brain uses just 20 watts. That’s led to growing interest in the development of artificial spiking neural networks as well as so-called neuromorphic hardware—computer chips that mimic the physical organization and principles of the brain—that could run them more efficiently.
But our understanding of these spike-based approaches is still underdeveloped, and they struggle to reach the performance of more conventional artificial neural nets. Now though, researchers from the Graz University of Technology in Austria think they may have found a way to approach the power of deep learning using a biological plausible learning approach that works with spiking neural networks.
In deep learning the network is trained by getting it to make predictions on the data and then assessing how far off it is. This error is then fed backwards through the network to guide adjustments in the strength of connections between neurons. This process is called backpropagation , and over many iterations will tune the network until it makes accurate predictions.
A similar approach can be applied to spiking neural networks, but it requires huge amounts of memory. It’s also clear that this is not how the brain solves the learning problem, because it requires error signals to be sent backwards in both time and space across the synapses between neurons, which is clearly impossible.
That prompted the researchers, who are part of the Human Brain Project, to look at two features that have become clear in experimental neuroscience data: each neuron retain s a memory of previous activity in the form of molecular markers that slowly fade with time; and the brain provides top-down learning signals using things like the neurotransmitter dopamine that modulate s the behavior of groups of neurons.
In a paper in Nature Communications , the Austrian team describe s how they create d artificial analogues of these two features to create a new learning paradigm they call e-prop. While the approach learns slower than backpropagation-based methods, it achieves comparable performance.
More importantly, it allows online learning. That means that rather than processing big batches of data at once, which requires constant transfer to and from memory that contributes significantly to machine learning’s energy bills, the approach simply learns from data as it becomes available. That dramatically cuts the amount of memory and energy it requires, which makes it far more practical to use for on-chip learning in smaller mobile devices.
The team is now working with researchers from Intel to integrate the approach with the next version of the company’s neuromorphic chip Loihi , which is optimized for spiking networks. They’re also teaming up with fellow Human Brain Project researchers at the University of Manchester to apply e-prop to the neuromorphic supercomputer SpiNNaker.
There’s still a long way to go before the technique can match the power of today’s leading AI. But if it helps us start to approach the efficiencies we see in biological brains, it might not be long before AI is everywhere.
Image Credit: Gerd Altmann from Pixabay Tags Artificial Intelligence RELATED Like Humans, This Breakthrough AI Makes Concepts Out of the Words It Learns October 31, 2023 Why Google and Bing’s Embrace of Generative AI Could Upend the SEO Industry October 29, 2023 AI in the C-Suite? Why We’ll Need New Laws to Govern AI Agents in Business October 27, 2023 latest Like Humans, This Breakthrough AI Makes Concepts Out of the Words It Learns October 31, 2023 Why Google and Bing’s Embrace of Generative AI Could Upend the SEO Industry October 29, 2023 This Week’s Awesome Tech Stories From Around the Web (Through October 28) October 28, 2023 featured Carl Sagan Detected Life on Earth 30 Years Ago—Here’s Why His Experiment Still Matters Today October 26, 2023 Singularity Hub News and Insights from Singularity Group Singularity labs A 360 Singularity Hub About Creative Commons Pitch Us Contact Us Terms of Use Privacy Policy Singularity Group About Executive Program Custom Programs Podcasts Insights Blog Stay connected Facebook Instagram RSS Twitter Youtube Get the latest news from Singularity Hub! Sign Up Copyright © Singularity Group. All rights reserved.
