old id = 834
Autonomous navigation of stratospheric balloons using reinforcement learning | Nature
2020
https://doi.org/10.1038%2Fs41586-020-2939-8

Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript.
Advertisement View all journals Search Log in Explore content About the journal Publish with us Sign up for alerts RSS feed nature articles article Download PDF Article Published: 02 December 2020 Autonomous navigation of stratospheric balloons using reinforcement learning Marc G. Bellemare ORCID: orcid.org/0000-0002-6096-0105 1 , Salvatore Candido ORCID: orcid.org/0000-0002-5847-0617 3 , Pablo Samuel Castro 1 , Jun Gong 3 , Marlos C. Machado 1 , Subhodeep Moitra 1 , Sameera S. Ponda 3 & … Ziyu Wang 2 Show authors Nature volume 588 , pages 77–82 ( 2020 ) Cite this article 13k Accesses 95 Citations 412 Altmetric Metrics details Subjects Aerospace engineering Computer science Abstract Efficiently navigating a superpressure balloon in the stratosphere 1 requires the integration of a multitude of cues, such as wind speed and solar elevation, and the process is complicated by forecast errors and sparse wind measurements. Coupled with the need to make decisions in real time, these factors rule out the use of conventional control techniques 2 , 3.
 Here we describe the use of reinforcement learning 4 , 5 to create a high-performing flight controller. Our algorithm uses data augmentation 6 , 7 and a self-correcting design to overcome the key technical challenge of reinforcement learning from imperfect data, which has proved to be a major obstacle to its application to physical systems 8.
 We deployed our controller to station Loon superpressure balloons at multiple locations across the globe, including a 39-day controlled experiment over the Pacific Ocean. Analyses show that the controller outperforms Loon’s previous algorithm and is robust to the natural diversity in stratospheric winds. These results demonstrate that reinforcement learning is an effective solution to real-world autonomous control problems in which neither conventional methods nor human intervention suffice, offering clues about what may be needed to create artificially intelligent agents that continuously interact with real, dynamic environments.
You have full access to this article via your institution.
Download PDF Download PDF Main Superpressure balloons 1 can autonomously operate in the stratosphere for months, making them a cost-effective platform for communication, Earth observation, gathering meteorological data and other applications. The altitude of a superpressure balloon is determined by its density relative to the ambient atmosphere. In a Loon superpressure balloon, vertical motion is achieved by pumping air ballast in and out of a fixed-volume envelope, and horizontal motion is dictated by the winds at the balloon’s location. To navigate, a flight controller must therefore ascend and descend to find and follow favourable wind currents (Fig.
1a ).
a , Schematic of a superpressure balloon navigating a wind field. The balloon remains close to its station by moving between winds at different altitudes. Its altitude range is indicated by the upper and lower dashed lines.
b , The balloon’s flight path, viewed from above. The station and its 50-km range are shown in light blue. Shaded arrows represent the wind field. The wind field constantly evolves, requiring the balloon to replan at regular intervals.
Full size image Despite these simple dynamics, long-term balloon control is challenging. The task of ‘station-keeping’, for example, involves maintaining the balloon above a fixed ground location. To succeed, an indirect flight path must be taken through the wind field (Fig.
1b ). Station-keeping also requires managing power over the day–night cycle, as descending uses solar energy stored in on-board batteries. Sparse wind measurements result in a phenomenon called partial observability 9 , and a good controller must weigh the costs and benefits of gathering distal observations. The nature of this partial observability alone limits the usefulness of conventional control techniques 2.
We used reinforcement learning 5 to train a flight controller from simulations. Reinforcement learning excels at producing control strategies that can handle high-dimensional, heterogeneous inputs and optimize long-term objectives—for example, resulting in superhuman game play 10 , 11 , 12.
 Reinforcement learning has proven effective in optimization problems in which a model of the process is available 13 , 14 , 15 , 16 , and has shown promising results in simulations of real applications and backtesting 17 , 18 , 19 , 20 with some commercial success in internet applications 21 , 22.
However, learning from imperfect data results in behavioural inaccuracies that compound over the planning horizon 23 , 24.
 This poses an important difficulty in scaling up to autonomous physical platforms such as ours, where acquiring flight data is slow and expensive, and where decisions have consequences more than 24 h into the future. Past results on autonomous flight, in comparison, have optimized shorter-horizon objectives such as trajectory following 25 , 26 or maximizing upwards velocity 27.
 In the extreme, a balloon may need weeks of roundabout flight to recover from control errors, without the benefit of a reset mechanism such as is common in robotics experiments 7 , 28 , 29 , 30 , 31.
 Finally, the controller must cope with ageing-related changes such as helium loss and battery fatigue.
Station-keeping We say that a balloon is successfully station-keeping when within 50 kilometres of its station, a distance at which it can comfortably communicate with a ground device. The availability of diverse opposing winds at different altitudes facilitates station-keeping. In the tropics, this diversity varies seasonally and is affected by large-scale weather phenomena 32 , 33 , 34.
 However, short-term forecasts for the region can substantially differ from actual measurements; wind-heading prediction errors greater than 90° are frequent, and are greatest at the equator 35 , 36.
 This makes exploration integral to an effective control strategy.
Loon’s previous hand-crafted algorithm, colloquially named StationSeeker, provides insight into what makes a good controller. StationSeeker tracks winds with headings that form an acute angle with the direction to the station, effectively tacking towards its destination. Once the balloon is in range, it seeks slow-moving winds. Decisions are made by maximizing a score function that incentivizes exploration (see Methods). Although heuristic, this controller has been tuned extensively and has reliably navigated Loon balloons for over 150,000 flight hours.
A better controller spends a greater fraction of time successfully station-keeping. To reserve energy for the balloon’s payload, it should also minimize power consumption. We therefore require that deployed controllers use on average no more power than StationSeeker, which is calibrated for normal operations.
We encode this objective using a reward, r , that is maximal ( r = 1) when the balloon is within ρ = 50 km of its station. This reward is associated with the balloon state s t at time t and is provided in response to an action a t (ascend, descend or stay). Given a discount factor γ < 1, an optimal controller maximizes the expected discounted sum of future rewards, or ‘return’ $${R}_{s}={\mathbb{E}}\,[\mathop{\sum }\limits_{t=0}^{\infty }{\gamma }^{t}r({s}_{t},{a}_{t})|{s}_{0}=s],$$ where \({\mathbb{E}}\) indicates the expected value.
R s characterizes the long-term value of a flight controller from an initial state s onwards.
Although setting the reward to r = 0 outside the 50 km range describes the objective, softening the transition at the boundary results in improved controllers. We reduce the reward r to a constant c cliff < 1 at 50 km and decay it by a half every τ = 100 km (see Methods). To incentivize power efficiency, r is further decreased when consuming energy. An ablation study confirms that the learning is robust to a wide range of reward parameters (Fig.
2a ).
a , Performance of reinforcement-learning controllers trained with different reward parameters. Performance is measured in terms of the fraction of time spent within 50 km of the station (TWR50), evaluated using our simulation benchmark (averaged over 6,000 simulations; see text). Each parameter is varied in isolation; the deployed controller’s parameter settings are indicated by a black outline.
b , Performance as a function of network size.
Full size image Wind measurements are typically only available at the balloon’s current position and in its wake, where they have been obtained from instruments. To extrapolate to other positions, we use a Gaussian process 37 to blend the balloon’s measurements with forecasts from the European Centre for Medium-Range Weather Forecast (ECMWF), using the wind forecast as a prior mean. The variance of the posterior distribution quantifies the uncertainty of different wind estimates. As inputs to our controller, we encode the magnitude and relative bearing of winds directly above and below the balloon, at 181 pressure levels ranging from 5 kPa to 14 kPa (15–20 km equivalent altitude). This wind column also forms the bulk of StationSeeker’s inputs. Using wind estimates outside of this column increases the computational costs without substantially affecting performance, because the wind field varies slowly with latitude and longitude and also because the Gaussian process integrates measurements from nearby balloons. We encode the uncertainty at each pressure level to provide an approximate belief state 38.
 By treating uncertainty as an input, we avoid explicitly enumerating plausible scenarios—a considerable computational advantage over search methods. Finally, we encode 16 ambient variables important to efficient flight (Extended Data Table 1 ).
The wind column encoding is centred at the balloon’s altitude, so that its frame of reference is independent of the balloon’s absolute coordinates. This provides the learning system with a useful inductive bias that takes advantage of natural symmetries and supports a simple strategy: ascend or descend when the winds above or below offer better returns, stay if they do not. Because poor decisions take the balloon to a lower-return state, this strategy corrects for many mistakes by design. We thus expect it to perform well in different wind conditions and handle discrepancies between real and simulated environments.
A large supply of realistic training data is key to successful reinforcement learning. Data from previous balloon flights are inadequate as they cannot be used to evaluate large deviations from historical behaviour. On the other hand, generating sufficiently accurate data from a physical atmospheric simulation is computationally prohibitive. Instead, we create plausible wind data based on the ECMWF’s ERA5 global reanalysis dataset 39 , which reinterprets historical weather observations using numerical models. ERA5 provides baseline winds that are modified with procedural noise 40 to generate high-resolution wind fields. By varying the random seed that drives the procedural noise, we can generate an arbitrary number of scenarios and emulate forecasting errors. The end product bears resemblance to data augmentation 6 , 7 , which is known to improve the robustness of reinforcement-learning controllers to modelling discrepancies.
We model the dynamics of a Loon superpressure balloon within these plausible wind fields to obtain a simulator for training and evaluating flight controllers. A trial consists of two simulated days of station-keeping at a fixed location, during which controllers receive inputs and emit commands at 3-min intervals. Flight controllers are thus exposed to diurnal cycles and scenarios in which the balloon must recover from difficult overnight conditions. These realistic flight paths come at the cost of relatively slow simulation—roughly 40 Hz on datacentre hardware. In comparison, the Arcade Learning Environment (ALE) benchmark 41 operates at over 8,000 Hz.
Our controller uses action values to predict the rewards resulting from its decisions. The action value R s , a estimates the expected return obtained when the action a is selected from a state s.
 During operation, the controller behaves ‘greedily’, selecting the action that has the highest estimate. Action-value estimation is particularly well suited to the problem of station-keeping, in which there are few actions to evaluate. Unlike direct policy optimization schemes 42 , 43 , it is also trivially combined with experience replay 44 to reuse simulator interactions and achieves greater data efficiency.
To obtain a state-of-the-art controller, we leveraged recent developments from the field of deep reinforcement learning, which emphasizes the use of deep neural networks in the learning process. Our controller estimates its action values using a feed-forward network with seven layers of 600 rectified linear units 45 each; the weights of the network are learned using the distributional QR-DQN algorithm 46.
 Using smaller networks or a non-distributional algorithm was found to degrade performance (Fig.
2b ).
We train the neural network in a distributed setting 47.
 Data are generated from 100 parallel simulations and stored in an array of circular replay buffers from which minibatches are sampled and provided to a learner process 11.
 Distributed training overcomes the low simulation rate and enables training within a reasonable amount of time. The training process alternates between its greedy policy and a momentum-based exploratory policy (see Methods).
Evaluation We evaluate controllers in simulation using a 6,000-flight benchmark. This benchmark serves as a reliable proxy for actual flight performance, and allows fast iterations over the controller design. By varying hyperparameters, we obtain a number of controllers that exhibit different trade-offs between station-keeping performance and energy usage. The best controller satisfying our power requirement achieves a score of 55.1% TWR50 (time within radius 50 km) on the benchmark. By comparison, StationSeeker achieves 40.5% TWR50 (Fig.
3a ). A 1% gain corresponds to 14.4 additional minutes of station-keeping in a 24-h period, and so the difference amounts to a substantial 3.5 h per day average improvement in time spent near the station.
a , Proportion of flight time spent within 50 km of the station (TWR50) by different controllers in the simulation benchmark (averaged over 6,000 simulations); a higher TWR50 reflects better performance. Also included are controllers that select actions uniformly at random and passively drift. Error bars show 99.9% confidence intervals. The shaded area indicates the estimated range for the maximum TWR50 in the benchmark. RL, reinforcement learning.
b , TWR50 and power consumption for alternative parametrizations of StationSeeker and OPD. The controllers from a , and the reinforcement-learning controller at different points in training, are indicated.
c , Per-flight performance of the reinforcement-learning controller after 24 and 41 days of training. Points indicate the two controllers’ TWR50 for specific flights from the benchmark. Both controllers achieve 55.1% TWR50, but differ substantially on a per-flight basis, illustrating the partially observable nature of station-keeping.
Full size image We estimate that the maximum achievable TWR50 on this benchmark is somewhere between 56.8% and 68.7% (see Methods). The upper limit corresponds to the performance of a tree-search controller with perfect wind information, and reflects the limitations of wind-based navigation. Even if favourable winds exist, partial observability implies that an agent that behaves optimally with respect to its knowledge must still sometimes gamble. This is reflected in the network’s distributional prediction, which we use to obtain the 56.8% lower limit. The per-flight TWR50 of equally performing controllers can vary substantially (Fig.
3c ), which suggests that the true maximum lies closer to the lower limit, implying that our controller is close to optimal in simulation.
As a strong competitor, we applied a tree-search algorithm (optimistic deterministic planning, OPD 48 ) to a coarse approximation of flight dynamics, used for computational reasons (see Methods for modelling details). At equal power consumption, the reinforcement-learning controller outperforms a controller based on OPD (51.6% TWR50; Fig.
3b ). Even a tuned-search controller that uses all available power remains worse (53.0%, Mann–Whitney U test n = 6,000, P < 10 −4 , all tests two-sided). Anecdotal evidence from prior tests suggests that the tuned-search controller performs poorly in real conditions, in part because its model does not adequately deal with partial observability.
We also tested the reinforcement-learning controller’s sensitivity to initial conditions. For 12 initial conditions, we ran 125 trials from perturbations of a start position, and measured the average distance between balloons (see Methods). The reinforcement-learning controller is much less sensitive to perturbations (56 km average pairwise distance) than StationSeeker (117 km). Supplementary Videos 1 – 2 illustrate this point in easier and more challenging station-keeping conditions.
Deployment We used the reinforcement-learning controller to station Loon balloons near an equatorial location over the Pacific Ocean where StationSeeker already performs superbly (see Methods for details on how controllers are integrated into the broader fleet management system). We cumulated a total of 2,884 flight hours from 17 December 2019 to 25 January 2020 (Extended Data Figs.
1 – 3 ). Flight data was divided into 851 three-hour periods, each of which can be viewed as a roughly independent sample. This process was repeated with data from 7,475 flight hours concurrently acquired with StationSeeker at the same location (48 segments, 2,185 periods). Real conditions differ from simulation due to modelling discrepancies in flight dynamics and power consumption, making this experiment a major challenge for the reinforcement-learning controller. Additionally, the controllers received observations from multiple other balloons operating in the vicinity of the station. As a result, many of the states encountered during deployment had never been experienced in training (Supplementary Video 3 visualizes a balloon station-keeping over the course of seven days).
Overall, our controller spends more time in range of the station (TWR50 79% versus 72%; U = 850,410.5, P < 10 −4 ), and uses less power for altitude control (average 29 W versus 33 W, U = 1,048,814, P < 10 −4 ). The reported TWR50 values are higher than the average for the simulation benchmark, which is curated to contain an artificially high proportion of difficult conditions. We find that our controller spends far fewer 3-h periods outside of the 50-km range, resulting in a lower 25th-percentile TWR50 (72% versus 39%; Fig.
4a ). On the basis of our simulation results, we expect a larger performance gap at other locations.
a , Distribution of time spent within 50 km (TWR50) across 3-h periods. The panel shows the cumulative fraction of periods ( x axis) during which up to a given fraction of time was spent within 50 km of the station ( y axis). A left-shifted curve indicates higher performance and the reinforcement-learning controller has a lower 25th-percentile TWR50.
b , Histogram of distances to the station across 3-h periods (bin width: 1 km).
c , Histogram of duration of excursions (bin width: 1 h). An excursion begins when the balloon leaves the 50-km range and ends when it returns within range. The distribution of StationSeeker’s excursion lengths shows a heavier tail. Inset shows cumulative frequency to 12 h (89.7% and 92.5% of data for the reinforcement-learning controller and StationSeeker, respectively).
d , Histogram of power usage across 3-h periods (bin width: 10 W) within the inner and outer rings (0–25 km and 25–50 km) and outside the range (50–100 km). Insets show cumulative frequencies, illustrating how the controllers use energy for different purposes.
e , Average altitude over the day (1-h increments). The shaded area indicates the period during which solar energy is available. Local time is −8 utc.
Full size image Within range, StationSeeker attempts to simultaneously slow down and navigate towards the station, a behaviour that often takes it through the centre of the range. By contrast, the reinforcement-learning controller uses different strategies depending on wind conditions. For example, it takes advantage of lateral winds to station-keep without closing in, benefiting from a larger area and thus remaining passive more often. This results in more time spent within the 25–50 km outer ring (Fig.
4b ), where it uses significantly less energy (27 W versus 43 W, U = 287,887, n 1 = 812, n 2 = 565, P < 10 −4 ; Fig.
4d ), Power consumption is also reduced in the inner ring (0–25 km; 15 W versus 18 W, U = 54,176, n 1 = 778, n 2 = 116, P = 0.0005).
Outside the range (50–100 km), the reinforcement-learning controller actively works to return within the target region, resulting in shorter excursion lengths (average 3.3 h versus 5.0 h, U = 21,738, n 1 = 295, n 2 = 123, P = 0.001; Fig.
4c ). This results in increased power consumption beyond 50 km (55 W versus 39 W, U = 14,060.5, n 1 = 367, n 2 = 95, P = 0.0036), evidencing how the reinforcement-learning controller utilizes energy savings made while within range. Finally, we find evidence that the controller uses altitude to convert solar energy in excess of battery capacity into potential energy (Fig.
4e ). This occurs because ascending does not require power; the stored potential energy is then released early in the night, when power availability is most uncertain.
Discussion We used reinforcement learning to create a flight controller that can efficiently station a stratospheric balloon for weeks at a time. We dealt with the unavailability of a tractable physical simulator or affordable data-collection mechanism by augmenting an existing meteorological dataset with procedural noise to create simulations sufficient for our purposes. We address the problem of partial observability 9 , 38 by incorporating uncertainty variables as additional inputs to the neural network, and used a relative encoding of the wind column to reduce the gap between simulation and reality 6 , 7.
Although short-term, low-level control of the balloon can be achieved using standard linearization techniques 2 , reinforcement learning is particularly well equipped to deal with the discrete factors that must be considered when optimizing complex, long-term objectives such as station-keeping. The algorithmic framework we used is by now reasonably standard, and many state features were inherited from StationSeeker. Reinforcement learning also compares favourably to runtime planning: offloading the optimization process to a training phase enables a longer decision-making horizon and reduces the computational cost of decisions during deployment. Although we reported on a single, controlled experiment, our method has now replaced StationSeeker across the Loon fleet.
In designing intelligent agents that carry out tasks autonomously in the real world, we encounter the issue of domain boundaries—for instance, what a plate-grasping robot 29 does once its dishwasher is empty. Currently, in between bouts of cognition, scripted behaviours and engineers take over from the agent to reset the system to its initial configuration, but this reliance on external agency limits the agent’s autonomy. By contrast, station-keeping offers an example of a fundamentally continual and dynamic activity, one in which ongoing intelligent behaviour is a consequence of interacting with a chaotic outside world. By reacting to its environment instead of imposing a model upon it, the reinforcement-learning controller gains a flexibility that enables it to continue to perform well over time. In our pursuit of autonomous intelligence, we may do well to pay attention to emergent properties of these and other agent–environment interactions 49 , 50.
Methods Platform The Loon superpressure balloons 1 ( http://www.loon.com ) used in our experiments are around 1,800 m 3 and carry a payload of around 100 kg. They fly in the stratosphere above wildlife, weather and most planes, using lighter-than-air lift gas to provide buoyancy. They stabilize at the altitude where the force of gravity on the system is equal to the lift created by the buoyancy of the lift gas. Each balloon has two chambers: one that contains lift gas and another that can be filled with ambient air. Doing so changes the mass of the balloon system and causes it to descend. Pumping air into the chamber requires energy but releasing it does not, creating asymmetry in control dynamics.
A fleet dispatch system assigns each balloon to a particular location, and a navigation controller brings it in the vicinity of that location before switching control to the station-keeping controller. All controllers run within a safety layer to ensure important behavioural invariants are respected. As examples, a controller is not permitted to use power in a way that would leave the balloon unable to maintain safety-critical functions, or permitted to navigate to an altitude that would damage the system. The observation vector contains variables indicating whether the balloon has reached the limits of its altitude range and whether it can use energy. These safety limits are present both in training and on the real system, so the algorithm learns to work within the constraints of the safety layer. At the end of their flight, the balloons are navigated to planned locations and landed in coordination with air traffic control.
The controllers navigate by providing a periodically updated setpoint to the altitude-control system. The actions used by the reinforcement-learning controller (ascend, descend and stay) correspond to setpoints at the lowest-permitted, highest-permitted and current altitudes.
Station-keeping as a control problem Station-keeping is initiated up to 300 km from the station, and continues until the balloon is reassigned. The objective of station-keeping is to maximize the proportion of time spent within range from the moment that the process is initiated. We further require that a controller’s average power usage be no greater than StationSeeker’s.
The nature of station-keeping creates challenges for standard model-predictive control tools 3.
 There is a nontrivial and nonlinear relationship between decisions and the signal we wish to control—that is, the distance to the station. At night, for example, the controller must weigh making short-term gains against long-term losses due to low battery charge. Because wind uncertainty cannot be accurately described in closed form, there is also no simple way of modelling the consequences of exploration. More generally, the mapping from a state to an optimal decision varies nonlinearly in terms of the input variables. Search-based model-predictive solutions such as the OPD controller are a more natural fit, but are computationally impractical.
Conditions for station-keeping We say that a location has diverse winds if we can identify altitudes at which the winds blow in opposing directions. We use wind diversity to characterize the difficulty of station-keeping at a particular location, and to filter out impossible training scenarios. In aggregate, we estimate that diverse winds are available in the tropics 67% of time. We obtain this estimate using a 1°-spaced lattice over the tropics (approximately 18,000 locations). For each location, wind diversity is estimated by querying the simulator every hour from 2000 to 2019 and over five random seeds. Each query results in a ‘diverse-or-not’ outcome. The aggregate availability is the proportion of queries that are diverse. Using the same technique, we estimate at 83.3% the average January availability for the equatorial location of our experiment. Note that, as this number measures the occurrence of a specific wind pattern and ignores partial observability, it is not directly comparable to TWR50.
The diversity of winds in the tropical stratosphere is influenced by several phenomena. The Quasi-Biennial Oscillation is a series of alternating easterly and westerly winds with a period of approximately 28 months, and is driven by vertically propagating gravity waves originating in the troposphere 34.
 The Madden–Julian Oscillation (MJO) is a progression of convective activity from the Indian Ocean into the Pacific Ocean with a periodicity of 30–90 days 32.
 It has been shown that the movement of precipitation centres by the Madden–Julian Oscillation alters the east-west winds in the tropical stratosphere 51.
 Finally, the El Niño–Southern Oscillation influences the tropical circulation in both the troposphere and stratosphere through several mechanisms 33.
StationSeeker StationSeeker assigns a score s l to pressure levels, l , ranging from 5 kPa to 14 kPa in 50 Pa increments ( l = 1, …, 181). The highest-scoring altitude is designated as a setpoint. The score function is parametrized to balance four considerations within the station-keeping objective: reaching the station, remaining close to the station, exploring the wind field and limiting power consumption.
A wind score g l is computed for each pressure level l on the basis of the wind magnitude μ l and bearing θ l : $${g}_{l}=(1-{\alpha }_{\varDelta }){{\rm{e}}}^{-{w}_{\varDelta }{\theta }_{l}}+{\alpha }_{\varDelta }{{\rm{e}}}^{-{k}_{1}{\mu }_{l}}.$$ The term α Δ ∈ [0, 1] is the relative weight associated with the wind magnitude and bearing, and the term w Δ defines the magnitude of the penalty associated with winds whose direction makes a large angle with the direction of the station. Both of these decrease with the distance to the station, Δ , and k 1 is a constant. Close to the station, the magnitude term dominates and the controller seeks slow winds.
To incentivize exploration and minimize power consumption, s l interpolates between g l and a default score g unknown and penalizes large altitude changes: $${s}_{l}=(1-{u}_{l}){g}_{l}+{u}_{l}{g}_{{\rm{unknown}}}+{k}_{2}{{\rm{e}}}^{-{k}_{3}|l-{l}_{{\rm{current}}}|}.$$ The interpolation factor u l ∈ [0, 1] is the uncertainty at pressure level l , and k 2 and k 3 are constants. Interpolating using uncertainty leads the controller to navigate to unexplored altitudes when no high-scoring winds have been observed.
All parameters effect a trade-off between power and performance; they were previously tuned through multiple interactions with the simulator and manual analysis of real flight data. We validated this tuning by evaluating different settings of w Δ , k 1 , k 2 and g unknown on our 6,000-flight benchmark. We sampled 50 settings uniformly at random from reasonable ranges, and additionally varied one parameter at a time (Extended Data Fig.
4 ). At equal or lower power consumption, the best parametrization of the evaluation function achieves barely higher TWR50 than StationSeeker (41.6% versus 40.5%), within the range where the two can be considered equivalent.
Tree-search controller (OPD) In theory, a search-based approach can achieve near-optimal performance by enumerating all possible futures. However, using it in a production-grade flight controller is computationally impractical as it requires growing the search tree at every decision point. This is problematic in our setting, where partial observability creates sizeable randomness.
Instead, the tree-search controller models the flight dynamics deterministically, using first-order physics to determine the balloon’s position following a decision. It models night-time power consumption as a constant baseline rate plus an additional descent cost. Although practical, one downside of this simplified model is that it cannot account for measurements obtained during the planning process.
We use OPD 48 to plan over this model. OPD is designed for deterministic discounted decision processes. It improves over breadth-first search by focusing on the most promising sequences first, enabling longer planning horizons. This is particularly effective in our setting, where many decisions move the balloon away from the station. Once N nodes have been expanded, OPD selects the action offering the greatest lower bound on the return.
Gaussian process queries dominate the computational cost of a node expansion (75–95% of total cost). To make the process tractable we use a caching mechanism that aggregates neighbouring winds and impose a planning limit of 500 km and 33 h. By parallelizing the process over 8 threads, our algorithm can expand 200,000 nodes in roughly 800 ms. Although additional engineering could scale this process further, our results suggest that this would produce, at best, marginal gains. The search process is parametrized by the duration T of a single decision and a stride parameter S.
 The available setpoints are those reachable within that time interval, spaced S pascals apart. If T = 15 min and S = 100 Pa these are 0, ±100 and ±200 Pa, relative to the current altitude. Planning at the same resolution as the reinforcement-learning controller ( T = 3 and S = 50) remains computationally infeasible; as a nontrivial point of comparison, even using T = 15 and S = 100 achieves a low TWR50 of 34.4%.
The search controllers reported in Fig.
3b were obtained by varying T ∈ [15, 60], S ∈ [50, 200], γ ∈ {0.993, 0.999}, and a potential penalty for power usage ( f ω ∈ {0.9, 1.0}; see below). The best controller uses T = 60, S = 100, γ = 0.993 and f ω = 1.0.
The tuned-search controller replaces the tree by a lattice of 400-km 2 cells. All locations within a cell are approximated with its centre. Additionally, it uses the negative of the distance to the station as reward, ignoring power or the 50-km boundary.
Reward function We consider a distance-based reward $${r}_{{\rm{d}}{\rm{i}}{\rm{s}}{\rm{t}}}(\varDelta )=\{\begin{array}{cc}1.0 & {\rm{i}}{\rm{f}}\,\varDelta < \rho \\ {c}_{{\rm{c}}{\rm{l}}{\rm{i}}{\rm{f}}{\rm{f}}}{2}^{-(\varDelta -\rho )/\tau } & {\rm{o}}{\rm{t}}{\rm{h}}{\rm{e}}{\rm{r}}{\rm{w}}{\rm{i}}{\rm{s}}{\rm{e}}\end{array},$$ where Δ is the distance to the station. Smaller values of c cliff penalize exiting the range so that the objective is preserved; with c cliff = 1 and a low decay rate τ , the balloon may prefer to remain just outside the range, owing to discounting and approximation errors. The best performance is provided by τ = 100 km and c cliff = 0.4 (Fig.
2a ).
We modulate this reward by a multiplicative penalty for energy that is drawn from the battery or could be used to charge it. We found this performs better than an additive penalty, which dominates the action-value estimates far from the station. We use a normalized measure of power consumption ω ∈ [0, 1] during the 3-min time step. The power-penalized reward is r ( s , a ) = r ( Δ , ω ) = f ω r dist ( Δ ), with $${f}_{\omega }=\{\begin{array}{ll}0.95-0.3\omega & {\rm{if}}\,\omega > 0\\ 1.0 & {\rm{otherwise}}\end{array}.$$ State encoding The state vector consists of 1,083 wind variables (361 triples) and 16 ambient variables (Extended Data Table 1 ). 181 triples correspond to pressure levels from 5 kPa to 14 kPa in 50-Pa increments, and the central triple always corresponds to the balloon’s current altitude. The remaining triples are invalid for any given state; some altitudes are also inaccessible owing to changes in temperature, infrared radiation and levels of lift gas. We encode these using the limit triple (1, 1, 0), semantically equivalent to being maximally confident that the wind is blowing infinitely fast away from the station.
Deep reinforcement learning An action value 52 describes the expected return that follows from taking action a in state s : $${R}_{s,a}={\mathbb{E}}\,[\mathop{\sum }\limits_{t=0}^{\infty }{\gamma }^{t}r({s}_{t},{a}_{t})|{s}_{0}=s,{a}_{0}=a].$$ We estimate action values using the QR-DQN algorithm 46 , an open-source implementation of which is available online 53.
 QR-DQN uses collections of N real-valued locations \({\theta }_{s,a}^{i}\) : $${R}_{s,a}\approx \frac{1}{N}\mathop{\sum }\limits_{i=1}^{N}{\theta }_{s,a}^{i}.$$ Each location approximates a quantile of the probability distribution of the random return. This approach stabilizes learning in neural networks and improves performance compared to direct mean estimation 54 ; Fig.
2b shows a small (+1.2%) TWR50 improvement. We also use these locations to estimate the maximum performance in our simulation benchmark (see below).
We model QR-DQN’s locations using a seven-layer, 600-unit neural network. The network takes in a state vector as input and outputs 51 quantiles per action. Fewer (21) or more (101) quantiles did not affect performance. We maintain a separate target network 11 and use n -step backups 5.
 The controller then acts by selecting the action with the highest estimate. The network weights are optimized using Adam 55.
Extended Data Table 2 summarizes the parameters of the learning algorithm (hyperparameters). These were tuned using Vizier 56 and grid search. The learning process is most sensitive to Adam parameters, and these formed the bulk of early tuning. The process otherwise performs well for a range of parameters and many of these (for example, minibatch size, update horizon and number of quantiles) are imported from open-source implementations. The deployed controller was trained for 24 days (wall-clock time) and 1.1 billion gradient updates using a V100 graphics processing unit (GPU), and was selected from among ten similarly trained parametric variations. The controllers with performances reported in Fig.
1 were trained in the same fashion, but for 300 million gradient updates.
Training-data generation A trial is defined by the balloon’s initial longitude, latitude, altitude, date, time, station location and a random seed for wind noise. For training, trial start dates and locations are sampled uniformly at random from 2005–2010 (inclusive) and from the tropics (latitudes −25° to 25°); configurations lacking wind diversity are rejected. The balloon’s initial altitude is sampled uniformly from the accessible range, and its initial distance from the station is drawn from a scaled beta distribution with support 0–200 km; note that poor navigation will cause the balloon to experience larger distances. This procedure provides a diversity of learning scenarios, as weather patterns recur seasonally but differ across the planet.
Data are recorded as a series of state–action–reward transitions. Once a trial completes, its transitions are appended to a replay buffer chosen uniformly at random. The final transition of a trial lacks continuation and is ignored.
To efficiently explore the consequences of different decisions, we use an exploratory policy. This policy first samples a setpoint uniformly at random from the accessible range. This setpoint is then periodically updated with zero-mean Gaussian noise. This causes the balloon to perform a random walk between altitudes, rather than taking individual actions, providing momentum to exploration and improving its efficiency.
A majority (80% probability) of trials are selected for exploration. These trials interleave the reinforcement-learning controller with the exploratory policy in intervals of four and two hours, respectively, for a total of eight cycles per trial. Interleaving allows the controller to efficiently test macroscopic deviations from its usual behaviour, as following the exploratory policy for more than a few hours would produce mostly uninteresting data.
Simulation benchmark Our simulation benchmark is divided equally across three locations in Peru and two in Kenya, regions in which Loon operates. This assesses our controller’s ability to respond to different weather statistics; empirically, station-keeping is easier over Kenya than over Peru. One hundred dates selected from 2000 to 2015 correspond to each of the five locations, omitting training dates (2005–2010), each with 12 random seeds. This yields 6,000 flight configurations. The per-location dates are sampled in equal proportions from five difficulty categories derived from StationSeeker’s performance. Each category corresponds to a 20% TWR50 interval. For example, there are 20 dates for which StationSeeker achieves 0–19% TWR50. The balloon’s initial position is sampled as in training.
We simulated 12 more flights sharing a common Peru station, beginning on the first of each month of 2002, which we refer to as ‘the 2002 simulations’. This procedure yielded 11,520 states. Of these, the first state of each trajectory is discarded to correct for the simulator’s burn-in period.
Estimation of maximum benchmark performance By providing the OPD controller with perfect wind information, we reduce station-keeping to a path-planning problem with energy constraints. However, partial observability carries a performance cost. This is reflected in the spread of the neural network’s return distribution predictions (Extended Data Fig.
5 ). We therefore estimate the lower limit on maximum performance as the perfect-information upper limit, minus the cost of partial observability.
We assume the following: 1) the action-value distribution predictions are reasonably accurate; 2) the environment dynamics are deterministic given the random seed, such that any predicted quantile is obtained by some sequence of decisions; and 3) the predicted outcomes correspond to a sum of contiguous binary rewards. This last assumption enables us to interpret action-value differences as differences in the time spent successfully station-keeping. We write $${R}_{s,a}={\gamma }^{{T}_{s,a}},$$ where T s , a is the number of time steps before the balloon arrives within range of the station, after which point it remains within range until the simulation ends. An alternative model, \({R}_{s,a}=(1-{\gamma }^{{T}_{s,a}+1})/(1-\gamma )\) , leads to similar estimates.
Under these assumptions, the only source of randomness is partial observability. The largest predicted quantile, \({R}_{s,a}^{\ast }\) , therefore estimates the optimal return in the absence of partial observability. We associate this larger quantile with an earlier arrival time, \({T}_{s,a}^{\ast }\).
 For a given state s , we compute the time steps saved owing to full observability: $${T}_{s,a}^{\ast }-{T}_{s,a}=\frac{\log (\mathop{max}\limits_{a}{R}_{s,a}^{\ast })-\log (\mathop{max}\limits_{a}{R}_{s,a})}{-\log \gamma }.$$ Using the 2002 simulations, we estimate \({T}_{s,a}^{\ast }\) − T s , a = 57 steps, or 2.85 h. We convert this difference into TWR50 by dividing it by 24 h (480 time steps). 24 h corresponds to the expected length of time remaining in a simulated trial, when sampling a time step at random, as we do here. Anecdotally, it also roughly corresponds to the longest time period in which a balloon may station-keep without interruption, owing to changing winds and low battery charge at night. This calculation results in a TWR50 difference of 11.9%, which, once subtracted from the perfect-information limit, yields 56.8% TWR50 as a plausible maximum on performance on the simulation benchmark.
Sensitivity to initial conditions For each of the 2002 simulations, we consider 125 perturbations of the balloon’s initial position, arranged in a box (0, ±0.01, ±0.02 degrees latitude and longitude, 0, ±100, ±200 Pa). We record their average distance to the station and the average distance between pairs of balloons (ignoring altitude), which we call the pairwise distance (Extended Data Fig.
6 ). The numbers reported in the main text correspond to pairwise distances averaged across months; the medians across months are 55 km (reinforcement-learning controller) and 66 km (StationSeeker).
Response to bearing and magnitude as a function of distance We use a sensitivity analysis to study the reinforcement-learning controller’s decision-making process, focusing on how it trades off magnitude and bearing as a function of distance. We use finite differences to estimate the partial derivative of the controller’s action-value estimates with respect to input i —that is, its ‘response’ to i.
 For a given state s and action a we query the neural network for its action-value estimate R s , a.
 We then add δ = 0.01 to the component of s corresponding to input i to produce a new state y ( i ), and query the neural network for the new estimate R y ( i ), a.
 The network’s response e i is $${e}_{i}=\frac{{R}_{y(i),a}-{R}_{s,a}}{\delta }.$$ We repeat the process for each state s of the 2002 simulations, resulting in an average response ē i.
To obtain a response that depends on the distance Δ , we vary the distance-to-station input from 0 to 200 km, and fix it across states before querying the network. This yields a function ē i ( Δ ), which we rescale according to $${\hat{e}}_{i}(\varDelta )=\frac{{\bar{e}}_{i}(\varDelta )}{\mathop{max}\limits_{\varDelta {\prime} }|{\bar{e}}_{i}(\varDelta {\prime} )|}.$$ We compare ê i ( Δ ) to the derivative of StationSeeker’s score function with respect to i , rescaled as above so that the two controllers’ responses can be compared. Negative responses to magnitude and bearing respectively indicate an aversion to fast winds and to winds bearing away from the station.
Both controllers emphasize magnitude and bearing respectively near and far from the station (Extended Data Fig.
7 ). However, the reinforcement-learning controller exhibits a sharp change in sensitivity at 50 km. Beyond this boundary, it prefers fast winds (positive response), supporting our empirical observation that the controller works to return within range.
Preprocessing of real flight data We divide the flight data from our Pacific Ocean experiment into segments, each corresponding to a specific balloon station-keeping during a contiguous time period. Segments shorter than 6 h are omitted. For our reinforcement-learning controller, this yields 25 segments (mean segment length: 115 h, maximum length: 377 h). For StationSeeker, this yields 48 segments. Extended Data Figs.
1 – 3 depict all segments lasting at least 12 h. Most segments begin with the balloon approaching from a distance; in our analysis we discard this initial approach so that all flight segments start 50 km from the station.
Data availability The data analysed in this paper are available from the corresponding authors on reasonable request.
Code availability The code used to train the flight controllers is proprietary. The code used to analyse the generated data is available from the corresponding authors on reasonable request.
References Lally, V. E.
Superpressure Balloons for Horizontal Soundings of the Atmosphere Technical report (National Center for Atmospheric Research, 1967).
Anderson, B. & Moore, B. J.
Optimal Control: Linear Quadratic Methods (Prentice-Hall, 1989).
Camacho, E. F. & Bordons, C.
Model Predictive Control (Springer, 2007).
Bellman, R. E.
Dynamic Programming (Princeton Univ. Press, 1957).
Sutton, R. S. & Barto, A. G.
Reinforcement Learning: An Introduction 2nd edn (MIT Press, 2018).
Jakobi, N., Husbands, P. & Harvey, I. Noise and the reality gap: the use of simulation in evolutionary robotics. In Proc. European Conf. Artificial Life (eds Moran, F. et al.) 704–720 (Springer, 1995).
Tobin, J. et al. Domain randomization and generative models for robotic grasping. In Proc. Intl Conf. Intelligent Robots and Systems 3482–3489 (IEEE, 2018).
Levine, S., Kumar, A., Tucker, G. & Fu, J. Offline reinforcement learning: tutorial, review, and perspectives on open problems. Preprint at https://arxiv.org/abs/2005.01643 (2020).
Kaelbling, L. P., Littman, M. L. & Cassandra, A. R. Planning and acting in partially observable stochastic domains.
Artif. Intell.
101 , 99–134 (1998).
Article MathSciNet Google Scholar Tesauro, G. Temporal difference learning and TD-Gammon.
Commun. ACM 38 , 58–68 (1995).
Article Google Scholar Mnih, V. et al. Human-level control through deep reinforcement learning.
Nature 518 , 529–533 (2015).
Article ADS CAS Google Scholar Silver, D. et al. Mastering the game of Go with deep neural networks and tree search.
Nature 529 , 484–489 (2016).
Article ADS CAS Google Scholar Lauer, C. J., Montgomery, C. A. & Dietterich, T. G. Managing fragmented fire-threatened landscapes with spatial externalities.
For. Sci.
66 , 443–456 (2020).
Article Google Scholar Simão, H. P. et al. An approximate dynamic programming algorithm for large-scale fleet management: a case application.
Transport. Sci.
43 , 178–197 (2009).
Article Google Scholar Mannion, P., Duggan, J. & Howley, E. An experimental review of reinforcement learning algorithms for adaptive traffic signal control. In Autonomic Road Transport Support Systems (eds McCluskey, T. L. et al.) 47–66 (Springer, 2016).
Mirhoseini, A. et al. Chip placement with deep reinforcement learning. Preprint at https://arxiv.org/abs/2004.10746 (2020).
Nevmyvaka, Y., Feng, Y. & Kearns, M. Reinforcement learning for optimized trade execution. In Proc. Intl Conf. Machine Learning (eds Cohen, W. W. & Moore, A.) 673–680 (ACM, 2006).
Pineau, J., Bellemare, M. G., Rush, A. J., Ghizaru, A. & Murphy, S. A. Constructing evidence-based treatment strategies using methods from computer science.
Drug Alcohol Depend.
88 , S52–S60 (2007).
Article Google Scholar Anderson, R. N., Boulanger, A., Powell, W. B. & Scott, W. Adaptive stochastic control for the smart grid.
Proc. IEEE 99 , 1098–1115 (2011).
Article Google Scholar Glavic, M., Fonteneau, R. & Ernst, D. Reinforcement learning for electric power system decision and control: past considerations and perspectives.
IFAC PapersOnLine 50 , 6918–6927 (2017).
Article Google Scholar Theocharous, G., Thomas, P. S. & Ghavazamdeh, M. Personalized ad recommendation systems for life-time value optimization with guarantees. In Proc. Intl Joint Conf. Artificial Intelligence (eds Yang, Q. & Wooldridge, M.) 1806–1812 (AAAI Press, IJCAI, 2015).
Ie, E. et al. SlateQ: a tractable decomposition for reinforcement learning with recommendation sets. In Proc. Intl Joint Conf. Artificial Intelligence (ed. Kraus, S.) 2592–2599 (IJCAI, 2019).
Ross, S., Gordon, G. & Bagnell, D. A reduction of imitation learning and structured prediction to no-regret online learning. In Proc. 14th Intl Conf. Artificial Intelligence and Statistics , (eds Gordon, G. et al.) 627–635 (PMLR, 2011).
Tan, J. et al. Sim-to-real: learning agile locomotion for quadruped robots. In Proc. Robotics: Science and Systems XIV (eds Kress-Gazir, H. et al.) 10 (2018).
Ng, A. Y., Kim, H. J., Jordan, M. I. & Sastry, S. Autonomous helicopter flight via reinforcement learning. In Advances in Neural Information Processing Systems 16 (NIPS 2003) (eds Saul, L. K. et al.) 799–806 (2004).
Abbeel, P., Coates, A., Quigley, M. & Ng, A. Y. An application of reinforcement learning to aerobatic helicopter flight. In Advances in Neural Information Processing Systems 19 (NIPS 2006) (eds Schölkopf, B. et al.) 1–8 (MIT Press, 2007).
Reddy, G., Wong-Ng, J., Celani, A., Sejnowski, T. J. & Vergassola, M. Glider soaring via reinforcement learning in the field.
Nature 562 , 236–239 (2018).
Article ADS CAS Google Scholar Lange, S., Riedmiller, M. & Voigtländer, A. Autonomous reinforcement learning on raw visual input data in a real world application. In Proc. Intl Joint Conf. Neural Networks https://doi.org/10.1109/IJCNN.2012.6252823 (IEEE, 2012).
Levine, S., Pastor, P., Krizhevsky, A., Ibarz, J. & Quillen, D. Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection.
Int. J. Robot. Res.
37 , 421–436 (2018).
Article Google Scholar Kalashnikov, D. et al. Scalable deep reinforcement learning for vision-based robotic manipulation. In Proc. Conf. Robot Learning Vol. 87 (eds Billard, A. et al.) 651–673 (PMLR, 2018).
Andrychowicz, O. M. et al. Learning dexterous in-hand manipulation.
Int. J. Robot. Res.
39 , 3–20 (2020).
Article Google Scholar Zhang, C. Madden–Julian Oscillation.
Rev. Geophys.
43 , RG2003 (2005).
ADS Google Scholar Domeisen, D. I., Garfinkel, C. I. & Butler, A. H. The teleconnection of El Niño Southern Oscillation to the stratosphere.
Rev. Geophys.
57 , 5–47 (2018).
Article ADS Google Scholar Baldwin, M. et al. The quasi-biennial oscillation.
Rev. Geophys.
39 , 179–229 (2001).
Article ADS Google Scholar Friedrich, L. S. et al. A comparison of Loon balloon observations and stratospheric reanalysis products.
Atmos. Chem. Phys.
17 , 855–866 (2017).
Article ADS CAS Google Scholar Coy, L., Schoeberl, M. R., Pawson, S., Candido, S. & Carver, R. W. Global assimilation of Loon stratospheric balloon observations.
J. Geophys. Res. D 124 , 3005–3019 (2019).
Article ADS Google Scholar Rasmussen, C. E. & Williams, C. K. I.
Gaussian Processes for Machine Learning (MIT Press, 2006).
Sondik, E.
The Optimal Control of Partially Observable Markov Processes.
 PhD thesis, Stanford Univ. (1971).
Hersbach, H. et al. The ERA5 global reanalysis.
Q. J. R. Meteorol. Soc.
146 , 1999–2049 (2020).
Article ADS Google Scholar Perlin, K. An image synthesizer.
Comput. Graph.
19 , 287–296 (1985).
Article Google Scholar Bellemare, M. G., Naddaf, Y., Veness, J. & Bowling, M. The Arcade Learning Environment: an evaluation platform for general agents.
J. Artif. Intell. Res.
47 , 253–279 (2013).
Article Google Scholar Kolter, Z. J. & Ng, A. Y. Policy search via the signed derivative. In Proc. Robotics: Science and Systems V (eds Trinkle, J. et al.) 27 (MIT Press, 2009).
Levine, S. & Koltun, V. Guided policy search. In Proc. Intl Conf. Machine Learning Vol. 28-3 (eds Dasgupta, S. & McAllester, D.) 1–9 (ICML, 2013).
Lin, L. Self-improving reactive agents based on reinforcement learning, planning and teaching.
Mach. Learn.
8 , 293–321 (1992).
Google Scholar Nair, V. & Hinton, G. E. Rectified linear units improve restricted Boltzmann machines. In Proc. 27th Intl Conf. Machine Learning (ed. Fürnkranz, J.) 807–814 (ICML, 2010).
Dabney, W., Rowland, M., Bellemare, M. G. & Munos, R. Distributional reinforcement learning with quantile regression. In Proc. AAAI Conf. Artificial Intelligence 2892–2901 (AAAI Press, 2018).
Mnih, V. et al. Asynchronous methods for deep reinforcement learning. In Proc. Intl Conf. Machine Learning Vol. 48 (eds Balcan, M.-F. &Weinberger, K. Q.) 1928–1937 (ICML, 2016).
Munos, R. From bandits to Monte-Carlo tree search: the optimistic principle applied to optimization and planning.
Found. Trends Mach. Learn.
7 , 1–129 (2014).
Article ADS Google Scholar Gibson, J. J.
The Ecological Approach to Visual Perception (Taylor & Francis, 1979).
Brooks, R. Elephants don’t play chess.
Robot. Auton. Syst.
6 , 3–15 (1990).
Article Google Scholar Alexander, M., Grimsdell, A., Stephan, C. & Hoffmann, L. MJO-related intraseasonal variation in the stratosphere: gravity waves and zonal winds.
J. Geophys. Res. D Atmospheres 123 , 775–788 (2018).
Article ADS Google Scholar Watkins, C. J. C. H.
Learning from Delayed Rewards.
 PhD thesis, Cambridge Univ. (1989).
Castro, P. S., Moitra, S., Gelada, C., Kumar, S. & Bellemare, M. G. Dopamine: a research framework for deep reinforcement learning. Preprint at https://arxiv.org/abs/1812.06110 (2018).
Bellemare, M. G., Dabney, W. & Munos, R. A distributional perspective on reinforcement learning. In Proc. Intl Conf. Machine Learning Vol. 70 (eds Precup, D. & Teh, Y. W.) 449–458 (PMLR, 2017).
Kingma, D. & Ba, J. Adam: A method for stochastic optimization. In Proc. Intl Conf. Learning Representations (eds Benigo, Y. & LeCun, Y.) (2015).
Golovin, D. et al. Google Vizier: a service for black-box optimization. In Proc. ACM SIGKDD Intl Conf. Knowledge Discovery and Data Mining (eds Matwin, S. et al.) 1487–1496 (ACM, 2017).
Download references Acknowledgements We are grateful to J. Davidson for early ideation and prototyping. We thank V. Vanhoucke, K. Choromanski, V. Sindhwani, C. Boutilier, D. Precup, S. Mourad, S. Levine, K. Murphy, A. Faust, H. Larochelle and J. Platt for discussions; M. Bowling, A. Guez, D. Tarlow, J. Drouin, and M. Brenner for feedback on earlier versions of the manuscript; W. Dabney for feedback and help with design; T. Larivee for help with visuals; N. Mainville for project management support; R. Carver for information on weather phenomena; and the Loon operations team.
Author information Authors and Affiliations Brain Team, Google Research, Montreal, Quebec, Canada Marc G. Bellemare, Pablo Samuel Castro, Marlos C. Machado & Subhodeep Moitra Brain Team, Google Research, Toronto, Ontario, Canada Ziyu Wang Loon, Mountain View, CA, USA Salvatore Candido, Jun Gong & Sameera S. Ponda Authors Marc G. Bellemare View author publications You can also search for this author in PubMed Google Scholar Salvatore Candido View author publications You can also search for this author in PubMed Google Scholar Pablo Samuel Castro View author publications You can also search for this author in PubMed Google Scholar Jun Gong View author publications You can also search for this author in PubMed Google Scholar Marlos C. Machado View author publications You can also search for this author in PubMed Google Scholar Subhodeep Moitra View author publications You can also search for this author in PubMed Google Scholar Sameera S. Ponda View author publications You can also search for this author in PubMed Google Scholar Ziyu Wang View author publications You can also search for this author in PubMed Google Scholar Contributions S.C. conceptualized the problem. J.G., S.C., S.S.P., P.S.C. and S.M. built the technical infrastructure. S.C., M.G.B., J.G., M.C.M. and P.S.C. developed and tested the algorithm. M.C.M., M.G.B., P.S.C., S.C., S.S.P., J.G. and Z.W. performed experimentation and data analysis. M.G.B. and S.C. managed the project. M.G.B., S.C., M.C.M., P.S.C. and S.S.P. wrote the paper. Authors are listed alphabetically by surname.
Corresponding authors Correspondence to Marc G. Bellemare or Salvatore Candido.
Ethics declarations Competing interests M.G.B., S.C., J.G. and M.C.M. have filed patent applications relating to navigating aerial vehicles using deep reinforcement learning. The remaining authors declare no competing financial interests.
Additional information Peer review information Nature thanks Scott Osprey and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. Peer reviewer reports are available.
Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
Extended data figures and tables Extended Data Fig. 1 Flight paths of the reinforcement-learning controller during the Pacific Ocean experiment.
The x and y axes represent longitude and latitude.
Extended Data Fig. 2 Flight paths of StationSeeker during the Pacific Ocean experiment, 1 of 2.
The x and y axes represent longitude and latitude, respectively.
Extended Data Fig. 3 Flight paths of StationSeeker during the Pacific Ocean experiment, 2 of 2.
The x and y axes represent longitude and latitude, respectively.
Extended Data Fig. 4 TWR50 and power consumption for different parametrizations of StationSeeker’s score function.
Grey points indicate settings chosen uniformly at random from the following ranges: w Δ ∈ [0.4, 0.8] (at close range), k 1 ∈ [0.01, 0.15], g unknown ∈ [0.4, 0.6] and k 2 ∈ [0, 0.2]. Each parameter was also varied in isolation (coloured points). Semantically interesting parameter choices are highlighted.
Extended Data Fig. 5 Distribution of returns predicted by the neural network.
Each panel indicates the predicted distributions for a particular state and action. The 51 quantiles output by the network are smoothed using kernel density estimation ( σ determined from Scott’s rule with interquartile range scaling). The dashed lines indicate the average of these locations. The states with depicted distributions are from different times (0, 3, 6 and 9 h) into the July 2002 simulation. We use the largest quantile to estimate the return that could be realized in the absence of partial observability.
Extended Data Fig. 6 Average distances and pairwise distances for perturbations of 12 initial conditions.
a , Distance to station, averaged over 125 perturbations. These numbers highlight how the 1 January to 1 June 2002 simulations (May excluded) were challenging station-keeping conditions. The 1 January configuration, in particular, lacked wind diversity.
b , Average distance between pairs of balloons (7,750 pairs). Our controller exhibits greater robustness to challenging conditions.
Extended Data Fig. 7 Scaled response of controllers to wind bearing and magnitude as a function of distance.
We use the derivative of the network’s action-value estimates, or response, as a proxy for the relative weight of an input. The two inputs tested here are the wind bearing and magnitude at the balloon’s altitude; the curves report the derivative for the ‘stay’ action.
Full size table Full size table Supplementary information Video 1 : Simulation of 125 balloons station keeping in challenging conditions. Simulation of 125 balloons starting from perturbations of a single initial position, either using the learned controller or StationSeeker. The station is denoted by an 'X', and the 50 km range by a dashed line. Unlike StationSeeker, the learned controller is able to remain near the station irrespective of initial conditions, despite a highly challenging wind field. It achieves this by navigating away from the station to avoid strong winds and remain in a relatively calm area, visible from 0:06 into the video.
Video 2 : Simulation of 125 balloons station keeping in easy conditions. Simulation of 125 balloons starting from perturbations of a single initial position, either using the learned controller or StationSeeker. In this relatively easy scenario, the learned controller arrives at the station faster than StationSeeker and regroups more quickly after the second night. Its performance is generally less dependent on the initial condition.
Video 3 : Flight #16 from the Pacific Ocean experiment. This video depicts flight #16 (learned controller) over the Pacific Ocean. The right panel depicts the controller's observed wind column, with colour representing uncertainty. StationSeeker's proposed choices are highlighted in that panel. When possible, the learned controller remains stationary by remaining at the interface between opposing wind sheets. Its station keeping patterns make use of the full 50 km range, providing it with additional energy late into the night..
Peer Review File Rights and permissions Reprints and Permissions About this article Cite this article Bellemare, M.G., Candido, S., Castro, P.S.
et al.
Autonomous navigation of stratospheric balloons using reinforcement learning.
Nature 588 , 77–82 (2020). https://doi.org/10.1038/s41586-020-2939-8 Download citation Received : 01 April 2020 Accepted : 29 September 2020 Published : 02 December 2020 Issue Date : 03 December 2020 DOI : https://doi.org/10.1038/s41586-020-2939-8 Share this article Anyone you share the following link with will be able to read this content: Sorry, a shareable link is not currently available for this article.
Provided by the Springer Nature SharedIt content-sharing initiative This article is cited by Adaptable control policies for variable liquid chromatography columns using deep reinforcement learning David Andersson Christoffer Edlund Rickard Sjögren Scientific Reports (2023) Optimal tracking strategies in a turbulent flow Chiara Calascibetta Luca Biferale Massimo Cencini Communications Physics (2023) Designing INS/GNSS integrated navigation systems by using IPO algorithms Ali Mohammadi Farid Sheikholeslam Seyedali Mirjalili Neural Computing and Applications (2023) Autonomous vehicles decision-making enhancement using self-determination theory and mixed-precision neural networks Mohammed Hasan Ali Mustafa Musa Jaber P. Punitha Multimedia Tools and Applications (2023) Near-grazing bifurcations and deep reinforcement learning control of an impact oscillator with elastic constraints Zhao Tan Shan Yin Xin Wu Meccanica (2023) Comments By submitting a comment you agree to abide by our Terms and Community Guidelines.
 If you find something abusive or that does not comply with our terms or guidelines please flag it as inappropriate.
You have full access to this article via your institution.
Download PDF Download PDF Associated content Autonomous balloons take flight with artificial intelligence Scott M. Osprey Nature News & Views Advertisement Explore content Research articles News Opinion Research Analysis Careers Books & Culture Podcasts Videos Current issue Browse issues Collections Subjects Follow us on Facebook Follow us on Twitter Sign up for alerts RSS feed About the journal Journal Staff About the Editors Journal Information Our publishing models Editorial Values Statement Journal Metrics Awards Contact Editorial policies History of Nature Send a news tip Publish with us For Authors For Referees Language editing services Submit manuscript Search Quick links Explore articles by subject Find a job Guide to authors Editorial policies Nature ( Nature ) ISSN 1476-4687 (online) ISSN 0028-0836 (print) nature.com sitemap About Nature Portfolio About us Press releases Press office Contact us Discover content Journals A-Z Articles by subject Nano Protocol Exchange Nature Index Publishing policies Nature portfolio policies Open access Author & Researcher services Reprints & permissions Research data Language editing Scientific editing Nature Masterclasses Live Expert Trainer-led workshops Research Solutions Libraries & institutions Librarian service & tools Librarian portal Open research Recommend to library Advertising & partnerships Advertising Partnerships & Services Media kits Branded content Career development Nature Careers Nature Conferences Nature events Regional websites Nature Africa Nature China Nature India Nature Italy Nature Japan Nature Korea Nature Middle East Privacy Policy Use of cookies Your privacy choices/Manage cookies Legal notice Accessibility statement Terms & Conditions Your US state privacy rights © 2023 Springer Nature Limited Close Sign up for the Nature Briefing newsletter — what matters in science, free to your inbox daily.
Close Get the most important science stories of the day, free in your inbox.
