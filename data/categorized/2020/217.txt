old id = 4899
Robots That Use Language | Annual Review of Control, Robotics, and Autonomous Systems
2020
https://www.annualreviews.org/doi/full/10.1146/annurev-control-101119-071628

Robots That Use LanguageRobots That Use LanguageAnnual Review of Control, Robotics, and Autonomous SystemsVol. 3:25-55 (Volume publication date May 2020)First published as a Review in Advance on January 31, 2020https://doi.org/10.1146/annurev-control-101119-071628Stefanie Tellex,1Nakul Gopalan,2Hadas Kress-Gazit,3and Cynthia Matuszek41Department of Computer Science, Brown University, Providence, Rhode Island 02912, USA; email:[email protected]2School of Interactive Computing, Georgia Institute of Technology, Atlanta, Georgia 30332, USA; email:[email protected]3Sibley School of Mechanical and Aerospace Engineering, Cornell University, Ithaca, New York 14853, USA; email:[email protected]4Department of Computer Science and Electrical Engineering, University of Maryland, Baltimore County, Baltimore, Maryland 21250, USA; email:[email protected]AbstractThis article surveys the use of natural language in robotics from a robotics point of view. To use human language, robots must map words to aspects of the physical world, mediated by the robot's sensors and actuators. This problem differs from other natural language processing domains due to the need to ground the language to noisy percepts and physical actions. Here, we describe central aspects of language use by robots, including understanding natural language requests, using language to drive learning about the physical world, and engaging in collaborative dialogue with a human partner. We describe common approaches, roughly divided into learning methods, logic-based methods, and methods that focus on questions of human–robot interaction. Finally, we describe several application domains for language-using robots.
Keywordsrobots,language,grounding,learning,logic,dialogue1. INTRODUCTIONAs robots become more capable, they are moving into environments where they are surrounded by people who are not robotics experts. Such robots are appearing in the home, in nondedicated manufacturing spaces, and in the logistics industry (1,2), among other places. Since most users will not be experts, it is becoming essential to provide natural, simple ways for people to interact with and control robots. However, traditional keyboard-and-mouse and touch-screen interfaces require training and must be complex in order to enable a person to command complex robotic behavior (3). Higher-level abstractions, such as automata (4), programming abstractions (5), and structured language (6), offer a great degree of power and flexibility but also require a great deal of training to use.
By contrast, people use language every day to direct behavior, ask and answer questions, provide information, and ask for help. Language-based interfaces require minimal user training and allow the expression of a variety of complex tasks. This article reviews the current state of the art in natural language communication with robots, compares different approaches, and discusses the challenges of creating robust language-based human–robot interaction (HRI). The fundamental question forgrounded language understandingis, How can words and language structures be grounded in the noisy, perceptual world in which a robot operates (7)?We distinguish between two dual problems: language understanding, where the robot must interpret and ground the language, usually producing a behavior in response, and language generation, in which the robot produces communicative language, for example, to ask for explanations or answer questions. In the latter problem, the robot may need to reason about information-gathering actions (such as when to ask clarification questions) or incorporate other communication modalities (such as gestures). Systems that address both problems enable robots to engage in collaborative dialogue.
There is a long history of systems that try to understand natural language in physical domains, beginning with Reference8. Generally, language is most effective as an interface when users are untrained, are under high cognitive load, and have their hands and eyes busy with other tasks. For example, in search-and-rescue tasks, robots might interact with human victims who are untrained and under great stress (9). The context in which language is situated can take many forms; examples include sportscasts of simulated soccer games (10), linguistic descriptions of spatial elements in video clips (11), GUI interactions (12), descriptions of objects in the world (13), spatial relationships (14), and the meaning of instructions (15). Language has also been used with a diverse group of robot platforms, ranging from manipulators to mobile robots to aerial robots.
Figure 1shows some examples.
Language for robotics is currently an area of significant research interest, as evidenced by the papers covered in this article and the many recent workshops on this subject (e.g., the Grounding Language for Physical Systems workshop at the 2012 Conference on Artificial Intelligence, the Model Learning for Human-Robot Interaction workshop at the 2016 Robotics: Science and Systems conference, the Language Grounding for Robotics workshop at the 2017 Annual Meeting of the Association for Computational Linguistics, the Models and Representations for Natural Human-Robot Communication workshop at the 2018 Robotics: Science and Systems conference, and the Combined Workshop on Spatial Language Understanding and Grounded Communication for Robotics at the 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies). Other survey papers have reviewed related topics; for example, Fong et al. (16) surveyed socially interactive robots, and Goodrich & Schultz (17) and Thomaz et al. (18) provided broad surveys of HRI, although neither focused on language specifically. This survey is intended for robotics researchers who wish to understand the current state of the art innatural language processing (NLP)as it pertains to robotics.
Figure 2shows a system flow diagram for a language-using robot. First, natural language input is collected via a microphone or text. Words are converted to a semantic representation via language processing; possible representations range from a finite set of actions to an expression in a formal representation language, such as predicate calculus. For example, the words “red block” might be converted to a formal expression such as. Next, symbols in the semantic representation are connected or grounded to aspects of the physical world. For example, the system might use inference to search for objects in its world model that satisfy the predicatesand. The results inform decision-making; the robot might perform a physical action (such as retrieval) or a communicative action (such as asking, “This red block?”). Many approaches to language for robotics fit into this framework; they vary in the behaviors they include, the problems they solve, and the underlying mathematics of the modules.
This article is organized as follows: Section 2 gives preliminary material common to all methods. Section 3 covers technical approaches, organized around the method used to achieve language-using robots. Section 4 provides an orthogonal view that organizes the state of the art around the problem being addressed: human-to-robot communication, robot-to-human communication, and two-way communication. Section 5 concludes with a summary and a discussion of current open questions.
2. PRELIMINARIESIn this section, we define common terminology used in this field and provide technical background needed to understand many of the approaches described in subsequent sections. We review the concept ofgrounded language, the syntactic and semantic structure of language, and statistical language processing.
2.1. Grounded LanguageGrounded language (also called situated language or physically situated language) has meaning in the context of the physical world—for example, by describing the environment, physical actions, or relationships between things (7,19). Possible groundings range from low-level motor commands to perceptual inputs to complex sequences of actions. Grounded language acquisition is the process of learning these connections between percepts and actions. For example, if a person instructs a robot to pick up a cup, the robot must map the word “cup” to a particular set of percepts in its high-dimensional sensor space—for example, by recognizing that a particular pattern in its camera sensor corresponds to this word. Then, to follow the command, it must produce a plan or policy that causes its end effector to create a stable grasp of the cup and lift it. Many aspects of this plan are implied by the language but not explicitly stated; for example, if the cup has water in it, the robot should lift it in a way that does not cause the water to spill. This mapping between language and objects, places, paths, and events or action sequences in the world is a key challenge for language and robotics and represents the grounding problem. For robots, language is used primarily as a mechanism for describing objects or desired actions in the physical world; much of the work described in this survey is in the domain of grounded language. A key research question is how to represent this mapping between words and symbols and high-dimensional data streaming in from sensors and high-dimensional outputs that are available from actuators.
Table 1shows examples of language and possible groundings. Note that in some cases, the grounding is a discrete output from a classifier, while in other cases it is a high-dimensional controller command, such as “contra-rotate the steering actuators.” These are examples of possible groundings that have been used in the literature; two key research questions are what the grounding process should look like and how this mapping should be carried out.
Examples of natural language and possible groundings2.2. Syntactic Representations and AnalysisNatural language has a hierarchical, compositional syntax (20) that is studied in linguistics and cognitive semantics. This structure enables people to understand novel sentences by combining individual words in new ways (21). This syntactic structure can be used to help extract semantic representations of the words’ meaning. A variety of formalisms have been created to express this structure, of which the best known is context-free grammars (CFGs), developed in the 1950s by Noam Chomsky (22). CFGs and their many variants are used to describe the syntactic structure of natural language. Sipser (23) provided a formal definition of CFGs, andFigure 3gives an example CFG for a small subset of English along with an associatedparse tree. Many variants of CFGs exist. Pretrained parsers are a common tool, many of them (24,25) trained using the Penn Treebank (26), a corpus of text manually annotated with parse trees. Other parsers are trained on corpora of text, such as newspaper articles. These data are often not a good fit for robotics tasks, which typically contain imperative commands and spatial language, leading to reduced performance on robotics tasks by off-the-shelf tools.
Many robotics applications use combinatory categorial grammars (CCGs) (27). CCGs are a grammar formalism created to handle linguistic constructions such as coordination (e.g., “John and Mary like apples and oranges”) that cannot be expressed by CFGs. CCGs are useful because they model both the syntax and the semantics of language—an approach that is useful for real-world language learning. These learned elements take the form of lexical entries, which combine natural language, syntax, and semantics. Extensive work has been done on automatically creating parsers (28–30), typically learning from pairs of natural language sentences and sentence meanings. CCGs have been applied to robotic language understanding in many contexts (29,31–33), which are reviewed in the following sections.
2.3. Formal Semantic Representations of LanguageSemantic representations, which capture the meanings of words and sentences in a formalism that can be operated on by computers, can be extracted with (or from) syntactic structures, such as the example inFigure 3. A possible semantic interpretation can be captured by the first-order logic formulawhich states, “There exists anxthat is an apple and that is being grabbed.” Given a consistent formal meaning for, e.g.,, this expression can be interpreted and used for understanding actions in the world. Extensive work has been done on symbolic representations of semantics (e.g.,20,34–36). CFG productions can be combined usingλ-calculusrules to automatically construct semantic representation from a syntax tree. In this section, we briefly mention the main semantic building blocks that are used by many approaches.
First-order predicate logic extends propositional (Boolean) logic with predicates, functions, and quantification. Semantic meaning can be extracted using compositional operators associated with each branch of the syntactic tree. To perform language grounding in the context of robotics, these operators must be grounded in the physical world, i.e., through sensors and actuators; for example,could be grounded to a manipulation action. Additional information regarding the formal syntax and semantics of first-order predicate logic can be found in logic texts, such as the textbook by Huth & Ryan (37).
Temporal logicsare modal logics that contain temporal operators (38), allowing for the representation of time-dependent truths. (For example, the phrase “grab an apple” implies that the apple should be grabbed at some future point in time, an operation referred to as “eventually,” written, whereGrabAppleis a Boolean proposition that becomesTruewhen the apple is grabbed.) There are different temporal logics that vary in several important dimensions, including whether time is considered to be discrete or continuous, whether time is linear (formulas are defined over single executions) or branching (formulas are defined over trees of executions), and whether the logics are deterministic or include probabilistic operators and reasoning. In a recent review, Kress-Gazit et al. (39) described the use of several temporal logics in the context of robot control.
2.4. Statistical Natural Language Processing and Deep LearningSubstantial progress in NLP has been made by eschewing the explicit modeling of linguistics structures. For example,n-gram models that focus on counting words (40) robustly capture aspects of language use without requiring a full understanding of syntax or meaning, by leveraging the statistics of word co-occurrence. Shallow parsing or chunking is useful to capture aspects of syntax and semantics without performing a complete analysis of the sentence (41). Many approaches rely on less linguistically plausible but more robust structures to achieve learnability and tractability. Modern approaches use word vectors to capture or learn structure, such as long short-term memory units (LSTMs) (42) combined with Word2Vec (43) or Paragraph Vector (44). These approaches learn a vector representation associated with either words or longer documents and then compute over an entire sentence to perform tasks such as language modeling, parsing, or machine translation. Many robotics applications leverage these techniques to learn a statistical or deep model that maps between a human language and one of the formal representations mentioned above.
3. CLASSIFICATIONS BY TECHNICAL APPROACHIn this section, we cluster approaches based on three broad categories: lexically grounded methods (Section 3.1), learning methods (Section 3.2), and HRI-centered approaches (Section 3.3). The first category, lexically grounded methods, focuses on defining word meanings in a symbol system, typically through a manual or knowledge base grounding process, and using logics, grammars, and other linguistic structures to understand and generate sentences. The second category of approaches covers learning word and utterance meanings from large data sets, with inspirations drawn from machine learning and computational linguistics. Finally, HRI-centered approaches focus on the language experience for people interacting with robots. While we use these broad categories to discuss approaches, in practice much of the work in this field belongs to more than one category. The categories are not intended to be mutually exclusive; rather, they provide a possible framework for considering the overall research space.
3.1. Lexically Grounded MethodsThis section describes work that uses a priori grounded tokens such as objects and actions, with formal symbolic representations for the underlying semantics. Many of these approaches are based on formal logics; temporal logics are frequently used, as there are algorithms to transform the resulting formulas into behaviors that provide guarantees on performance and correctness (39). These approaches are often less robust to unexpected inputs produced by untrained users and can be difficult to implement at scale due to the manually grounded lexicon; however, they enable grounding rich linguistic phenomena such as anaphora (for example, the “it” in “grab the apple, I want to eat it”) and reasoning about incomplete information.
Common to the formal approaches described in this section is the grounding of linguistic tokens, such as nouns and verbs, to perceptual information and robot actions. For example, the token “cup” can be grounded to the output of an object detector, or the action “open door” can be grounded to a motion planner that controls a manipulator. These groundings can be either learned or manually prescribed, but in contrast to learning approaches (Section 3.2), analysis of utterances and groundings is performed using syntactic and formal semantic structures. Because manually grounding words in a lexicon is a time-consuming process, existing knowledge bases and cognitive architectures are often used to automatically enrich the lexicon using a base set of manual groundings.
Many existing knowledge bases provide real-world, common-sense knowledge that can be used to create language-using robots. WordNet (45) provides a lexicon of word meanings in English along with relations to other words in a hierarchy. These relations map symbols to other symbols and can be used to initialize or enrich groundings, especially nouns. VerbNet (46) is a large lexicon of verbs, including frames, argument structures, and parameterized actions. Given a grounding of an action, many verbs can be used in associated natural language utterances (47). Similarly, FrameNet (48) created a data set of verb meanings with parameterized actions. ImageNet (49) is an image database organized using nouns in the WordNet hierarchy. This data set has been used extensively in computer vision and provides information that could enable a robot to detect objects and ground noun phrases. Data sets that are specific to a particular type of grounding task also exist, such as RefCOCO (50) forreferring expressions(41).
Similar to knowledge bases, cognitive architectures encode relationships between symbols; however, these architectures typically encode complex relations between concepts in cognitive models designed to support reasoning mechanisms that enable completion of inferential tasks. In the context of language and robotics, work has been done with Soar (51;https://soar.eecs.umich.edu), ACT-R (Adaptive Character of Thought–Rational) (52), and DIARC (Distributed Integrated Affect, Reflection, and Cognition) (53,54), among others.
Soar (51;https://soar.eecs.umich.edu) is a theoretical framework and software tool designed to model human cognition. It includes knowledge, hierarchical reasoning, planning, execution, and learning, with the intent of creating general-purpose intelligent agents able to accomplish many different tasks. Researchers have proposed NL-Soar (55), a system that enables language understanding and generation that is interleaved with task execution. From the language side, tree-based syntactic models, semantic models, and discourse models are constructed that enable the system to create a dialogue with a person. Building on this work, Huffman & Laird (56) introduced Instructo-Soar, enabling new instructions to be grounded to procedures in Soar. Instructo-Soar assumes simple imperative sentences that are straightforward to parse and instantiate as a new operator template. Language groundings can also be learned from mixed-initiative HRIs that include language, gestures, and perceptual information (57). The language to be grounded is first syntactically parsed based on a given grammar and dictionary, and then the noun phrases are mapped to objects in the perceptual field, the verbs to actions in the Soar database, and spatial relations to a set of known primitives.
ACT-R and ACT-R/E (Adaptive Character of Thought–Rational/Embodied), introduced by Trafton et al. (52), are frameworks in which cognition is implemented in an embodied agent that must move in space. ACT-R/E has as a goal the ability to model and understand human cognition in order to reproduce and imitate human cognitive capabilities. It has some language capabilities in order to accept commands such as “go hide” to play hide-and-seek.
The DIARC architecture (53,54), which has been under development for more than 15 years, adopts a distributed architecture that does not attempt to model human cognition. Instead, different instantiations that correspond to different cognitive abilities with varying levels of complexity can be created, determined by the intended use. In the DIARC architecture (54,58–60), researchers created a system that incrementally processes natural language utterances, creates goals for a planner, and executes the instructions, as shown inFigure 1e(31). In that work, the lexicon is labeled with both syntactic annotations from a CCG (27,29) and semantic annotations in the form of λ-expressions related to the temporal logic CTL*(38) and first-order dynamic logic. When an utterance is provided, it is incrementally parsed—i.e., a parse is available after every token, the parse is updated as new tokens are received, and the semantics are incrementally produced. Later work employed pragmatic inference to enable more complex language interaction where the meaning of the utterances may be implicit and where context and semantics are combined (61,62).
PRAC (Probabilistic Action Cores) (63), while not a cognitive architecture per se, generalizes the notion of a knowledge base by creating a system that enables inferring over, disambiguating, and completing vague or underspecified natural language instructions by using information from existing lexical databases and drawing on background knowledge from WordNet and FrameNet, among other sources. From this information, the robot can infer a motor action that causes a source object to end up in a goal location.
Figure 1gshows an image from this work.
All of these architectures rely on hand-coded atomic knowledge that a human designer imparts to the robot, plus composition operators that enable the creation of more complex knowledge. These frameworks are carefully designed based on theories of cognition, leading to rich, evocative demonstrations. However, it is difficult for these systems to scale to large data sets of language or situations produced by untrained users. This sort of scaling and robustness is a key future challenge.
In addition to grounding tokens such as objects and places into detectors, approaches that utilize formal reasoning typically attach semantic structures to lexical items, such as verbs, and to the production rules of the grammar. These semantic structures are used to understand the semantics of utterances and define new lexical items, such as objects and actions. The semantics are typically fed into either a dialogue manager or a planner that executes situated robot actions. Broadly speaking, the following approaches to language interactions follow a similar pipeline: Natural language utterances in the form of text are syntactically parsed and then semantically resolved (and, in some work, pragmatically analyzed) to produce formal representations of the language's meaning.
Early examples of end-to-end systems that use formal representations for natural language interactions were GRACE (GraduateRobotAttendingConference) and GEORGE (GraduateRobot AttendingConference), robots that competed in the Association for the Advancement of Artificial Intelligence (AAAI) robot challenges. At the 2004 AAAI National Conference on Artificial Intelligence, GRACE acted as an information kiosk, providing information about the conference and giving directions, while GEORGE physically escorted people to their destinations (64). Both robots utilized the Nautilus parser (65), which uses a CFG to produce an intermediate syntactic representation that can be pattern matched to a semantic structure available to the interpreter. Building on the Nautilus parser and the GRACE system, the MARCO agent (66) was created to interpret route instructions given in natural language, combining syntactic and semantic structures with information from the perception system regarding the environment.
The process of grounding and executing natural language instructions from websites such as wikiHow was explored by Tenorth et al. (67). The system uses the Stanford parser (68), which uses a probabilistic CFG to syntactically parse instructions. These instructions are grounded using WordNet (45) and CYC (69) and are captured as a set of instructions in a knowledge base. Later work (70) discussed controlled natural language as a way to repair missing information through explicit clarification. Nyga et al. (71) used a similar probabilistic model to utilize relational knowledge to fill in gaps for aspects of the language missing from the workspace.
Raman et al. (72) and Lignos et al. (47) grounded high-level natural language tasks to linear temporal logic (LTL) (38) formulas by using part-of-speech tagging and parsing to create the syntactic structure. VerbNet (46) is then used to find the sense of the verb and assign a set of LTL formulas as the semantics. In that work, the mapping of verb senses to LTL is done manually; in other work (73,74), semantic mappings are learned using the distributed correspondence graph framework (75);Figure 1ishows an image from this work.
Siskind (76) presented another framework for formally reasoning about time and state changes with manually defined verb meanings. The approach allowed a robot to identify objects and generate actions by defining a formal framework for objects and contact. The work was based on force dynamics and event logic, a set of logical operators about time.
3.2. Learning MethodsThis section covers work on learning models of language meanings from large data sets. The task is to learn a mapping between natural language and symbols in a formal language. In some approaches, the symbols are given. In others, symbols are created as these groundings are learned; these methods are robust to a wide variety of language produced by untrained users but offer few guarantees on performance and correctness.
Learning-based approaches use a wide variety of data sets, tasks, and formats for training. Data sets typically consist of natural language paired with some form of sensor-based context information about the physical environment. An annotated symbolic representation is often also provided. The form of sensor data varies; raw perceptual input, such as joint angles, is often too low level, but higher-level representations depend on the specific approach.
Table 2lists some of the common data sets currently used in language grounding and robotics along with the type of sensor, language, and annotation data.
Data sets used in language grounding and roboticsWe accompanyTable 2with a brief example of applying a data set for a robotic task. The MARCO data set (66) of navigation instructions is the most widely used of the existing data sets (14,29,66,77,78). Beyond being one of the earliest available data sets in this space, its wide uptake is partly because it contains not only route directions but a complete simulation environment in which to navigate. Thus, potential users of the data set do not need to provide their own robot or handle potentially different sensing or actuation capabilities. Instead, language-learning approaches can be directly compared with previous approaches to the same problem by using the natural language instructions in MARCO, then testing in the same simulated environment.
For example, 10 years after the original work used a handcrafted grammar to explicitly model language (66), Mei et al. (79) used a long short-term memory recurrent neural network (LSTM-RNN) to learn to follow directions. This work estimated action sequences from natural language directions, performing end-to-end learning directly from raw data consisting of tuples of natural language instructions, perceived world state, and actions. The LSTM-RNN encodes the navigational instruction sequence and decodes to action sequences, incorporating the observed context (world state) as an extra connection in the decoder step.
The challenge in using any of these data sets is the mismatch between the data provided and the actual data that will be encountered in a real robotic task. The robot in a task may have different sensors, actuators, and representations than the one used in the task. For example, the MARCO data set uses butterflies as a landmark object; most real environments do not have these butterflies but do have other landmarks that may not appear in MARCO. Learning more general concepts such as “landmarks” is an important open question for future work.
A key question for data-based methods is determining a space of possible meanings for words: Into what domain might language be grounded? Domains may consist of specific objects or areas in the environment, perceptual characteristics, robot actions, or combinations thereof. The meaning of language is often grounded into predefined formalisms, which maps well to existing work in formal semantics (20). However, in work more oriented toward machine learning, there is a trend toward systems that learn the representation space itself from data, leading to systems that do not need a designer to prespecify a fully populated set of symbols and allowing robots to adapt to unexpected input. For example, Matuszek et al. (13) and Pillai & Matuszek (80) showed that symbols for shape, color, and object type can be learned from perceptual data, enabling the robot to create new symbols based on its perceptual experience, while Richards & Matuszek (81) extended that work to creating symbols that are not category limited.
We divide the following approaches into those that use primarily predefined languages (Section 3.2.2), those that are more concerned with discovering the domain (Section 3.2.3), and recent work on using deep neural networks for language understanding (Section 3.2.4). In practice, work in this area falls along a spectrum, ranging from formal-methods approaches that use completely manually defined word meanings (66), to learning mappings between words and a prespecified formal language (10,73,82), to learning new symbols from data while specifying perceptually motivated features (83), to learning new features from data as well as a mapping between word meanings and those features (13).
Mapping to predefined symbolic structures has a natural analog in machine translation research. In machine translation, the goal is to translate a sentence from one language to another language (for example, “pick up the block” in English to “podnieść blok” in Polish). Many approaches take as input a parallel corpus of sentences in the two natural languages and then learn a mapping between the languages. When applied to robotics, the input language is a natural language, and the output is a formal representation language that the robot can act on. The challenge is then to specify an appropriate formal robotic language and acquire a data set or parallel corpus with which to train the model.
This approach has been applied to a variety of domains, such as enabling a robot to learn to interpret natural language directions from pairs of directions and programs that follow those directions (10,66,77). The same approach can be used for the inverse problem of generating natural language descriptions of formally represented events, such as RoboCup soccer games (84). MacGlashan et al. (85) showed that a robot can learn to map to a predefined space of symbolic reward functions using the classic IBM Model 2 machine translation approach (86); once the reward function has been inferred, the robot finds a plan that maximizes the reward, even in environments with unexpected obstacles. Misra et al. (87) learned to map between words and a predefined symbolic planning space using a graphical modeling approach, interpreting commands such as “turn off the stove.”Other approaches use semantic parsing to automatically extract a formal representation of word meanings in some formal robot language. These systems vary in terms of the formal language used. For example, Matuszek et al. (82) created a system that learns to parse natural language directions into Robot Control Language (RCL), a control language for movement. This work could learn programmatic structures in language such as loops (e.g., “drive until you reach the end of the hallway”). Alternatively, Artzi & Zettlemoyer (29) created a system for learning semantic parses for mapping instructions to actions in order to follow natural language route instructions, while Thomason et al. (88) used an approach that learned semantic parse information and grounded word meanings from dialogue interactions with users. Fasola & Matarić (89) used a probabilistic approach to learn mappings between commands and a space of actions of service robots, including models for spatial prepositions. Boteanu et al. (73,74), Brooks et al. (90), and Arumugam et al. (91) grounded language to objects and specifications expressed in LTL. A key difference in all of these approaches is the formal language chosen to represent the meaning of the human language; in many cases, the formal language can represent only a subset of the meanings possible in natural language.
We draw a distinction between learning to map between predefined symbol spaces and approaches that extend the space of symbols that natural language may be grounded into. We emphasize that this is a spectrum; all learning approaches rely to a greater or lesser extent on some predefined structure. Less prespecification means the system is more general and can be extended to unexpected tasks and environments but also increases the difficulty of the learning problem. Substantial current effort is focused on learning from very little prespecified data.
The Generalized Grounding Graph (G3) framework (83) was introduced to interpret natural language commands given to a robotic forklift, as well as to interpret route instructions for a wheelchair (14) and a micro air vehicle (92). It uses a graphical model framework to represent the compositional structure of language, so that the framework can map between words in language and specific groundings (objects, places, paths, and events) in the physical world. It learns feature weights in a prespecified feature space to approximate a function for mapping between words in language and aspects of the world. This work has been extended to enable robots to ask natural language questions that clarify ambiguous commands (78,93) and to enable robots to ask for help (94). It has also been extended to create an efficient interface for interpreting grounded language by mapping to planning formalisms (75), an approach that dramatically increases the speed with which words can be interpreted by the robot. Building on this framework, Paul et al. (95) created a system that learns to interpret subsets of objects, such as “the middle block in the row of five blocks.”Other approaches do not require features to be prespecified but do encode a space of possible features as well as data sources from which features are derived. Roy & Pentland (96) created a system for learning nouns and adjectives from video of objects paired with infant-directed speech. It learned to segment audio and map phonemes to perceptual features without a predefined symbol system. Matuszek et al. (13) created a system for learning word meanings for words by automatically creating new features for visual object attributes, while Pillai & Matuszek (80) learned to select negative examples for grounded language learning. Guadarrama et al. (97) created a system for interpreting open-vocabulary object descriptions and mapping them to bounding boxes in images, leveraging large online data sets combined with a model to learn how to use information from each data set. Blukis et al. (98) developed a method that learns to create a semantic map of the environment by projecting between the camera frame and a global reference frame. These approaches represent emerging steps toward an end-to-end learning framework from language to low-level robot control.
Modern deep learning–based approaches of convolutional neural networks, recurrent neural networks, and deep Q-networks led to successes in computer vision, machine translation, and reinforcement learning. Using neural networks or a connectionist architecture is not novel. Older neural network–based approaches (e.g.,99,100) learned robot behavior from demonstrations and mapped language to these behaviors. Roy & Pentland (96) used recurrent neural networks to learn word boundaries by phoneme detection directly from speech signals. However, the amount of data being used and represented in modern deep learning methods is much larger in scale and allows for end-to-end learning. These novel deep approaches were applied to solve problems of language grounding (e.g.,79,101). In this article, we do not survey these methods in great detail, but we do provide a short introduction to the types of problems that have been tackled with deep learning–based approaches. We split this discussion based on the problems addressed by these methods.
Some of the earliest progress was made in the area of instruction following (79,91). This is a supervised problem where an agent performs a sequence of actions in response to a natural language command. In this problem setup, a common theme is to treat a language command and a sequence of actions performed by the agent as a machine translation problem using recurrent neural network–based sequence-to-sequence approaches (79). Others have abstracted the problem to learn the grounding from natural language to subgoals or goals (102,103). These methods have been implemented in robots only when the abstract fixed grounding symbols have been provided (91).
Some approaches try to reduce the amount of supervision by converting this instruction-following problem into a reinforcement learning problem. This was first done with classical policy gradient methods by Branavan et al. (12); more recently, it has been applied to richer environments with visual inputs (104–106). A common strategy is to model the agent and its environment as a Markov decision process and encode the instruction given to the agent as the state of the environment. Such agents have been able to answer questions about the properties of objects or navigate to objects in simulation. This approach is difficult to implement in a physical robot given the number of episodes required to learn behaviors.
Grounding or captioning objects within images to their names is an active area of research within deep learning. Initially, this work used classifiers to recognize an object class within an image (107). It then progressed to captioning images densely, that is, recognizing all objects within an image (108,109). A general approach, first described by Karpathy & Fei-Fei (109), is to align vectorized object representations within the image with the vectorized representations of sentences used to describe the objects in the image. These approaches are capable of labeling activities being performed by the objects of interest and also allow retrieval of images described by natural language (108). They have been implemented in physical robots in an object retrieval setting by training the robot on simulated images (110,111).
Blukis et al. (98) developed a system that learned to map between navigation instructions and low-level control actions, mediated by the robot's sensor input and control actions. This work aimed to perform end-to-end learning from language to control actions and has since been demonstrated in physical robots.
3.3. Approaches Centered on Human–Robot InteractionThe final broad category of work we consider is that which lies primarily in the area of HRI. While work in the previous sections is grouped by learning and representation models, here we describe how NLP research supports and is supported by robots that interact directly with people. It is often these approaches that create the most robust behaviors and end-to-end systems, drawing on insights from learning and logic-based methods.
We discuss language-based HRI efforts divided broadly by tasks, considered on a spectrum (seeFigure 4). On one end, language provides a natural supporting mechanism for robot learning (Section 3.3.1). In this area, language is used as a tool to help robots learn other tasks. On the other end, robots provide an ideal testbed for learning to understand physically situated language; here, the robot is a platform for learning grounded language. This subtopic is substantial and has been covered in Section 3.2. Tied to both areas are efforts whose primary goal is the development of systems that use language in order to support robust HRI (Section 3.3.2).
Robots that learn have the potential to automatically adapt to their environment and achieve more robust behavior. In this section, we describe how language technology can enable more efficient and effective robot learning, especially from human teachers. Natural language provides a rich, accessible mechanism for teaching robots while still being grounded in the physical world. The vast body of literature on human learning provides questions about learning modalities, information presentation, reward functions, and interaction-based learning. We describe current work on developing robot systems that learn about the world from natural language inputs, including efforts on learning from demonstration (LfD), learning reward functions from language, active learning, and learning how to elicit instructional language.
When learning physical concepts like object characteristics or actions, the physical referent must be linked to linguistic structures. This is seen both explicitly, as in referring expressions (e.g., “this is a yellow block”), and implicitly, as when connections are learned from the coexistence of words and percepts during training. Exploring this connection between linguistic references and their grounded referents is the basis of substantial work on LfD, in which demonstrations connect the learning concepts and the language used.
In LfD, language is used as a learning signal to improve robot learning and capabilities. Steels & Kaplan (112) used language and camera percepts to learn novel instance-based objects and their associations with words. Billard et al. (99) used LfD to ground language with a constrained vocabulary to sequences of actions demonstrated by the teacher. Chao et al. (113) used LfD to ground concepts for goal learning, where the concepts are discrete, grounded percepts based in shared sensory elements with human explanations. Concepts are denoted in words to human participants, but language is not part of the learning problem; word meanings are provided to the system by the designer. Krening et al. (114) used object-focused advice provided by people to improve the learning speed of an agent. Language can also be used to describe actions rather than perceived objects, as in programming by demonstration, in which demonstrations of actions are paired with natural language commands (115). Programming by demonstration can also rely on more complex semantic parsing, as in the approach developed by Artzi et al. (116), in which language is interpreted in the context provided by robot state. In all of these papers, humans use language to provide information, advice, or warnings to the robot to improve task performance.
Language can be used to provide explicit feedback to a learning system. The mechanism for learning from that feedback can be treated as a learning problem itself. In this framework, language is learned jointly with policies rather than jointly with direct observations, allowing learning that is less situation specific (85). This approach can allow a nonspecialist to give an agent explicit reward signals (117) or can model implicit feedback strategies inherent in human teaching (118,119).
Robots asking questions about their environment is a form of active learning in which the learning agent partially or fully selects data points to label. Asking questions that correspond to a person's natural teaching behavior (120) is balanced with selecting data that optimize learning, as queries to a user are a sharply limited resource (121). In general, incorporating active learning makes learning more efficient and makes it possible to learn from fewer data points (122,123). This form of learning can be implemented in a domain-independent way, as done by Knox et al. (124), and can improve efficiency on learning tasks, including both explicit language grounding (125) and more general robotics problems, such as learning conceptual symbols (126), spatial concepts (127), or task constraints (128).
Another topic in learning from language provided by nonspecialists is how to correctly elicit information and demonstrations from people. Chao & Thomaz (129) explored conducting dialogue correctly, with appropriate multimodal timing, turn-taking, and responsiveness behavior (130). Learning from nonspecialists also means figuring out what questions to ask; Cakmak & Thomaz (131) studied how humans ask questions and designed an approach to asking appropriately targeted questions for LfD, while Pillai & Matuszek (80) demonstrated a method for automatically selecting negative examples in order to train classifiers for positively labeled grounded terms.
HRI is one of the most active areas for grounded language research. Language provides a natural mechanism for interacting with physical agents in order to direct their actions, learn about the environment, and improve interactions. At the same time, interacting with people provides a rich source of information and training data that robots can learn from in order to improve their capabilities. Language-based HRI is a broad, active field of study. In this section, we provide an overview of some of the categories of current research on HRI and language.
Childhood education is a significant area of research for HRI studies (132), both because there is a chronic shortage of personnel in education and child care and because increasing the role of technology in childhood education is a critical part of attracting a larger and more diverse population into STEM fields. Research in this area focuses largely on the role of interactive play in child development. This play can take the form of acting out stories between children and robots (133), assisting with language development (134–137), or serving as intelligent tutoring systems (138,139).
Language in HRI is often paired with other interaction modalities. Modalities such as gesture and gaze direction affect everything from deictic (referential or attention-drawing) interactions to what role a robot may play in a setting (140). There is a growing body of work in which language is incorporated into multimodal HRIs (141). Matuszek et al. (32) used a combination of language and unconstrained, natural human gestures to drive deictic interactions when using language to teach a robot about objects, while Huang et al. (92) used modeling to evaluate robots’ use of gesture. In the inverse direction, Pejsa et al. (142) used people's speech, gaze, and gestures to learn a multimodal interaction model, which was then used to generate natural behaviors for a narrating robot.
Another key area of HRI research is work on assistive robotics, in which robots perform tasks designed to support persons with physical or cognitive disabilities. This support can take many forms; with respect to language, social and cognitive support is most common. Socially assistive robot systems have been used to engage elderly users in physical exercise (143,144), incorporating language pragmatics and anaphor resolution (145,146) as well as verbal feedback. Verbal robots have also been explored in the context of autism support (147) and tutoring for deaf infants (148).
4. CLASSIFICATIONS BY PROBLEM ADDRESSEDMost of the above approaches can be applied to more than one communication task. Here we review those tasks, divided into three sections: understanding communications from a human to a robot (the largest body of work), generating linguistic communication from a robot to a human, and two-way systems that endeavor to both understand and generate language.
4.1. Human-to-Robot CommunicationHuman-to-robot communication is the problem of enabling robots to interpret natural language directives given by people. Understanding a person's language requires mapping between words and actions or referents in the physical world. Two specific subproblems include understanding commands and information given to the robot by a person.
Command understanding is the problem of mapping between language and physical actions on the part of the robot. One early and widely considered domain is route direction following, where a mobile robot must interpret instructions on how to move through an environment. MacMahon (66) created a large data set of route directions in simulation, which has been used in a number of papers (10,29). Kollar et al. (14) used a statistical approach to interpret instructions for a robotic wheelchair. Shimizu & Haas (149) used a conditional random field approach to learn word meanings, and Matuszek et al. (77) used a machine translation approach to learn to follow instructions in real-world environments, including counting and procedural language such as “the third door” or “until the end of the hall.” Robotic platforms used for this problem include a robotic wheelchair (14,66), robotic unmanned aerial vehicles (92), and mobile robots (150). Understanding navigational commands remains a significant and ongoing area of research (151).
A second class of problems is interpreting natural language commands for manipulator robots. This problem has been studied in the subdomains of interpreting textual recipes (152,153), following instructions for a robotic forklift (83), and interpreting instructions to a tabletop arm (32,67) and in Baxter robots (73,74). Such language may refer only to the robot's motion; for example, Correa et al. (154) created a robotic forklift with a multimodal user interface that interpreted shouted commands such as “stop!” However, since manipulators manipulate things in the world at least some of the time, this class of commands is frequently blended with understanding language about objects.
Another frequently studied task is understanding instructions in cooking, particularly focusing on following the semiconstrained language of recipes. Beetz et al. (153) used a reasoning system to interpret recipes and cook pancakes. Tasse & Smith (155) created a data set of recipes mapped to a formal symbolic representation, while Kiddon et al. (156) created an unsupervised hard expectation–maximization approach to automatically map recipes to sequenced action graphs; neither system used robots. Bollini et al. (152) created a system for interpreting recipes but did not ground ingredients into perception. Although the language of recipes is constrained, understanding them remains a challenging problem, in part because ingredients are combined into new things that do not exist at the time of original interpretation—for example, flour, eggs, water, and sugar are transformed into a batter, which is then transformed into a quick bread. Interpreting forward-looking language that maps to objects that do not yet exist is a difficult problem. Similarly, instructions often require the robot to detect certain perceptual properties, as in “cook until the cornbread is brown.” Correctly detecting these properties requires advances in perception combined with language to create or select a visual detector to identify when this condition has been met.
A second element of language interpretation is enabling robots to use language to improve their knowledge of the world. Compared with instruction following, this topic is a less studied area, but there is nonetheless a rich array of approaches. Cantrell et al. (157) created a system that updates its planning model based on human instructions, while the system of Walter et al. (158) incorporates information from language into a semantic map of the environment. Pronobis & Jensfelt (159) described a multimodal probabilistic framework that incorporates semantic information from a wide variety of modalities, including perceived objects and places as well as human input.
We briefly discuss two specific important subproblems in human-to-robot communication: how robots can resolve references to and understand descriptions of objects, and understanding descriptions involving spatial relationships. One of the major areas in which robots have the potential to help people is in interacting with objects in the environment, meaning it is critical to be able to learn about and understand physical references, both spatial (as in “the door near the elevators”) and descriptive (as in “the yellow one between the two toys” or, more abstractly, “a nice view”).
Robots may need to retrieve, manipulate, avoid, or otherwise be aware of objects being referred to in language. Language about objects and landmarks in the world can be broken down by level of specificity; we roughly categorize language at these different levels of abstraction as (a) general language about object characteristics, such as color, shape, or size (32,160,161); (b) descriptions of objects at the type, or category membership, level, which encompasses approaches that tie language into object recognition (80,96,162,163); and (c) language about particular instances of objects, such as “my mug” (14,62,83,112). These categories often overlap. For example, the first step for recognizing an instance is often finding all objects in that category, or object types might be further differentiated by their attributes, as in “the yellow block.”Another issue is interpreting complex descriptions. For example, one route direction corpus contains the instruction “you will see a nice view,” referring to a view out of a set of windows the robot would pass. This expression requires the robot to make a subjective judgment about the world. A corpus of object descriptions contains the phrase “a small pyramid like the pharaohs lived in” (32), which requires differentiating direct physical descriptions from background knowledge. In addition, it is not always clear what defines an object. A bottle consists of a bottle and a cap, and a person referencing “the bottle” may mean both, or they may say, “Grab the bottle and then turn the cap,” referring to them separately. For assembly tasks, a part such as a screw and a table leg may combine to form a completed assembly, the table (94,164). Grounding these sorts of expressions is an open problem.
Understanding natural language expressions that denote particular things in the robot's environment is another key subproblem. Referring expressions may occur in commands (e.g., “go through the door near the elevators,” in which the robot must identify the referenced door) as well as manipulation instructions (e.g., “pick up the green pepper”) (62,83). Chai et al. (165) created a system that interprets multimodal referring expressions using a graph-based approach. Matuszek et al. (32) and Whitney et al. (166) merged information from language and gesture to interpret multimodal referring expressions in real time using a filtering approach and a joint classification approach, respectively; an image from Matuszek et al. (32) is shown inFigure 1d. Golland et al. (167) generated spatial descriptions using game theory to generate human-interpretable referring expressions in a virtual environment.
Interpreting spatial relationships is a well-known, complex problem in NLP. For route instructions, the language may take the form of “the door near the elevators” or “past the kitchen”; for object descriptions, it may take the form of “at the top left corner.” Understanding these instructions frequently requires not only referring-expression resolution to understand phrases referring to landmarks but also pragmatic disambiguation of possible meanings. Spatial prepositions are frequently used to refer to objects, places, or paths in the physical world. Spatial prepositions are a closed-class part of speech; a typical language has only a few, and new ones are rarely added. Cognitive semantics has focused on the structure of spatial language and how humans use it, especially the argument structure as well as semantic features that allow it to be interpreted (36,168). Some work has focused specifically on spatial prepositions (11,127,169,170). This problem also arises in the context of referring-expression resolution, since expressions such as “near” or “between” require identifying a place or an object from distractors.
4.2. Robot-to-Human CommunicationIn the context of natural language user interfaces, people frequently expect spoken responses when they speak to a system such as a robot. Language is an obvious way to engage in active disambiguation, convey information, and provide context. Researchers have studied the problem of enabling a robot to produce natural language to answer questions, ask for help, or provide instructions. This problem is the inverse of language understanding: The robot desires to communicate something to the person and must find words that convey its ideas. Subproblems include robots instructing people, robots asking questions, and robots informing people.
Often a robot might use language to try to get a person to do something, typically by asking for help or asking the person to carry out an action. The most basic approach to language generation is template-based or scripted approaches, in which a designer encodes the words the robot will say. For example, Fasola & Matarić (146) used templates to generate language to motivate physical exercise for older adults (as shown inFigure 1h). This approach is straightforward and can result in sophisticated sentences but is limited in its adaptability to novel environments and situations. Other approaches focus on enabling a robot to adaptively generate sentences based on the context. Knepper et al. (164) generated natural language requests for help in assembling Ikea furniture from untrained, distracted users. CoBots navigate an office environment delivering objects and ask for navigation help using a human-centered planner to determine whom to ask for assistance (171).
A second sort of instruction is actively using language to induce a person to provide additional information, for example, by asking a question. Deits et al. (93) presented an algorithm to generate targeted questions based on information theory to reduce confusion. Rosenthal & Veloso (172) modeled humans as information providers, using a partially observable Markov decision process to ask questions when the robot encountered problems. Thomason et al. (173) created a system for opportunistically collecting information from someone about objects in its environment (in which a robot asks about objects near a person, including questions irrelevant to the immediate task) and learning about objects from attributes (174) (as shown inFigure 1c). Pillai et al. (125), Cakmak & Thomaz (131), and others have used active learning to select focused questions that allow the robot to efficiently collect information. All of these approaches use statistical frameworks to generate instructions or queries given the robot's current physical context.
In addition to trying to instruct people with language, a robot may also need to inform people about aspects of the world. For example, Chen et al. (84) created a system that learns to generate natural language descriptions of RoboCup soccer games by probabilistically mapping between word meanings and game events. Mutlu et al. (175) created a storytelling robot that uses language as well as gaze to engage a human listener. Cascianelli et al. (176) created a system that enables a robot to learn to describe events in a video stream and released a data set for service robotics applications. All of these applications require the robot to communicate with a person about aspects of the environment.
For the same reasons that a robot may need to understand references to things in its environment (see Section 4.1.2.1), a robot may need to generate referring expressions about objects, landmarks, or people. Dale & Reiter (177) carried out seminal work on generating referring expressions for definite noun phrases referring to physical objects, such as “the red cup,” following Gricean maxims of quantity and quality of the communication (178) and focusing on computational cost. This approach assumes a symbolic representation of context, rather than grounding to perception. Golland et al. (167) generated spatial descriptions using game theory to produce referring expressions in a virtual environment that are interpretable by a human partner. Mitchell et al. (179) generated expressions that refer to visible objects that a robot might observe with its camera. Tellex et al. (94) provided an inverse-semantics algorithm for generating requests for help, including expressions such as “the black leg on the white table” (shown inFigure 1a). Fang et al. (180) created a system for collaborative referring-expression generation using a graph-based approach that changes the generated language based on human feedback, while Zender et al. (181) created a system for enabling a mobile robot to generate natural language referring expressions for objects in the environment and to resolve expressions, using context to determine how specific or general to make the resolution. From a robotics perspective, these examples represent different contexts in which a physical agent may use language production to improve its ability to accomplish real-world tasks or goals.
4.3. Two-Way CommunicationTwo-way communication involves enabling a collaborative interaction between a human and a robot, either asynchronously or in dialogue. Such a robot must both interpret a person's communicative acts and generate communicative actions of its own. Two-way communication requires more than simply combining language understanding and generation; the robot must reason about uncertainty in its own percepts, retain conversational state, react quickly to a person's input, and work toward a communicative collaboration. Partly as a result of these challenges, much work has focused on issues that arise from building robotic systems that engage in dialogue with a user and the associated design questions that arise. A variety of end-to-end robotic systems have been created that use language. These systems typically involve the integration of many software and hardware components in order to create an end-to-end user interaction. The focus is often on multimodal communication, where language constitutes one communication mode in the overall interaction.
For example, Bohus & Horvitz (182) created a computational framework for turn-taking that allows an embodied conversational agent to take and release the conversational floor using gaze, gesture, and speech. Some of these systems communicate by understanding language, performing actions, and seeking help when problems are encountered. Matuszek et al. (32) created a system for learning from unscripted deictic gesture combined with language in order to perform manipulations. Okuno et al. (183) created a robot for giving route directions by integrating language utterances, gestures, and timing. Fasola & Matarić (146) created a socially assistive robot system designed to engage elderly users in physical exercise. Veloso et al. (184) created the CoBots, mobile robots that engage in tasks in an office environment, such as fetching objects. Marge et al. (185) created a heads-up, hands-free approach for controlling a pack-bot as it moved on the ground. Tse & Campbell (186) created a system that incorporates and communicates probabilistic information about the environment. A more direct approach is to learn the spatial semantics of actions directly from language (187) (shown inFigure 1b). The CoBot systems learned to follow commands such as “take me to the meeting room,” engaging in dialogue with humans in their environment to improve their abilities (188) (shown inFigure 1f).
While these robots understand language, the robot-to-human side of communication can take a form other than, or in addition to, speech. This multimodality reflects the multimodal nature of interagent communication: Even when talking, humans expect to be able to use gesture, gaze, and body language, as well as utterance timing and even prosody (voice tone and inflection). Language-using robots must therefore be aware of these expectations and work to address or mitigate them; failing to do so runs the risk of frustrating users when attempting to communicate.
5. CONCLUSION AND OPEN QUESTIONSLanguage-using robots require models that span all areas of robotics, from perception to planning to action. Researchers from diverse communities have contributed to ongoing work in this exciting area, and much remains to be done. In this article, we have reviewed methods for robots that use language. We covered technical approaches, ranging from formal methods to machine learning to HRI approaches. We discussed problems to solve for robotic language use, including learning from and receiving information from people, asking questions, and giving people instructions. We presented some of the most immediately relevant NLP problems, such as referring-expression resolution. Additionally, we briefly reviewed work in related areas, including linguistics, cognitive science, computational linguistics, vision and language, ontologies and formal representations, and nonverbal communication.
Research in formal methods has pointed toward mechanisms for capturing complex linguistic phenomena such as anaphora resolution, interpreting commands about ongoing action, and abstract objects. However, statistical methods often use simpler representations focused on concrete noun phrases and commands for ease of learning. As more sophisticated formal models mature, statistical methods will enable learning of formal-methods-based representations, combining benefits of robustness with more capable and complex language understanding. At the same time, advances in deep learning have enabled approaches that can learn from less data with end-to-end supervision. We expect that deep learning applied to robotic language use will build on existing work to learn with less and less supervision over time. We see opportunities for sophisticated semantic structures from formal methods combined with learning approaches from deep learning to create a new generation of language-using robots capable of robustly interpreting sophisticated commands produced by untrained users.
The power and challenge of language lie in its ability to construct arbitrarily fine-grained and specific sentences that apply to all parts of the robot and its environment. As a result, robust language-using robots must integrate language with all parts of a robotic system, a formidable task. As we move toward language-using collaborative robots, we need more robust models for the entire planning and perceptual stack of the robot in order to integrate with natural language requests, questions people might pose, learning from language, and the generation of appropriate language and dialogue by the robot. Similarly, the robot must combine verbal and nonverbal modalities in interactive systems in order to fully understand how people interact and to detect and recover from errors. Although daunting, the scale and complexity of the problems described in this survey are indicative of the potential power in bringing language into robotics and in building flexible, interactive, and robust systems by bringing the fields together.
disclosure statementThe authors are not aware of any affiliations, memberships, funding, or financial holdings that might be perceived as affecting the objectivity of this review.
literature citedDynamic Walking: Toward Agile and Efficient Bipedal RobotsToward Robotic ManipulationWhat Is Robotics? Why Do We Need It and How Can We Get It?Grounded CognitionMethods for Robot Behavior Adaptation for Cognitive NeurorehabilitationEscaping Oz: Autonomy in Socially Assistive RoboticsRobots for Use in Autism ResearchEscaping Oz: Autonomy in Socially Assistive RoboticsThe Grammar of Degree: Gradability Across LanguagesDistributivity in Formal SemanticsSemantic Anomaly, Pragmatic Infelicity, and UngrammaticalityReflexives and ReflexivitySign Language and the Foundations of AnaphoraExistential Sentences Crosslinguistically: Variations in Form and MeaningBringing Machine Learning and Compositional Semantics TogetherApproximately Optimal Mechanism DesignBringing Machine Learning and Compositional Semantics TogetherBringing Machine Learning and Compositional Semantics TogetherSentiment AnalysisMore Than Words? Computer-Aided Text Analysis in Organizational Behavior and Psychology ResearchMachine Translation: Mining Text for Social TheoryBringing Machine Learning and Compositional Semantics TogetherMethods for Robot Behavior Adaptation for Cognitive NeurorehabilitationCurrent Advances in Neural NetworksNeurocomputational Models of Language ProcessingBalancing Flexibility and Interference in Working MemoryQuantitative Molecular Positron Emission Tomography Imaging Using Advanced Deep Learning TechniquesExtension of Plant Phenotypes by the Foliar MicrobiomeEmerging Applications of Machine Learning in Food SafetySyntactic Structure from Deep LearningIdentifying Regulatory Elements via Deep LearningRepresentation Learning: A Statistical PerspectiveMachine Learning for Fluid MechanicsDeep Learning: The Good, the Bad, and the UglyDeep Learning and Its Application to LHC PhysicsDeep Learning in Biomedical Data ScienceComputational Neuroscience: Mathematical and Statistical PerspectivesDeep Neural Networks: A New Framework for Modeling Biological Vision and Brain Information ProcessingON KNOWING A WORDSynthesis for Robots: Guarantees and Feedback for Robot BehaviorLightness Perception in Complex ScenesAdvances in Inference and Representation for Simultaneous Localization and MappingInvariant Recognition Shapes Neural Representations of Visual InputShape from Contour: Computation and RepresentationBig Data Approaches for Modeling Response and Resistance to Cancer DrugsComputational Neuroscience: Mathematical and Statistical PerspectivesVisual Object Recognition: Do We (Finally) Know More Now Than We Did?Deep Neural Networks: A New Framework for Modeling Biological Vision and Brain Information ProcessingWhat Is Robotics? Why Do We Need It and How Can We Get It?Advances in Inference and Representation for Simultaneous Localization and MappingMachine Learning for Structural MaterialsRepresentation Learning: A Statistical PerspectiveMachine Learning for Fluid MechanicsDeep Learning: The Good, the Bad, and the UglyMaterial PerceptionConcepts and Compositionality: In Search of the Brain's Language of ThoughtRecent Advances in Robot Learning from DemonstrationRecent Advances in Robot Learning from DemonstrationRecent Advances in Robot Learning from DemonstrationEscaping Oz: Autonomy in Socially Assistive RoboticsEscaping Oz: Autonomy in Socially Assistive RoboticsEscaping Oz: Autonomy in Socially Assistive RoboticsWhat Is Robotics? Why Do We Need It and How Can We Get It?Escaping Oz: Autonomy in Socially Assistive RoboticsAutonomy in Rehabilitation Robotics: An IntersectionLANGUAGE AND SPACEThe Pervasive Role of Pragmatics in Early LanguageConcepts and Compositionality: In Search of the Brain's Language of ThoughtDeep Learning: The Good, the Bad, and the UglyMore AR articles citing this referenceTERMS AND DEFINITIONSlanguage that refers to or is interpreted in reference to the physical worldinterpreting a natural language utterance in terms of the physical state of the robot and the environmenta formalism for expressing computation in terms of function arguments and applicationcomputational techniques for transforming human languages such as English into machine-usable structuresa structure that represents the syntactic decomposition of an utterance in the form of a rooted treenatural language expressions that uniquely denote objects, areas, or people to which the speaker is referringlogic that includes temporal operators; roughly speaking, the truth value of a formula is evaluated over sequences of states labeled with the truth values of the propositionsFigure 1Robots used for language-based interactions. (a) Using language to ask for help with a shared task. Panel adapted from Reference94. (b) A Baxter robot learning via dialogue, demonstrations, and performing actions in the world. Panel adapted from Reference187with permission from IJCAI (https://ijcai.org). (c) A Jaco arm identifying objects from attributes (here “silver, round, and empty”). Panel adapted from Reference174with permission from IJCAI (https://ijcai.org). (d) A Gambit manipulator following multimodal pick-and-place instructions (32). (e) A Pioneer AT robot achieving goals specified as “go to the break room and report the location of the blue box.” © 2009 IEEE. Reprinted, with permission, from Reference31. (f) A CoBot learning to follow commands such as “take me to the meeting room.” © 2013 IEEE. Reprinted, with permission, from Reference188. (g) TUM-Rosie making pancakes by downloading recipes from wikiHow. © 2012 IEEE. Reprinted, with permission, from Reference63. (h) A socially assistive robot helping elderly users in performing physical exercises. © 2012 IEEE. Reprinted, with permission, from Reference146. (i) A Baxter robot performing a sorting task synthesized from natural language (73).
Figure Locations...ranging from manipulators to mobile robots to aerial robots. Figure 1 shows some examples. ...
...creates goals for a planner, and executes the instructions, as shown in Figure 1e (31)....
...the robot can infer a motor action that causes a source object to end up in a goal location. Figure 1g shows an image from this work....
...semantic mappings are learned using the distributed correspondence graph framework (75); Figure 1i shows an image from this work....
...respectively; an image from Matuszek et al. (32) is shown in Figure 1d....
...Fasola & Matarić (146) used templates to generate language to motivate physical exercise for older adults (as shown in Figure 1h)....
...including questions irrelevant to the immediate task) and learning about objects from attributes (174) (as shown in Figure 1c)....
...including expressions such as “the black leg on the white table” (shown in Figure 1a)....
...A more direct approach is to learn the spatial semantics of actions directly from language (187) (shown in Figure 1b)....
...The CoBot systems learned to follow commands such as “take me to the meeting room,” engaging in dialogue with humans in their environment to improve their abilities (188) (shown in Figure 1f)....
Figure 2System diagram showing language input and output integrated into a robotic system. Many approaches include only a subset of the modules. Grayed-out modules are relevant to language interpretation but are not reviewed in this article.
Figure Locations...Figure 2 shows a system flow diagram for a language-using robot....
Figure 3Grammar and parse tree for the English sentence “Grab an apple.” (a) Context-free grammar for a small subset of English. (b) The structure defining compositional relations among word meanings. Abbreviations: DT, determiner; NN, noun; NP, noun phrase; VB, verb; VP, verb phrase.
Figure Locations...and Figure 3 gives an example CFG for a small subset of English along with an associated parse tree....
...can be extracted with (or from) syntactic structures, such as the example in Figure 3....
Figure 4A categorization of work using language for human–robot interaction (HRI). This visualization spans efforts that use language to support efficient robot learning, efforts to use language in order to maximize the effectiveness of HRIs, and the use of robots as physically situated agents to support language learning.
Figure Locations...We discuss language-based HRI efforts divided broadly by tasks, considered on a spectrum (see Figure 4)....
Table 1Examples of natural language and possible groundingsAbbreviation: RGB-D, red, green, and blue plus depth.
aNatural language that might occur when instructing or informing a robot.
bPossible sensors or actuators providing the physical context.
cThe underlying task or reasoning problem implicitly encoded in the language.
dThe physically situated, or grounded, meaning of the language.
Table 2Data sets used in language grounding and roboticsAbbreviations: CIFF, Cornell Instruction Following Framework; CLEVR, Compositional Language and Elementary Visual Reasoning; EQA, Embodied Question Answering; H2R, Humans to Robots; IQA, Interactive Question Answering; NLVR, Natural Language for Visual Reasoning; R2R, Room-to-Room; SLU, Spatial Language Understanding.
Most Downloaded from this journalLearning-Based Model Predictive Control: Toward Safe Learning in ControlLukas Hewing, Kim P. Wabersich, Marcel Menner, and Melanie N. ZeilingerVol. 3, 2020AbstractRecent successes in the field of machine learning, as well as the availability of increased sensing and computational capabilities in modern control systems, have led to a growing interest in learning and data-driven control techniques. Model predictive ...
Read MoreFigure 1: Robust and stochastic nonparametric estimation techniques. Noisy measurements of the true function (red) are shown as crosses; the nominal function estimate is shown in blue. (a) Illustratio...
Figure 2: Gaussian process–based model predictive control for autonomous racing. (a) Application to a full-sized race car (90), showing the position constraint set (red); the predicted trajectory incl...
Figure 3: (a) The concept of a model predictive safety filter. Based on the current state x, a learning-based controller provides an input , which is processed by the safety filter and applied to the...
Planning and Decision-Making for Autonomous VehiclesWilko Schwarting, Javier Alonso-Mora, Daniela RusVol. 1, 2018AbstractIn this review, we provide an overview of emerging trends and challenges in the field of intelligent and autonomous, or self-driving, vehicles. Recent advances in the field of perception, planning, and decision-making for autonomous vehicles have led to ...
Read MoreFigure 1: Schema of the planning and decision-making components described in this review: fleet management (Section 6); approaches for generating control commands from sensory data, namely traditional...
Factor Graphs: Exploiting Structure in RoboticsFrank DellaertVol. 4, 2021AbstractMany estimation, planning, and optimal control problems in robotics have an optimization problem at their core. In most of these optimization problems, the objective to be maximized or minimized is composed of many different factors or terms that are ...
Read MoreFigure 1: (a) An example of mapping with a 2D lidar as a pose graph optimization problem. (b) An example of visual–inertial navigation in an urban setting.
Figure 2: A tour of factor graphs. Abbreviations: PGO, pose graph optimization; SfM, structure from motion; SLAM, simultaneous localization and mapping.
Figure 3: Factor graph for a larger, simulated SLAM example. Abbreviation: SLAM, simultaneous localization and mapping. Figure adapted with permission from Reference 2.
Figure 4: (a) The measurement Jacobian associated with the problem in Figure 3, which has unknowns. The number of rows, 1,126, is equal to the number of (scalar) measurements. (b) The upper triangul...
Figure 5: Evolution of the Bayes tree for a simple synthetic SLAM example. (a) Four simulation steps from a Manhattan world-simulated sequence. (b) The Bayes trees for these four steps, showing how th...
Figure 6: A nested dissection ordering that leads to a hierarchical decomposition of a SLAM problem, here shown for the Victoria Park data set. The four panels show four different levels of partitioni...
Figure 7: (a) Three robots and a synthetic landmark used in a multirobot mapping experiment. (b) Final results for all three robots at the end of the experiment. Figure adapted with permission from Re...
Figure 8: (a) A map of 10 floors of the Massachusetts Institute of Technology Stata Center created using a real-time visual SLAM system. The data were collected in 14 sessions over a six-month period....
Figure 9: (a) Preintegration on the tangent space of , as done by Forster et al. (30). (b) State-of-the-art VIO results with preintegrated IMU measurements and visual odometry fused within iSAM2, as c...
Figure 10: Factor graphs for legged robots. (a) The Cassie base frame and contact frame. These components are related by forward kinematics. (b) A factor graph fragment showing the forward kinematics ...
Figure 11: (a) An ANYbotics ANYmal quadruped, with relevant coordinate frames. (b) Factor graph showing visual landmarks tightly integrated with the IMU factor from Forster et al. (30), but with an a...
Figure 12: (a) Oily puddle environment in which the ANYbotics ANYmal quadruped was tested. (b) Performance of a preintegrated velocity bias factor. Figure adapted with permission from Reference 36.
Figure 13: Incremental replanning results in iGPMP2. Abbreviation: iGPMP2, Incremental Gaussian Process Motion Planner 2. Figure adapted with permission from Reference 39.
Figure 14: (a) A dynamic factor graph for a two-link manipulator. (b) A kinodynamic motion planning factor graph. To simplify the notation, we use a dynamics factor to represent all the dynamic constr...
Figure 15: A simple example illustrating STEAP with a robot that navigates to a goal while avoiding obstacles. For each step, panel a shows the environment with ground-truth, estimated, and replanned ...
Figure 16: (a) An example of the multiple-hypothesis algorithm MH-iSAM2 (51) running on the Victoria Park data set (21) when random outliers are added to the measurements. iSAM2 cannot deal with these...
Figure 17: (a) SfM pipeline with latent codes as well as pairwise consistency factors . (b) Image-intensity-conditioned depth components for different entries of the latent code. Abbreviation: SfM, s...
Figure 18: (a) A recovered trajectory and (b) a semantically annotated mesh from the Kimera system (54). This system builds on the VIO from Section 4.3 and fuses it with a data-driven semantic segment...
Figure 19: (a) A factor graph to compute SfM is mapped to the tiles in a Graphcore IPU. (b) Computation proceeds by message passing between factors and variables, as worked out by Yedidia et al. (59)....
Autonomy in Surgical RoboticsAleks Attanasio, Bruno Scaglioni, Elena De Momi, Paolo Fiorini, Pietro ValdastriVol. 4, 2021AbstractThis review examines the dichotomy between automatic and autonomous behaviors in surgical robots, maps the possible levels of autonomy of these robots, and describes the primary enabling technologies that are driving research in this field. It is ...
Read MoreFigure 1: Commercially available systems organized by clinical application: (a) CyberKnife M6, Accuray; (b) neuromate, Renishaw; (c) ROSA ONE, Zimmer Biomet; (d) Magellan, Hansen Medical; (e) Monarch,...
Figure 2: The role of a level-1 system in surgery. The surgeon interacts with the robot, which in turn provides the clinician with manual guidance or virtual fixtures. In this case, the control loop i...
Figure 3: Two systems used in orthopedics for joint replacement: (a) the Stryker Mako and (b) the Zimmer Biomet ROSA.
Figure 4: The role of a level-2 system in surgery. The surgeon provides the necessary information for the robot to then accomplish a specific task. During the autonomous execution of this task, contro...
Figure 5: (a) The Smart Tissue Anastomosis Robot (STAR) system. (b) The STAR system's plenoptic camera (to retrieve depth information) and near-infrared camera (to detect hidden structures in the tiss...
Figure 6: The role of a level-3 system in surgery. In contrast to level-2 systems, where the surgeon provides the specifications for a surgical task to be performed, level-3 systems can define the spe...
Figure 7: The role of a level-4 system in surgery. Preoperative and intraoperative information is used to devise an interventional plan comprising a sequence of tasks and then execute it autonomously,...
Figure 8: The stages of autonomous tumor debridement on a phantom, as reported by McKinley et al. (88, 138). (a) A palpation probe scans the tissue to define the tumor boundaries. (b) An incision is p...
Recent Advances in Robot Learning from DemonstrationHarish Ravichandar, Athanasios S. Polydoros, Sonia Chernova, Aude BillardVol. 3, 2020AbstractIn the context of robotics and automation, learning from demonstration (LfD) is the paradigm in which robots acquire new skills by learning to imitate an expert. The choice of LfD over other robot learning methods is compelling when ideal behavior can be ...
Read MoreFigure 1: Consistent growth in the number of publications concerning learning from demonstration over the past decade, as reflected by the trend in the number of search results on Google Scholar that ...
Figure 2: Categorization of learning-from-demonstration methods based on the demonstrations they utilize.
Figure 3: Examples of the three categories of robot demonstrations.
Figure 4: Categorization of learning-from-demonstration methods based on learning outcome.
Figure 5: Categorization of policy-learning methods based on the policy's input space.
Figure 6: Categorization of policy-learning methods based on policy's output space.
Figure 7: Categorization of policy-learning methods based on the mathematical class of the policy.
Figure 8: Categorization of cost- and reward-learning methods.
Figure 9: Categorization of learning-from-demonstration methods that learn task plans.
