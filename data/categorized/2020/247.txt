old id = 2279
Computing Power Can Keep Growing as Moore's Law Winds Down. Here's How
2020
https://singularityhub.com/2020/06/08/computing-power-can-keep-growing-as-moores-law-winds-down-heres-how

Topics AI Biotech Computing Space Energy Future Tech Robotics Science Experts Featured Experts Perspectives Books Events Videos Latest Series Interviews About Singularity About Programs Membership Experts Community Careers Subscribe Welcome! Search.
News and Insights from Singularity Group Search Subscribe to our newsletter Singularity Group Singularity Community Facebook Instagram Twitter Youtube.
st0{fill:#FFFFFF;} .st1{fill:url(#SVGID_1_);} Singularity Hub News and Insights from Singularity Group Topics Experts Events Videos Search singularity group singularity community Facebook Instagram Twitter Youtube.
st0{fill:#FFFFFF;} .st1{fill:url(#SVGID_1_);} Singularity Hub News and Insights from Singularity Group Topics Experts Events Videos Search Computing Power Can Keep Growing as Moore’s Law Winds Down. Here’s How By Moore’s Law is faltering, but that doesn’t mean the end of progress in processing power. R ather than relying on semiconductor physics and silicon-fabrication technology, though, we need to turn to innovations in software, algorithms, and hardware, says a group of leading experts.
Despite powering exponential growth in computing power for nearly half a century, the miniaturization of transistors is about to hit fundamental buffers. Not only do the physics behind these performance gains start to change as we approach atomic levels, but the costs quickly become prohibitive.
A host of alternatives to silicon are waiting in the wings, such as photonics, carbon nanotube transistors , and superconducting circuits, but none are close to being able to replace the technology we’ve become so reliant on. But according to the authors of a new paper in Science , all is not lost, because there is huge scope for performance gains in software, algorithms, and chip architectures.
While Moore’s Law was doubling chip performance every year or two it was easy to get lazy, the authors say. Because regular boosts in speed were guaranteed, software development focused on cutting the time it took to develop an application rather than the time it takes to run that application. Similarly, there was little incentive to develop a chip specialized to a particular task when you knew that in a couple of years a general-purpose chip would outperform it.
Now that those easy gains have gone, though, the authors say it’s going to be increasingly important to refocus computer scientists’ efforts on optimizing all elements of the computing stack for performance.
I t will be hard , and the gains will be uneven and sporadic.
But they also highlight the kind of headroom there is for improvement. In a simple example of a piece of code for matrix multiplication, they show that just switching from the coding language Python to C can generate a 50x speedup, and then optimizing the code to take advantage of chip hardware features provides a 60,000x boost.
For software, many of the gains will come from better tailoring code to take advantage of hardware features; for instance, exploiting the multiple cores on modern chips by running many operations in parallel rather than sequentially. Most modern software also features huge amounts of bloat due to lazily re-purposing code for things it wasn’t built for.
Unfortunately, the authors note that high-performance code tends to be much more complicated to write than slow code, so there needs to be better training for programmers and more productivity tools to assist them.
At the level of algorithms, the path to improvements is less clear, because breakthrough s tend to rely on human ingenuity and tend to be specific to a particular problem. But despite their unpredictability, the potential for enormous gains in efficiency means that averaged over time they can rival those we’ve come to expect from Moore’s Law.
There are some simple wins—for example, updating the model underlying much of today’s algorithmic development, which assumes a processor can only do one operation at a time and that it costs the same to access any part of its memory, which is not true of today’s hardware.
When it comes to chip architecture, the authors say, designers will need to turn their approach on its head.
Moore ’s Law was powered by squeezing more transistors onto a chip, but future gains will come from streamlining by using fewer transistor s and less chip space to carry out hardware functions.
That extra space will make it possible to squeeze more cores onto chips and further boost the number of operations running in parallel. This trend has already been behind much of the performance gain since processor clock speeds stopped rising around 2004, but there’s still lots of scope for further improvements.
Simplifying processors and specializing them to specific applications also holds promise for removing much of the overhead involved in building jack-of-all-trades general-purpose chips. The GPUs that power both computer graphics and machine learning are an excellent example of the power of specialization.
T hey excel in matrix multiplications and can run the snippet of code mentioned above 360,000x faster than the original implementation.
One of the biggest challenge s is that many of these avenues for performance gains are intertwined. For instance, more cores on a processor only gives you a boost if code is designed to be more parallel.
That means the easiest place to make these gains is with what the authors call “big components”—complex and self-contained bits of computing machinery such as operating systems, microprocessors, or compilers—particularly when they’re owned by a single entity who has the freedom and motivation to make significant changes.
Whether the owners of these big components heed the authors’ call remains to be seen. But at least now we have a roadmap to get us over the hump until one of the post-Moore’s Law computing technologies takes us beyond the limitations of silicon.
Image Credit: Miguel Á. Padriñán from Pixabay Tags Computing Tech RELATED Why Google and Bing’s Embrace of Generative AI Could Upend the SEO Industry October 29, 2023 Atom Computing Says Its New Quantum Computer Has Over 1,000 Qubits October 25, 2023 This Brain-Like IBM Chip Could Drastically Cut the Cost of AI October 24, 2023 latest Like Humans, This Breakthrough AI Makes Concepts Out of the Words It Learns October 31, 2023 Why Google and Bing’s Embrace of Generative AI Could Upend the SEO Industry October 29, 2023 This Week’s Awesome Tech Stories From Around the Web (Through October 28) October 28, 2023 featured Carl Sagan Detected Life on Earth 30 Years Ago—Here’s Why His Experiment Still Matters Today October 26, 2023 Singularity Hub News and Insights from Singularity Group Singularity labs A 360 Singularity Hub About Creative Commons Pitch Us Contact Us Terms of Use Privacy Policy Singularity Group About Executive Program Custom Programs Podcasts Insights Blog Stay connected Facebook Instagram RSS Twitter Youtube Get the latest news from Singularity Hub! Sign Up Copyright © Singularity Group. All rights reserved.
