old id = 2959
The Turbulent Past and Uncertain Future of Artificial Intelligence - IEEE Spectrum
1958
https://spectrum.ieee.org/history-of-ai

TopicsSectionsMoreFor IEEE MembersFor IEEE MembersIEEE SpectrumFollow IEEE SpectrumSupport IEEE SpectrumIEEE websites place cookies on your device to give you the best user experience. By using our websites, you agree to the placement of these cookies. To learn more, read our Privacy Policy.
Enjoy more free content and benefits by creating an accountSaving articles to read later requires an IEEE Spectrum accountThe Institute content is only available for membersDownloading full PDF issues is exclusive for IEEE MembersAccess toSpectrum's Digital Edition is exclusive for IEEE MembersFollowing topics is a feature exclusive for IEEE MembersAdding your response to an article requires an IEEE Spectrum accountCreate an account to access more content and features onIEEE Spectrum, including the ability to save articles to read later, download Spectrum Collections, and participate in conversations with readers and editors. For more exclusive content and features, considerJoining IEEE.
Join the world’s largest professional organization devoted to engineering and applied sciences and get access to all of Spectrum’s articles, archives, PDF downloads, and other benefits.
Learn more →Access Thousands of Articles — Completely FreeCreate an account and get exclusive content and features:Save articles, download collections,andtalk to tech insiders— all free! For full access and benefits,join IEEEas a paying member.
The Turbulent Past and Uncertain Future of Artificial IntelligenceIs there a way out of AI's boom-and-bust cycle?The 1958 perceptron was billed as "the first device to think as the human brain." It didn't quite live up to the hype.
In the summer of 1956,a group of mathematicians and computer scientists took over the top floor of the building that housed the math department of Dartmouth College. For about eight weeks, they imagined the possibilities of a new field of research.
John McCarthy, then a young professor at Dartmouth, had coined the term "artificial intelligence" when he wrote hisproposalfor the workshop, which he said would explore the hypothesis that "every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it."The researchers at that legendary meeting sketched out, in broad strokes, AI as we know it today. It gave rise to the first camp of investigators: the "symbolists," whose expert systems reached a zenith in the 1980s. The years after the meeting also saw the emergence of the "connectionists," who toiled for decades on the artificial neural networks that took off only recently. These two approaches were long seen as mutually exclusive, and competition for funding among researchers created animosity. Each side thought it was on the path to artificial general intelligence.
This article is part of our special report on AI, “The Great AI Reckoning.”A look back at the decades since that meeting shows how often AI researchers' hopes have been crushed—and how little those setbacks have deterred them. Today, even as AI is revolutionizing industries and threatening to upend the global labor market, many experts are wondering if today's AI is reaching its limits. As Charles Choi delineates in "Seven Revealing Ways AIs Fail," the weaknesses of today's deep-learning systems are becoming more and more apparent. Yet there's little sense of doom among researchers. Yes, it's possible that we're in for yet another AI winter in the not-so-distant future. But this might just be the time when inspired engineers finally usher us into an eternal summer of the machine mind.
Researchers developing symbolic AIset out to explicitly teach computers about the world. Their founding tenet held that knowledge can be represented by a set of rules, and computer programs can use logic to manipulate that knowledge. Leading symbolistsAllen NewellandHerbert Simonargued that if a symbolic system had enough structured facts and premises, the aggregation would eventually produce broad intelligence.
The connectionists, on the other hand, inspired by biology, worked on "artificial neural networks" that would take in information and make sense of it themselves. The pioneering example was theperceptron, an experimental machine built by the Cornell psychologistFrank Rosenblattwith funding from the U.S. Navy. It had 400 light sensors that together acted as a retina, feeding information to about 1,000 "neurons" that did the processing and produced a single output. In 1958, aNew York Timesarticlequoted Rosenblatt as saying that "the machine would be the first device to think as the human brain."Frank Rosenblatt invented the perceptron, the first artificial neural network.
Cornell University Division of Rare and Manuscript CollectionsUnbridled optimism encouraged government agencies in the United States and United Kingdom to pour money into speculative research. In 1967, MIT professorMarvin Minskywrote: "Within a generation...the problem of creating 'artificial intelligence' will be substantially solved." Yet soon thereafter, government funding started drying up, driven by a sense that AI research wasn't living up to its own hype. The 1970s saw the first AI winter.
True believers soldiered on, however. And by the early 1980s renewed enthusiasm brought a heyday for researchers in symbolic AI, who received acclaim and funding for "expert systems" that encoded the knowledge of a particular discipline, such as law or medicine. Investors hoped these systems would quickly find commercial applications. The most famous symbolic AI venture began in 1984, when the researcherDouglas Lenatbegan work on a project he namedCycthat aimed toencode common sense in a machine. To this very day, Lenat and his team continue to add terms (facts and concepts) to Cyc's ontology and explain the relationships between them via rules. By 2017, the team had 1.5 million terms and 24.5 million rules. Yet Cyc is still nowhere near achieving general intelligence.
In the late 1980s, the cold winds of commerce brought on the second AI winter. The market for expert systems crashed because they required specialized hardware and couldn't compete with the cheaper desktop computers that were becoming common. By the 1990s, it was no longer academically fashionable to be working on either symbolic AI or neural networks, because both strategies seemed to have flopped.
The field of AI began at a 1956 workshop [top] attended by, from left, Oliver Selfridge, Nathaniel Rochester, Ray Solomonoff, Marvin Minsky, an unidentified person, workshop organizer John McCarthy, and Claude Shannon. Symbolists such as Herbert Simon [middle] and Allen Newell [bottom] wanted to teach AI rules about the world.
The Minsky Family; Carnegie Mellon University (2)But the cheap computers that supplanted expert systems turned out to be a boon for the connectionists, who suddenly had access to enough computer power to run neural networks with many layers of artificial neurons. Such systems became known as deep neural networks, and the approach they enabled was called deep learning.
Geoffrey Hinton, at the University of Toronto, applied a principle calledback-propagationto make neural nets learn from their mistakes (see "How Deep Learning Works").
One of Hinton's postdocs,Yann LeCun, went on to AT&T Bell Laboratories in 1988, where he and a postdoc namedYoshua Bengioused neural nets foroptical character recognition; U.S. banks soon adopted the technique for processing checks. Hinton, LeCun, and Bengio eventuallywon the 2019 Turing Awardand are sometimes called the godfathers of deep learning.
But the neural-net advocates still had one big problem: They had a theoretical framework and growing computer power, but there wasn't enough digital data in the world to train their systems, at least not for most applications. Spring had not yet arrived.
Over the last two decades,everything has changed. In particular, the World Wide Web blossomed, and suddenly, there was data everywhere. Digital cameras and then smartphones filled the Internet with images, websites such as Wikipedia and Reddit were full of freely accessible digital text, and YouTube had plenty of videos. Finally, there was enough data to train neural networks for a wide range of applications.
The other big development came courtesy of the gaming industry. Companies such asNvidiahad developed chips called graphics processing units (GPUs) for the heavy processing required to render images in video games. Game developers used GPUs to do sophisticated kinds of shading and geometric transformations. Computer scientists in need of serious compute power realized that they could essentially trick a GPU into doing other tasks—such as training neural networks. Nvidia noticed the trend and createdCUDA, a platform that enabled researchers to use GPUs for general-purpose processing. Among these researchers was a Ph.D. student in Hinton's lab namedAlex Krizhevsky, who used CUDA to write the code for a neural network that blew everyone away in 2012.
MIT professor Marvin Minsky predicted in 1967 that true artificial intelligence would be created within a generation.
The MIT MuseumHe wrote it for the ImageNet competition, which challenged AI researchers to build computer-vision systems that could sort more than 1 million images into 1,000 categories of objects. While Krizhevsky'sAlexNetwasn't the first neural net to be used for image recognition, itsperformance in the 2012 contestcaught the world's attention. AlexNet's error rate was 15 percent, compared with the 26 percent error rate of the second-best entry. The neural net owed its runaway victory to GPU power and a "deep" structure of multiple layers containing 650,000 neurons in all. In the next year's ImageNet competition, almost everyone used neural networks. By 2017, many of the contenders' error rates had fallen to 5 percent, and the organizers ended the contest.
Deep learning took off. With the compute power of GPUs and plenty of digital data to train deep-learning systems, self-driving cars could navigate roads, voice assistants could recognize users' speech, and Web browsers could translate between dozens of languages. AIs also trounced human champions at several games that were previously thought to be unwinnable by machines, including theancient board game Goand the video gameStarCraft II. The current boom in AI has touched every industry, offering new ways to recognize patterns and make complex decisions.
A look back across the decades shows how often AI researchers' hopes have been crushed—and how little those setbacks have deterred them.
But the widening array of triumphs in deep learning have relied on increasing the number of layers in neural nets and increasing the GPU time dedicated to training them. One analysis from the AI research companyOpenAIshowed that the amount of computational power required to train the biggest AI systems doubled every two years until 2012—and after thatit doubled every 3.4 months. As Neil C. Thompson and his colleagues write in "Deep Learning's Diminishing Returns," many researchersworrythat AI's computational needs are on anunsustainable trajectory. To avoid busting the planet's energy budget, researchers need to bust out of the established ways of constructing these systems.
While it might seemas though the neural-net camp has definitively tromped the symbolists, in truth the battle's outcome is not that simple. Take, for example, the robotic hand from OpenAI that made headlines for manipulating andsolving a Rubik's cube. The robot used neural netsandsymbolic AI. It's one of many new neuro-symbolic systems that use neural nets for perception and symbolic AI for reasoning, a hybrid approach that may offer gains in both efficiency and explainability.
Neither symbolic AI projects such as Cyc from Douglas Lenat [top] nor the deep-learning advances pioneered by [from top] Geoffrey Hinton, Yann LeCun, and Yoshua Bengio have yet produced human-level intelligence.
From top: Bob E. Daemmrich/Sygma/Getty Images; Christopher Wahl/The New York Times/Redux; Bruno Levy/REA/Redux; Cole Burston/Bloomberg/Getty ImagesAlthough deep-learning systems tend to be black boxes that make inferences in opaque and mystifying ways, neuro-symbolic systems enable users to look under the hood and understand how the AI reached its conclusions. The U.S. Army is particularly wary of relying on black-box systems, as Evan Ackerman describes in "How the U.S. Army Is Turning Robots Into Team Players," so Army researchers are investigating a variety of hybrid approaches to drive their robots and autonomous vehicles.
Imagine if you could take one of the U.S. Army's road-clearing robots and ask it to make you a cup of coffee. That's a laughable proposition today, because deep-learning systems are built for narrow purposes and can't generalize their abilities from one task to another. What's more, learning a new task usually requires an AI to erase everything it knows about how to solve its prior task, a conundrum called catastrophic forgetting. AtDeepMind, Google's London-based AI lab, the renowned roboticistRaia Hadsellis tackling this problem with a variety of sophisticated techniques. In "How DeepMind Is Reinventing the Robot," Tom Chivers explains why this issue is so important for robots acting in the unpredictable real world. Other researchers are investigating new types of meta-learning in hopes of creating AI systems that learn how to learn and then apply that skill to any domain or task.
All these strategies may aid researchers' attempts to meet their loftiest goal: building AI with the kind of fluid intelligence that we watch our children develop. Toddlers don't need a massive amount of data to draw conclusions. They simply observe the world, create a mental model of how it works, take action, and use the results of their action to adjust that mental model. They iterate until they understand. This process is tremendously efficient and effective, and it's well beyond the capabilities of even the most advanced AI today.
Although the current level of enthusiasm has earned AI its ownGartner hype cycle, and although the funding for AI has reached an all-time high, there's scant evidence that there's a fizzle in our future. Companies around the world are adopting AI systems because they see immediate improvements to their bottom lines, and they'll never go back. It just remains to be seen whether researchers will find ways to adapt deep learning to make it more flexible and robust, or devise new approaches that haven't yet been dreamed of in the 65-year-old quest to make machines more like us.
This article appears in the October 2021 print issue as "The Turbulent Past and Uncertain Future of AI."Eliza Stricklandis a senior editor atIEEE Spectrum, where she covers AI, biomedical engineering, and other topics. She holds a master's degree in journalism from Columbia University.
You write :" The market for expert systems crashed because they required specialized hardware and couldn't compete with the cheaper desktop computers that were becoming common". this in completely incorrect, hardware had little to do with the problem of managing systems with huge numbers of rules, or - more importantly - keeping the rules up to date when the experts were no longer around.
The development of artificial intelligence is not an isolated field, but the result of a great breakthrough in information technology and industrial intelligence. The result will affect all aspects of human society, subvert and reshape the whole world economic pattern. The artificial intelligence system is based on the neural network algorithm composed of simulated neurons, and carries out in-depth learning, memory and operation through data mining and probability summary. With the support of hardware, more and more algorithms are used for interactive learning and improvement, so as to continuously improve the ability of prediction and decision-making, comprehensively promote the intelligent development of society, and realize large-scale replacement of manpower in a cascade way.
The development speed of artificial intelligence is limited by computing power. When an artificial intelligence needs continuous development, it needs multiple computing power. This leads to the fact that artificial intelligence has only one ability, and it can't do many kinds of things like a real human. As the article said, "you could not take one of the U.S. Army's road clearing robots and ask it to make you a cup of coffee.".
Practical Power Beaming Gets RealVideo Friday: Drone in a CageRemembering 1982 IEEE President Robert LarsonAcer Goes Big on Glasses-Free, 3D Monitors—Look Out, VRIs this what’s needed to bring augmented reality to the home office?Matthew S. Smithwrites IEEE Spectrum's Gizmo column and is a freelance consumer-tech journalist. An avid gamer, he is a former staff editor at Digital Trends and is particularly fond of wearables, e-bikes, all things smartphone, and CES, which he has attended every year since 2009.
Content creators are a key target for Acer’s glasses-free 3D.
Acer, the world’s fifth largest PC brand, wants to take the growing AR/VR market by the horns with its SpatialLabs glasses-free stereoscopic 3D displays.
First teased in 2021 in a variant of Acer’s ConceptD 7 laptop, the technology expands this summer in a pair of portable monitors, the SpatialLabs View and View Pro, and select Acer Predator gaming laptops. The launch is paired with artificial-intelligence-powered software for converting existing 2D content into stereoscopic 3D.
“We see a convergence of virtual and reality,” Jane Hsu,head of business Development for SpatialLabs, said in an interview. “It’s a different form for users to start interacting with a virtual world.” Glasses-free stereoscopic 3D isn’t new.
Evolutionary, not revolutionaryThe technology has powered several niche products and prototypes,such as Sony’s Spatial Reality Display, but its most famous debut was Nintendo’s 3DS portable game console.
The 3DS filtered two images through a display layer called a parallax barrier. This barrier controlled the angle an image reached the user’s eyes to create the 3D effect. Because angle was important, the 3DS used cameras that detected the user’s eyes and adjusted the image to compensate for viewing angle.
“The PC in 2022 is encountering a lot of problems.”—Jerry Kao, AcerAcer’s technology is similar. It also displays two images which are filtered through an “optical layer” and has cameras to track and compensate for the user’s viewing angle.
So, what’s different this time?“The fundamental difference is that the computing power is way different, and resolution is way different,” said Hsu. “The Nintendo, that was 800 by 240. In a sense, the technology is the same, but over time it has improved for a crystal-clear, high-resolution experience.”Resolution is important to this form of glasses-free 3D. Because it renders two images to create the 3D effect, the resolution of the display is cut in half on the horizontal axis when 3D is on. The 3DS cut resolution to 400 by 240 when 3D was on and blurry visuals werea common complaint among critics.
Acer’s SpatialLabs laptops and displays are a big improvement. Each provides native 4K (3,840 by 2,160 resolution) in 2D. That’s 43 times the pixel count of Nintendo’s 3DS. Turning 3D on shaves resolution to 1,920 by 2,160, which, while lower, is still sharper than that of a 27-inch 4K monitor.
Hsu says advancements in AI compute are also key. Partners like Nvidia and Intel can now accelerate AI in hardware, a feature that wasn’t common a half decade ago.
Acer has harnessed this for SpatialLabs GO, a software utility that can convert full-screen content from 2D to stereoscopic 3D. This should make SpatialLabs useful with a wider range of content. It can also help creators generate content for use in stereoscopic 3D by importing and converting existing assets.
A new angle on augmented realityAcer was a lead partner in Microsoft’s push for mixed-reality headsets. They were a flop, and their failure taught Acer hard lessons about how people approach AR/VR hardware in the real world.
“Acer spent a lot bringing VR headsets to market, but...it was not very successful,”Acer Co-COO Jerry Kao said in an interview. “There were limitations. It’s not comfortable, or it’s expensive, and you need space around you. So, we wanted to address this.”SpatialLabs is a complementary alternative. Creators can use Spatial Labs to achieve a 3D effect in their home office without pushing aside furniture. The Acer View Pro, meant for commercial use, may have a future in retail displays, a use that headsets can't address.
The View Pro display is built for use in kiosks and retail displays.
AcerMost of the SpatialLabs product line, including the ConceptD 7 laptop and View displays, lean toward creative professionals using programs like Maya and Blender to create 3D content. Acer says its software suite has “out-of-the-box support for all major file formats.” It recently added support for Datasmith, a plug-in used to import assets toEpic’s Unreal Engine.
But the technology is also coming to Predator gaming laptops for glasses-free stereoscopic 3D in select titles likeForza Horizon 5andThe Witcher 3: Wild Hunt. Gaming seems a natural fit given its history in Nintendo’s handheld, and Hsu thinks it will help attract mainstream attention.
“When the Turn 10 team [developer of the Forza Horizon series] saw what we had done withForza Horizon 5, they were like, ‘Wow, this is so great!’ ” said Hsu. “They said, ‘You know what? I think I can build the scene with even more depth.’ And this is just the beginning.”Does glasses-free 3D really stand a chance?SpatialLabs brings gains in resolution and performance, but it’s far from a surefire hit. Acer is the only PC maker currently pursuing the hardware. Going it alone won’t be easy.
“While the tech seems quite appealing, it will likely remain a niche product that’ll be used in rare instances by designers or developers rather than the average consumer,”Jitesh Ubrani, research manager at IDC, said in an email. He thinks Acer could find it difficult to deliver on price and availability, “both of which are tough to do for such a fringe technology.”I asked Hsu how Acer will solve these issues. “In a way he’s right, it is difficult. We’re building this ourselves,” said Hsu. “But also, the hardware is more mature.”Kao chimed in to say SpatialLabs will stand out in what might be weak year for home computers. “The PC in 2022 is encountering a lot of problems,” Kao said. He sees that as a motivation, not a barrier, for novel technology on the PC.
“Intel, Google, Microsoft, and a lot of people, they have technology,” said Kao. “But they don’t know how to leverage that technology in the product and deliver the experience to specific people. That is what Acer is good at.”DARPA Wants a Better, Badder Caspian Sea MonsterLiberty Lifter X-plane will leverage ground effectArguably, the primary job of any military organization is moving enormous amounts of stuff from one place to another as quickly and efficiently as possible. Some of that stuff is weaponry, but the vast majority are things that support that weaponry—fuel, spare parts, personnel, and so on. At the moment, the U.S. military has two options when it comes to transporting large amounts of payload. Option one is boats (a sealift), which are efficient, but also slow and require ports. Option two is planes (an airlift), which are faster by a couple of orders of magnitude, but also expensive and require runways.
To solve this, the Defense Advanced Research Projects Agency (DARPA) wants to combine traditional sealift and airlift with theLiberty Lifter program, which aims to “design, build, and flight test an affordable, innovative, and disruptive seaplane” that “enables efficient theater-range transport of large payloads at speeds far exceeding existing sea lift platforms.”DARPADARPA is asking for a design like this to take advantage of ground effect, which occurs when an aircraft’s wing deflects air downward and proximity to the ground generates a cushioning effect due to the compression of air between the bottom of the wing and the ground. This boosts lift and lowers drag to yield a substantial overall improvement in efficiency. Ground effect works on both water and land, but you can take advantage of it for only so long on land before your aircraft runs into something. Which is why oceans are the ideal place for these aircraft—or ships, depending on your perspective.
During the late 1980s, the Soviets (and later the Russians) leveraged ground effect in the design of a handful of awesomely bizarre ships and aircraft. There’s theVVA-14, which was also an airplane, along with the vehicle shown in DARPA’s video above, theLun-class ekranoplan, which operated until the late 1990s. The video clip really does not do this thing justice, so here’s a better picture, taken a couple of years ago:InstagramTheLun(only one was ever made) had a wingspan of 44 meters and was powered by eight turbojet engines. It flew about 4 meters above the water at speeds of up to 550 kilometers per hour, and could transport almost 100,000 kilograms of cargo for 2,000 km. It was based on an earlier, even larger prototype (the largest aircraft in the world at the time) that the CIA spotted in satellite images in 1967 and which seems to have seriously freaked them out. It was nicknamed the Caspian Sea Monster, and it wasn’t until the 1980s that the West understood what it was and how it worked.
In the mid 1990s, DARPA itself took a serious look at a stupendously large ground-effect vehicle of its own, theAerocon Dash 1.6 wingship. The concept image below is of a 4.5-million-kg vehicle, 175 meters long with a 100-meter wingspan, powered by 20 (!) jet engines:WikipediaWith a range of almost 20,000 km at over 700 km/h, the wingship could have carried 3,000 passengers or 1.4 million kg of cargo. By 1994, though, DARPA had decided that the potential billion-dollar project to build a wingship like this was too risky, and canceled the whole thing.
Less than 10 years later, Boeing’s Phantom Works started exploring an enormous ground-effect aircraft, thePelican Ultra Large Transport Aircraft. The Pelican would have been even larger than the Aerocon wingship, with a wingspan of 152 meters and a payload of 1.2 million kg—that’s about 178 shipping containers’ worth. Unlike the wingship, the Pelican would take advantage of ground effect to boost efficiency only in transit above water, but would otherwise use runways like a normal aircraft and be able to reach flight altitudes of 7,500 meters. Operating as a traditional aircraft and with an optimal payload, the Pelican would have a range of about 12,000 km. In ground effect, however, the range would have increased to 18,500 km, illustrating the appeal of designs like these. But Boeing dropped the project in 2005 to focus on lower cost, less risky options.
We’d be remiss if we didn’t at least briefly mention two other massive aircraft: theH-4 Hercules, the cargo seaplane built by Hughes Aircraft Co. in the 1940s, and theStratolaunch carrier aircraft, which features a twin-fuselage configuration that DARPA seems to be favoring in its concept video for some reason.
From the sound of DARPA’s announcement, they’re looking for something a bit more like the Pelican than the Aerocon Dash or theLun. DARPA wants the Liberty Lifter to be able to sustain flight out of ground effect if necessary, although it’s expected to spend most of its time over water for efficiency. It won’t use runways on land at all, though, and should be able to stay out on the water for 4 to 6 weeks at a time, operating even in rough seas—a significant challenge for ground-effect aircraft.
DARPA is looking for an operational range of 7,500 km, with a maximum payload of at least 90,000 kg, including the ability to launch and recover amphibious vehicles. The hardest thing DARPA is asking for could be that, unlike most other X-planes, the Liberty Lifter should incorporate a “low cost design and construction philosophy” inspired by the mass-produced Liberty ships of World War II.
With US $15 million to be awarded to up to two Liberty Lifter concepts, DARPA is hoping that at least one of those concepts will pass a system-level critical design review in 2025. If everything goes well after that, the first flight of a full-scale prototype vehicle could happen as early as 2027.
Modeling Microfluidic Organ-on-a-Chip DevicesRegister for this webinar to enhance your modeling and design processes for microfluidic organ-on-a-chip devices using COMSOL MultiphysicsIf you want to enhance your modeling and design processes for microfluidic organ-on-a-chip devices,tune into this webinar.
You will learn methods for simulating the performance and behavior of microfluidic organ-on-a-chip devices and microphysiological systems in COMSOL Multiphysics. Additionally, you will see how to couple multiple physical effects in your model, including chemical transport, particle tracing, and fluid–structure interaction. You will also learn how to distill simulation output to find key design parameters and obtain a high-level description of system performance and behavior.
There will also be a live demonstration of how to set up a model of a microfluidic lung-on-a-chip device with two-way coupled fluid–structure interaction. The webinar will conclude with a Q&A session.
Register now for this free webinar!Trending StoriesPractical Power Beaming Gets RealDARPA Wants a Better, Badder Caspian Sea MonsterSimple, Cheap, and Portable: A Filter-Free Desalination System for a Thirsty WorldVideo Friday: Drone in a CageAcer Goes Big on Glasses-Free, 3D Monitors—Look Out, VRAndrew Ng: Unbiggen AIHydrogen Helps Make Topological Insulators PracticalBefore Ships Used GPS, There Was the Fresnel Lens
