old id = 723
"CBLL, Research Projects, Computational and Biological Learning Lab, Courant Institute, NYU"
2007
http://www.cs.nyu.edu/~yann/research/relreg

CBLL HOME VLG Group News/Events Seminars People Research Publications Talks Demos Datasets Software Courses Links Group Meetings Join CBLL Y. LeCun's website CS at Courant Courant Institute NYU Relational Regression Models Time Period : September 2006 - present.
Participants : Sumit Chopra, Yann LeCun (Courant Institute/CBLL), Trivikraman Thampy, John Leahy, Andrew Caplin (Economics Dept, NYU).
Description : We are developing a new type of relational graphical models that can be applied to "structured regression problem". A prime example of structured regression problem is the prediction of house prices. The price of a house depends not only on the characteristics of the house, but also of the prices of similar houses in the neighborhood, or perhaps on hidden features of the neighborhood that influence them. Our relational regression model infers a hidden "desirability sruface" from which house prices are predicted.
Publications 130.
Sumit Chopra, Trivikraman Thampy, John Leahy, Andrew Caplin and Yann LeCun : Discovering the hidden structure of house prices with non-parametric latent manifold model , Proc. Knowledge Discovery in Databases (KDD'07) , 2007 , \cite{chopra-kdd-07}.
422KB DjVu 1182KB PDF 17382KB PS.GZ Description.
li-itemize{margin:1ex 0ex;} .li-enumerate{margin:1ex 0ex;} .dd-description{margin:0ex 0ex 1ex 4ex;} .dt-description{margin:0ex;} .toc{list-style:none;} .thefootnotes{text-align:left;margin:0ex;} .dt-thefootnotes{margin:0em;} .dd-thefootnotes{margin:0em 0em 0em 2em;} .footnoterule{margin:1em auto 1em 0px;width:50%;} .caption{padding-left:2ex; padding-right:2ex; margin-left:auto; margin-right:auto} .title{margin:auto;text-align:center} .center{text-align:center;margin-left:auto;margin-right:auto;} .flushleft{text-align:left;margin-left:0ex;margin-right:auto;} .flushright{text-align:right;margin-left:auto;margin-right:0ex;} DIV TABLE{margin-left:inherit;margin-right:inherit;} PRE{text-align:left;margin-left:0ex;margin-right:auto;} BLOCKQUOTE{margin-left:4ex;margin-right:4ex;text-align:left;} TD P{margin:0px;} .boxed{border:1px solid black} .textboxed{border:1px solid black} .vbar{border:none;width:2px;background-color:black;} .hbar{border:none;height:2px;width:100%;background-color:black;} .hfill{border:none;height:1px;width:200%;background-color:black;} .vdisplay{border-collapse:separate;border-spacing:2px;width:auto; empty-cells:show; border:2px solid red;} .vdcell{white-space:nowrap;padding:0px;width:auto; border:2px solid green;} .display{border-collapse:separate;border-spacing:2px;width:auto; border:none;} .dcell{white-space:nowrap;padding:0px;width:auto; border:none;} .dcenter{margin:0ex auto;} .vdcenter{border:solid #FF8000 2px; margin:0ex auto;} .minipage{text-align:left; margin-left:0em; margin-right:auto;} .marginpar{border:solid thin black; width:20%; text-align:left;} .marginparleft{float:left; margin-left:0ex; margin-right:1ex;} .marginparright{float:right; margin-left:1ex; margin-right:0ex;} .theorem{text-align:left;margin:1ex auto 1ex 0ex;} .part{margin:auto;text-align:center} A majority of supervised learning algorithms process an input point independently of other points, based on the assumptions that the input data is sampled independently and identically from a fixed underlying distribution. However in a number of real-world problems the value of variables associated with each sample not only depends on features specific to the sample, but also on the features and variables of other samples related to it. We say the data possesses a relational structure.
Prices of real estate properties is one such example. Price of a house is a function of its individual features, such as the number of bedrooms, etc. In addition the price is also influenced by features of the neighborhood in which it lies. Some of these features are measurable, such as the quality of the local schools. However most of them that make a particular neighborhood desirable are very difficult to measure directly, and are merely reflected in the price of houses in that neighborhood. Hence the "desirability" of a location/neighborhood can be modeled as a latent variable, that must be estimated as part of the learning process, and efficiently inferred for unseen samples.
A number of authors have recently proposed architectures and learning algorithms that make use of relational structure. The earlier techniques were based on the idea of influence propagation&#XA0;[ 1 , 3 , 6 , 5 ]. Probabilistic Relational Models (PRMs) were introduced in&#XA0;[ 4 , 2 ] as an extension of Bayesian networks to relational data. Their discriminative extensions, called Relational Markov Networks (RMN) were later proposed in&#XA0;[ 7 ].
This paper introduces a general framework for prediction in relational data. An architecture is presented that allows efficient inference algorithms for continuous variables with relational dependencies. The class of models introduced is novel in several ways: 1. it pertains to relational regression problems in which the answer variable is continuous; 2. it allows inter-sample dependencies through hidden variables as well as through the answer variables; 3. it allows log-likelihood functions that are non-linear in the parameters (non exponential family), which leads to non-convex loss functions but are considerably more flexible; 4. it eliminates the intractable partition function problem through appropriate design of the relational and non-relational factors.
The idea behind a relational factor graph is to have a single factor graph that models the entire collection of data samples and their dependencies. The relationships between samples is captured by the factors that connect the variables associated with multiple samples. We are given a set of N training samples, each of which is described by a sample-specific feature vector X i and an answer to be predicted Y i.
 Let the collection of input variables be denoted by X = { X i , &#XA0; i = 1 &#X2026; N }, the output variables by Y = { Y i , &#XA0; i = 1 &#X2026; N }, and the latent variables by Z.
 The EBRFG is defined by an energy function of the form E ( W , Z , Y , X ) = E ( W , Z , Y 1 , &#X2026;, Y N , X 1 , &#X2026;, X N ), in which W is the set of parameters to be estimated by learning. Given a test sample feature vector X 0 , the model is used to predict the value of the corresponding answer variable Y 0.
 One way to do this is by minimizing the following energy function augmented with the test sample ( X 0 , Y 0 ) Y 0* &#XA0;=&#XA0; argmin Y 0 &#XA0;{ min Z E ( W ,&#XA0; Z ,&#XA0; Y 0 ,&#XA0;&#X2026;,&#XA0; Y N ,&#XA0; X 0 ,&#XA0;&#X2026;,&#XA0; X N )}. &#XA0;&#XA0;&#XA0;&#XA0;(1) For it to be usable on new test samples without requiring excessive work, the energy function must be carefully constructed is such a way that the addition of a new sample in the arguments will not require re-training the entire system, or re-estimating some high-dimensional hidden variables. Moreover, the parameterization must be designed in such a way that its estimation on the training sample will actually result in good prediction on test samples. Training an EBRFG can be performed by minimizing the negative log conditional probability of the answer variables with respect to the parameter W.
 We propose an efficient training and inference algorithm for the general model.
The architecture of the factor graph that was used for predicting the prices of real estate properties is shown in Figure&#XA0; 1 (top). The price of a house is modeled as a product of two quantities: 1. its "intrinsic" price which is dependent only on its individual features, and 2. the desirability of its location. A pair of factors E xyz i and E zz i are associated with every house.
E xyz i is non-relational and captures the sample specific dependencies. It is modeled as a parametric function with learnable parameters W xyz.
 The parameters W xyz are shared across all the instances of E xyz i.
 The factor E zz i is relational and captures the dependencies between the samples via the "hidden" variables Z i.
 These dependencies influence the answer for a sample through the intermediary hidden variable d i.
 The variables Z i can be interpreted as the desirability of the location of the i -th house, and d i can be viewed as the estimated desirability of the house using the desirabilities of the houses related to it (those that lie in its vicinity). This factor is modeled as a non-parametric function. In particular we use a locally weighted linear regression, with weights given by a gaussian kernel.
The model is trained by maximizing the likelihood of the training data, which is realized by minimizing the negative log likelihood function with respect to W and Z.
 However we show that this minimization reduces to L ( W , Z )&#XA0; &#XA0;&#XA0;=&#XA0;&#XA0;&#XA0; n &#X2211; i &#XA0;=&#XA0;1 &#XA0; 1 2 &#XA0;( Y i &#XA0;&#X2212;&#XA0;( G ( W xyz , X i )&#XA0;+&#XA0; H ( Z &#XA0; N i , X i ))) 2 &#XA0;+&#XA0; R ( Z ), &#XA0;&#XA0;&#XA0;&#XA0;(2) where R ( Z ) is a regularizer on Z (an L 2 regularizer in the experiments). This is achieved by applying a type of deterministic generalized EM algorithm. It consists of iterating through two phases. Phase 1 involves keeping the parameters W fixed and minimizing the loss with respect to Z.
 The loss is quadratic in Z and we show that its minimization reduced to solving a large scale sparse quadratic system. We used conjugate gradient method using an adaptive threshold to minimize it. Phase 2 involves fixing the hidden variables Z and minimizing with respect to W.
 Since the parameters W are shared among the factors, this can be done using gradient descent. Inference on a new sample X o involves computing its neighboring training samples, and using the learnt values of their hidden variables Z N o to get an estimate of its "desirability" d o ; passing the house specific features X h o through the learnt parametric model to get its "intrinsic" price; and combining the two to get its predicted price.
The model was trained and tested on a very challenging and diverse real world dataset that included 42,025 sale transactions of houses in Los Angeles county in the year 2004. Each house was described using a set of 18 house specific attributes like gps coordinates, living area, year build, number of bedrooms, etc. In addition, for each house, a number of neighborhood specific attributes obtained from census tract data and the school district data were also used. It included attributes like average house hold income of that area, percentage of owner occupied homes etc. The performance of the proposed model was compared with a number of standard non-relational techniques that that have been used in literature for this problem, namely nearest neighbor, locally weighted regression, linear regression, and fully connected neural network. EBRFG gives the best prediction accuracy by far, compared to other models. In addition we also plot the "desirabilities" learnt by our model (Figure&#XA0; 1 (bottom)). The plot shows that the model is actually able to learn the "desirabilities" of various areas in a way that is reflective of the real world situation.
References [1] S.&#XA0;Chakrabarti, B.&#XA0;Dom, and P.&#XA0;Indyk. Enhanced hypertext categorization using hyperlinks.
In. Proc. of ACM SIGMOD98 , pages 307 &#X2013; 318, 1998.
[2] N.&#XA0;Friedman, L.&#XA0;Getoor, D.&#XA0;Koller, and A.&#XA0;Pfeffer. Learning probabilistic relational models.
In Proc. IJCAI99 , pages 1300 &#X2013; 1309, 1999.
[3] J.&#XA0;M. Klienberg. Authoritative sources in a hyperlinked environment.
Journal of the ACM , 46(5):604 &#X2013; 632, 1999.
[4] D.&#XA0;Koller and A.&#XA0;Pfeffer. Probabilistic frame-based systems.
In Proc. AAAI98 , pages 580 &#X2013; 587, 1998.
[5] J.&#XA0;Neville and D.&#XA0;Jensen. Iterative classification in relational data.
In Proc. AAAI00 Workshop on Learning Statistical Models From Relational Data , pages 13 &#X2013; 20, 2000.
[6] S.&#XA0;Slattery and T.&#XA0;Mitchell. Discovering test set regularities in relational domain.
In. Proc. ICML00 , pages 895 &#X2013; 902, 2000.
[7] B.&#XA0;Taskar, P.&#XA0;Abbeel, and D.&#XA0;Koller. Discriminative probabilistic models for relational data.
Eighteenth Conference on Uncertainty on Machine Intelligence (UAI02) , 2002.
Table 1: Prediction accuracies of various algorithms on the test set. Absolute Relative Forecasting Error ( fe ) was measured. The error ( fe i ) on the i th sample is defined as fe i = | A i &#X2212; Pr i |/ A i , where A i is the actual selling price and Pr i is the predicted price. Three performance quantities on the test set are reported; percentage of houses with a forecasting error of less than 5%, with less than 10% and with less 15%.
Model Class Model < 5% <10% < 15% non-parametric Nearest Neighbor 25.41 47.44 64.72 non-parametric Locally Weighted Regression 32.98 58.46 75.21 parametric Linear Regression 26.58 48.11 65.12 parametric Fully Connected Neural Network 33.79 60.55 76.47 hybrid Relational Factor Graph 39.47 65.76 81.04 -0.1in &#XA0; &#XA0; Figure 1: (Top) A typical Energy Based Relational Factor Graph showing the connections between three samples. The factors E xyz i capture the dependencies between the features of individual samples and their answer variable Y i , as well as the dependence on local latent variables d i.
 The factors E zz i captures the dependencies between the hidden variables of multiple samples. The connection to these two factors may exist only from a subset of samples that are related to sample i.
 When the energy of factor E zz is quadratic in d.
(Bottom) The color coded values of the desirability surface at the location of the test samples. For every test sample, the estimate of its desirability is computed and is color coded according to its value. Blue color implies low desirability and red color implies high desirability.
This document was translated from L A T E X by H E V E A..
