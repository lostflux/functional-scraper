old id = 1117
"CBLL, Research Projects, Computational and Biological Learning Lab, Courant Institute, NYU"
2007
http://www.cs.nyu.edu/~yann/research/relreg

Relational Regression ModelsPublications130.
Sumit Chopra, Trivikraman Thampy, John Leahy, Andrew Caplin and Yann LeCun:Discovering the hidden structure of house prices with non-parametric latent manifold model,Proc. Knowledge Discovery in Databases (KDD'07),2007,\cite{chopra-kdd-07}.
Description.li-itemize{margin:1ex 0ex;} .li-enumerate{margin:1ex 0ex;} .dd-description{margin:0ex 0ex 1ex 4ex;} .dt-description{margin:0ex;} .toc{list-style:none;} .thefootnotes{text-align:left;margin:0ex;} .dt-thefootnotes{margin:0em;} .dd-thefootnotes{margin:0em 0em 0em 2em;} .footnoterule{margin:1em auto 1em 0px;width:50%;} .caption{padding-left:2ex; padding-right:2ex; margin-left:auto; margin-right:auto} .title{margin:auto;text-align:center} .center{text-align:center;margin-left:auto;margin-right:auto;} .flushleft{text-align:left;margin-left:0ex;margin-right:auto;} .flushright{text-align:right;margin-left:auto;margin-right:0ex;} DIV TABLE{margin-left:inherit;margin-right:inherit;} PRE{text-align:left;margin-left:0ex;margin-right:auto;} BLOCKQUOTE{margin-left:4ex;margin-right:4ex;text-align:left;} TD P{margin:0px;} .boxed{border:1px solid black} .textboxed{border:1px solid black} .vbar{border:none;width:2px;background-color:black;} .hbar{border:none;height:2px;width:100%;background-color:black;} .hfill{border:none;height:1px;width:200%;background-color:black;} .vdisplay{border-collapse:separate;border-spacing:2px;width:auto; empty-cells:show; border:2px solid red;} .vdcell{white-space:nowrap;padding:0px;width:auto; border:2px solid green;} .display{border-collapse:separate;border-spacing:2px;width:auto; border:none;} .dcell{white-space:nowrap;padding:0px;width:auto; border:none;} .dcenter{margin:0ex auto;} .vdcenter{border:solid #FF8000 2px; margin:0ex auto;} .minipage{text-align:left; margin-left:0em; margin-right:auto;} .marginpar{border:solid thin black; width:20%; text-align:left;} .marginparleft{float:left; margin-left:0ex; margin-right:1ex;} .marginparright{float:right; margin-left:1ex; margin-right:0ex;} .theorem{text-align:left;margin:1ex auto 1ex 0ex;} .part{margin:auto;text-align:center}A majority of supervised learning algorithms process an input point independently of other points, based on the assumptions that the input data is sampled independently and identically from a fixed underlying distribution. However in a number of real-world problems the value of variables associated with each sample not only depends on features specific to the sample, but also on the features and variables of other samples related to it. We say the data possesses arelational structure.
Prices of real estate properties is one such example. Price of a house is a function of its individual features, such as the number of bedrooms, etc. In addition the price is also influenced by features of the neighborhood in which it lies. Some of these features are measurable, such as the quality of the local schools. However most of them that make a particular neighborhood desirable are very difficult to measure directly, and are merely reflected in the price of houses in that neighborhood. Hence the "desirability" of a location/neighborhood can be modeled as a latent variable, that must be estimated as part of the learning process, and efficiently inferred for unseen samples.
A number of authors have recently proposed architectures and learning algorithms that make use of relational structure. The earlier techniques were based on the idea of influence propagation&#XA0;[1,3,6,5]. Probabilistic Relational Models (PRMs) were introduced in&#XA0;[4,2] as an extension of Bayesian networks to relational data. Their discriminative extensions, called Relational Markov Networks (RMN) were later proposed in&#XA0;[7].
This paper introduces a general framework for prediction in relational data. An architecture is presented that allows efficient inference algorithms for continuous variables with relational dependencies. The class of models introduced is novel in several ways: 1. it pertains torelational regression problemsin which the answer variable is continuous; 2. it allows inter-sample dependencies through hidden variables as well as through the answer variables; 3. it allows log-likelihood functions that are non-linear in the parameters (non exponential family), which leads to non-convex loss functions but are considerably more flexible; 4. it eliminates the intractable partition function problem through appropriate design of the relational and non-relational factors.
The idea behind a relational factor graph is to have a single factor graph that models the entire collection of data samples and their dependencies. The relationships between samples is captured by the factors that connect the variables associated with multiple samples. We are given a set ofNtraining samples, each of which is described by a sample-specific feature vectorXiand an answer to be predictedYi. Let the collection of input variables be denoted byX= {Xi, &#XA0;i= 1 &#X2026;N}, the output variables byY= {Yi, &#XA0;i= 1 &#X2026;N}, and the latent variables byZ. The EBRFG is defined by anenergy functionof the formE(W,Z,Y,X) =E(W,Z,Y1, &#X2026;,YN,X1, &#X2026;,XN), in whichWis the set of parameters to be estimated by learning. Given a test sample feature vectorX0, the model is used to predict the value of the corresponding answer variableY0. One way to do this is by minimizing the following energy function augmented with the test sample (X0,Y0)Y0*&#XA0;=&#XA0;argminY0&#XA0;{minZE(W,&#XA0;Z,&#XA0;Y0,&#XA0;&#X2026;,&#XA0;YN,&#XA0;X0,&#XA0;&#X2026;,&#XA0;XN)}. &#XA0;&#XA0;&#XA0;&#XA0;(1)For it to be usable on new test samples without requiring excessive work, the energy function must be carefully constructed is such a way that the addition of a new sample in the arguments will not require re-training the entire system, or re-estimating some high-dimensional hidden variables. Moreover, the parameterization must be designed in such a way that its estimation on the training sample will actually result in good prediction on test samples. Training an EBRFG can be performed by minimizing the negative log conditional probability of the answer variables with respect to the parameterW. We propose an efficient training and inference algorithm for the general model.
The architecture of the factor graph that was used for predicting the prices of real estate properties is shown in Figure&#XA0;1(top). The price of a house is modeled as a product of two quantities: 1. its "intrinsic" price which is dependent only on its individual features, and 2. the desirability of its location. A pair of factorsExyziandEzziare associated with every house.
Exyziis non-relational and captures the sample specific dependencies. It is modeled as a parametric function with learnable parametersWxyz. The parametersWxyzare shared across all the instances ofExyzi. The factorEzziis relational and captures the dependencies between the samples via the "hidden" variablesZi. These dependencies influence the answer for a sample through the intermediary hidden variabledi. The variablesZican be interpreted as the desirability of the location of thei-th house, anddican be viewed as the estimated desirability of the house using the desirabilities of the houses related to it (those that lie in its vicinity). This factor is modeled as a non-parametric function. In particular we use a locally weighted linear regression, with weights given by a gaussian kernel.
The model is trained by maximizing the likelihood of the training data, which is realized by minimizing the negative log likelihood function with respect toWandZ. However we show that this minimization reduces toL(W,Z)&#XA0; &#XA0;&#XA0;=&#XA0;&#XA0;&#XA0;n&#X2211;i&#XA0;=&#XA0;1&#XA0;12&#XA0;(Yi&#XA0;&#X2212;&#XA0;(G(Wxyz,Xi)&#XA0;+&#XA0;H(Z&#XA0;Ni,Xi)))2&#XA0;+&#XA0;R(Z), &#XA0;&#XA0;&#XA0;&#XA0;(2)whereR(Z) is a regularizer onZ(anL2regularizer in the experiments). This is achieved by applying a type of deterministic generalized EM algorithm. It consists of iterating through two phases. Phase 1 involves keeping the parametersWfixed and minimizing the loss with respect toZ. The loss is quadratic inZand we show that its minimization reduced to solving a large scale sparse quadratic system. We used conjugate gradient method using an adaptive threshold to minimize it. Phase 2 involves fixing the hidden variablesZand minimizing with respect toW. Since the parametersWare shared among the factors, this can be done using gradient descent. Inference on a new sampleXoinvolves computing its neighboring training samples, and using the learnt values of their hidden variablesZNoto get an estimate of its "desirability"do; passing the house specific featuresXhothrough the learnt parametric model to get its "intrinsic" price; and combining the two to get its predicted price.
The model was trained and tested on a very challenging and diverse real world dataset that included 42,025 sale transactions of houses in Los Angeles county in the year 2004. Each house was described using a set of 18 house specific attributes like gps coordinates, living area, year build, number of bedrooms, etc. In addition, for each house, a number of neighborhood specific attributes obtained from census tract data and the school district data were also used. It included attributes like average house hold income of that area, percentage of owner occupied homes etc. The performance of the proposed model was compared with a number of standard non-relational techniques that that have been used in literature for this problem, namely nearest neighbor, locally weighted regression, linear regression, and fully connected neural network. EBRFG gives the best prediction accuracy by far, compared to other models. In addition we also plot the "desirabilities" learnt by our model (Figure&#XA0;1(bottom)). The plot shows that the model is actually able to learn the "desirabilities" of various areas in a way that is reflective of the real world situation.
References[1]S.&#XA0;Chakrabarti, B.&#XA0;Dom, and P.&#XA0;Indyk. Enhanced hypertext categorization using hyperlinks.
In. Proc. of ACM SIGMOD98, pages 307 &#X2013; 318, 1998.
[2]N.&#XA0;Friedman, L.&#XA0;Getoor, D.&#XA0;Koller, and A.&#XA0;Pfeffer. Learning probabilistic relational models.
In Proc. IJCAI99, pages 1300 &#X2013; 1309, 1999.
[3]J.&#XA0;M. Klienberg. Authoritative sources in a hyperlinked environment.
Journal of the ACM, 46(5):604 &#X2013; 632, 1999.
[4]D.&#XA0;Koller and A.&#XA0;Pfeffer. Probabilistic frame-based systems.
In Proc. AAAI98, pages 580 &#X2013; 587, 1998.
[5]J.&#XA0;Neville and D.&#XA0;Jensen. Iterative classification in relational data.
In Proc. AAAI00 Workshop on Learning Statistical Models From Relational Data, pages 13 &#X2013; 20, 2000.
[6]S.&#XA0;Slattery and T.&#XA0;Mitchell. Discovering test set regularities in relational domain.
In. Proc. ICML00, pages 895 &#X2013; 902, 2000.
[7]B.&#XA0;Taskar, P.&#XA0;Abbeel, and D.&#XA0;Koller. Discriminative probabilistic models for relational data.
Eighteenth Conference on Uncertainty on Machine Intelligence (UAI02), 2002.
Table 1:Prediction accuracies of various algorithms on the test set. Absolute Relative Forecasting Error(fe)was measured. The error(fei)on theithsample is defined asfei= |Ai&#X2212;Pri|/Ai, whereAiis the actual selling price andPriis the predicted price. Three performance quantities on the test set are reported; percentage of houses with a forecasting error of less than 5%, with less than 10% and with less 15%.
Model ClassModel< 5%<10%< 15%non-parametricNearest Neighbor25.4147.4464.72non-parametricLocally Weighted Regression32.9858.4675.21parametricLinear Regression26.5848.1165.12parametricFully Connected Neural Network33.7960.5576.47hybridRelational Factor Graph39.4765.7681.04-0.1in&#XA0;&#XA0;Figure 1:(Top)A typical Energy Based Relational Factor Graph showing the connections between three samples. The factorsExyzicapture the dependencies between the features of individual samples and their answer variableYi, as well as the dependence on local latent variablesdi. The factorsEzzicaptures the dependencies between the hidden variables of multiple samples. The connection to these two factors may exist only from a subset of samples that are related to samplei. When the energy of factorEzzis quadratic ind.
(Bottom)The color coded values of the desirability surface at the location of the test samples. For every test sample, the estimate of its desirability is computed and is color coded according to its value. Blue color implies low desirability and red color implies high desirability.
This document was translated from LATEX byHEVEA.
