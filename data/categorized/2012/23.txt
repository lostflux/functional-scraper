old id = 2946
Deep Learning’s Diminishing Returns - IEEE Spectrum
2012
https://spectrum.ieee.org/deep-learning-computational-cost

TopicsSectionsMoreFor IEEE MembersFor IEEE MembersIEEE SpectrumFollow IEEE SpectrumSupport IEEE SpectrumIEEE websites place cookies on your device to give you the best user experience. By using our websites, you agree to the placement of these cookies. To learn more, read our Privacy Policy.
Enjoy more free content and benefits by creating an accountSaving articles to read later requires an IEEE Spectrum accountThe Institute content is only available for membersDownloading full PDF issues is exclusive for IEEE MembersAccess toSpectrum's Digital Edition is exclusive for IEEE MembersFollowing topics is a feature exclusive for IEEE MembersAdding your response to an article requires an IEEE Spectrum accountCreate an account to access more content and features onIEEE Spectrum, including the ability to save articles to read later, download Spectrum Collections, and participate in conversations with readers and editors. For more exclusive content and features, considerJoining IEEE.
Join the world’s largest professional organization devoted to engineering and applied sciences and get access to all of Spectrum’s articles, archives, PDF downloads, and other benefits.
Learn more →Access Thousands of Articles — Completely FreeCreate an account and get exclusive content and features:Save articles, download collections,andtalk to tech insiders— all free! For full access and benefits,join IEEEas a paying member.
Deep Learning’s Diminishing ReturnsThe cost of improvement is becoming unsustainableDeep learning is nowbeing used to translate between languages, predicthow proteins fold,analyze medical scans, andplay games as complex as Go, to name just a few applications of a technique that is now becoming pervasive. Success in those and other realms has brought this machine-learning technique from obscurity in the early 2000s to dominance today.
Although deep learning's rise to fame is relatively recent, its origins are not. In 1958, back when mainframe computers filled rooms and ran on vacuum tubes, knowledge of the interconnections between neurons in the brain inspiredFrank Rosenblatt at Cornellto design the first artificial neural network, which he presciently described as a "pattern-recognizing device." But Rosenblatt's ambitions outpaced the capabilities of his era—and he knew it. Even his inaugural paper was forced to acknowledge the voracious appetite of neural networks for computational power, bemoaning that "as the number of connections in the network increases...the burden on a conventional digital computer soon becomes excessive."This article is part of our special report on AI, “The Great AI Reckoning.”Fortunately for such artificial neural networks—later rechristened "deep learning" when they included extra layers of neurons—decades ofMoore's Lawand other improvements in computer hardware yielded a roughly10-million-fold increasein the number of computations that a computer could do in a second. So when researchers returned to deep learning in the late 2000s, they wielded tools equal to the challenge.
These more-powerful computers made it possible to construct networks with vastly more connections and neurons and hence greater ability to model complex phenomena. Researchers used that ability to break record after record as they applied deep learning to new tasks.
While deep learning's rise may have been meteoric, its future may be bumpy. Like Rosenblatt before them, today's deep-learning researchers are nearing the frontier of what their tools can achieve. To understand why this will reshape machine learning, you must first understand why deep learning has been so successful and what it costs to keep it that way.
Deep learningis a modern incarnation of the long-running trend in artificial intelligence that has been moving from streamlined systems based on expert knowledge toward flexible statistical models. Early AI systems were rule based, applying logic and expert knowledge to derive results. Later systems incorporated learning to set their adjustable parameters, but these were usually few in number.
Today's neural networks also learn parameter values, but those parameters are part of such flexible computer models that—if they are big enough—they become universal function approximators, meaning they can fit any type of data. This unlimited flexibility is the reason why deep learning can be applied to so many different domains.
The flexibility of neural networks comes from taking the many inputs to the model and having the network combine them in myriad ways. This means the outputs won't be the result of applying simple formulas but instead immensely complicated ones.
For example, when the cutting-edge image-recognition systemNoisy Studentconverts the pixel values of an image into probabilities for what the object in that image is, it does so using a network with 480 million parameters. The training to ascertain the values of such a large number of parameters is even more remarkable because it was done with only 1.2 million labeled images—which may understandably confuse those of us who remember from high school algebra that we are supposed to have more equations than unknowns. Breaking that rule turns out to be the key.
Deep-learning models are overparameterized, which is to say they have more parameters than there are data points available for training. Classically, this would lead to overfitting, where the model not only learns general trends but also the random vagaries of the data it was trained on. Deep learning avoids this trap by initializing the parameters randomly and then iteratively adjusting sets of them to better fit the data using a method called stochastic gradient descent. Surprisingly, this procedure has been proven to ensure that the learned model generalizes well.
The success of flexible deep-learning models can be seen in machine translation. For decades, software has been used to translate text from one language to another. Early approaches to this problem used rules designed by grammar experts. But as more textual data became available in specific languages, statistical approaches—ones that go by such esoteric names as maximum entropy, hidden Markov models, and conditional random fields—could be applied.
Initially, the approaches that worked best for each language differed based on data availability and grammatical properties. For example, rule-based approaches to translating languages such as Urdu, Arabic, and Malay outperformed statistical ones—at first. Today, all these approaches have been outpaced by deep learning, which has proven itself superior almost everywhere it's applied.
So the good news is that deep learning provides enormous flexibility. The bad news is that this flexibility comes at an enormous computational cost. This unfortunate reality has two parts.
Extrapolating the gains of recent years might suggest that by 2025 the error level in the best deep-learning systems designed for recognizing objects in the ImageNet data set should be reduced to just 5 percent [top]. But the computing resources and energy required to train such a future system would be enormous, leading to the emission of as much carbon dioxide as New York City generates in one month [bottom].
SOURCE: N.C. THOMPSON, K. GREENEWALD, K. LEE, G.F. MANSOThe first part is true of all statistical models: To improve performance by a factor ofk,at leastk2more data points must be used to train the model. The second part of the computational cost comes explicitly from overparameterization. Once accounted for, this yields a total computational cost for improvement ofat leastk4. That little 4 in the exponent is very expensive: A 10-fold improvement, for example, would require at least a 10,000-fold increase in computation.
To make the flexibility-computation trade-off more vivid, consider a scenario where you are trying to predict whether a patient's X-ray reveals cancer. Suppose further that the true answer can be found if you measure 100 details in the X-ray (often called variables or features). The challenge is that we don't know ahead of time which variables are important, and there could be a very large pool of candidate variables to consider.
The expert-system approach to this problem would be to have people who are knowledgeable in radiology and oncology specify the variables they think are important, allowing the system to examine only those. The flexible-system approach is to test as many of the variables as possible and let the system figure out on its own which are important, requiring more data and incurring much higher computational costs in the process.
Models for which experts have established the relevant variables are able to learn quickly what values work best for those variables, doing so with limited amounts of computation—which is why they were so popular early on. But their ability to learn stalls if an expert hasn't correctly specified all the variables that should be included in the model. In contrast, flexible models like deep learning are less efficient, taking vastly more computation to match the performance of expert models. But, with enough computation (and data), flexible models can outperform ones for which experts have attempted to specify the relevant variables.
Clearly, you can getimproved performance from deep learning if you use more computing power to build bigger models and train them with more data. But how expensive will this computational burden become? Will costs become sufficiently high that they hinder progress?To answer these questions in a concrete way,we recently gathered datafrom more than 1,000 research papers on deep learning, spanning the areas of image classification, object detection, question answering, named-entity recognition, and machine translation. Here, we will only discuss image classification in detail, but the lessons apply broadly.
Over the years, reducing image-classification errors has come with an enormous expansion in computational burden. For example, in 2012AlexNet, the model that first showed the power of training deep-learning systems on graphics processing units (GPUs), was trained for five to six days using two GPUs. By 2018, another model,NASNet-A, had cut the error rate of AlexNet in half, but it used more than 1,000 times as much computing to achieve this.
Our analysis of this phenomenon also allowed us to compare what's actually happened with theoretical expectations. Theory tells us that computing needs to scale with at least the fourth power of the improvement in performance. In practice, the actual requirements have scaled with at least theninthpower.
This ninth power means that to halve the error rate, you can expect to need more than 500 times the computational resources. That's a devastatingly high price. There may be a silver lining here, however. The gap between what's happened in practice and what theory predicts might mean that there are still undiscovered algorithmic improvements that could greatly improve the efficiency of deep learning.
To halve the error rate, you can expect to need more than 500 times the computational resources.
As we noted, Moore's Law and other hardware advances have provided massive increases in chip performance. Does this mean that the escalation in computing requirements doesn't matter? Unfortunately, no. Of the 1,000-fold difference in the computing used by AlexNet and NASNet-A, only a six-fold improvement came from better hardware; the rest came from using more processors or running them longer, incurring higher costs.
Having estimated the computational cost-performance curve for image recognition, we can use it to estimate how much computation would be needed to reach even more impressive performance benchmarks in the future. For example, achieving a 5 percent error rate would require 1019billion floating-point operations.
Important workby scholars at the University of Massachusetts Amherst allows us to understand the economic cost and carbon emissions implied by this computational burden. The answers are grim: Training such a model would cost US $100 billion and would produce as much carbon emissions as New York City does in a month. And if we estimate the computational burden of a 1 percent error rate, the results are considerably worse.
Is extrapolating out so many orders of magnitude a reasonable thing to do? Yes and no. Certainly, it is important to understand that the predictions aren't precise, although with such eye-watering results, they don't need to be to convey the overall message of unsustainability. Extrapolating this waywouldbe unreasonable if we assumed that researchers would follow this trajectory all the way to such an extreme outcome. We don't. Faced with skyrocketing costs, researchers will either have to come up with more efficient ways to solve these problems, or they will abandon working on these problems and progress will languish.
On the other hand, extrapolating our results is not only reasonable but also important, because it conveys the magnitude of the challenge ahead. The leading edge of this problem is already becoming apparent. When Google subsidiaryDeepMindtrained its system to play Go, it wasestimated to have cost $35 million. When DeepMind's researchers designeda system to play theStarCraft IIvideo game, they purposefully didn't try multiple ways of architecting an important component, because the training cost would have been too high.
AtOpenAI, an important machine-learning think tank, researchers recently designed and trained a much-laudeddeep-learning language system called GPT-3at the cost of more than $4 million. Even though they made a mistake when they implemented the system, they didn't fix it, explaining simply in a supplement to their scholarly publication that "due to the cost of training, it wasn't feasible to retrain the model."Even businesses outside the tech industry are now starting to shy away from the computational expense of deep learning. A large European supermarket chain recently abandoned a deep-learning-based system that markedly improved its ability to predict which products would be purchased. The company executives dropped that attempt because they judged that the cost of training and running the system would be too high.
Faced with risingeconomic and environmental costs, the deep-learning community will need to find ways to increase performance without causing computing demands to go through the roof. If they don't, progress will stagnate. But don't despair yet: Plenty is being done to address this challenge.
One strategy is to use processors designed specifically to be efficient for deep-learning calculations. This approach was widely used over the last decade, as CPUs gave way to GPUs and, in some cases, field-programmable gate arrays and application-specific ICs (including Google'sTensor Processing Unit). Fundamentally, all of these approaches sacrifice the generality of the computing platform for the efficiency of increased specialization. But such specialization faces diminishing returns. So longer-term gains will require adopting wholly different hardware frameworks—perhaps hardware that is based on analog, neuromorphic, optical, or quantum systems. Thus far, however, these wholly different hardware frameworks have yet to have much impact.
We must either adapt how we do deep learning or face a future of much slower progress.
Another approach to reducing the computational burden focuses on generating neural networks that, when implemented, are smaller. This tactic lowers the cost each time you use them, but it often increases the training cost (what we've described so far in this article). Which of these costs matters most depends on the situation. For a widely used model, running costs are the biggest component of the total sum invested. For other models—for example, those that frequently need to be retrained— training costs may dominate. In either case, the total cost must be larger than just the training on its own. So if the training costs are too high, as we've shown, then the total costs will be, too.
And that's the challenge with the various tactics that have been used to make implementation smaller: They don't reduce training costs enough. For example, one allows for training a large network but penalizes complexity during training. Another involves training a large network and then "prunes" away unimportant connections. Yet another finds as efficient an architecture as possible by optimizing across many models—something called neural-architecture search. While each of these techniques can offer significant benefits for implementation, the effects on training are muted—certainly not enough to address the concerns we see in our data. And in many cases they make the training costs higher.
One up-and-coming technique that could reduce training costs goes by the name meta-learning. The idea is that the system learns on a variety of data and then can be applied in many areas. For example, rather than building separate systems to recognize dogs in images, cats in images, and cars in images, a single system could be trained on all of them and used multiple times.
Unfortunately, recent work byAndrei Barbuof MIT has revealed how hard meta-learning can be. He and his coauthors showed that even small differences between the original data and where you want to use it can severely degrade performance. They demonstrated that current image-recognition systems depend heavily on things like whether the object is photographed at a particular angle or in a particular pose. So even the simple task of recognizing the same objects in different poses causes the accuracy of the system to be nearly halved.
Benjamin Rechtof the University of California, Berkeley, and others made this point even more starkly, showing that even with novel data sets purposely constructed to mimic the original training data, performance drops by more than 10 percent. If even small changes in data cause large performance drops, the data needed for a comprehensive meta-learning system might be enormous. So the great promise of meta-learning remains far from being realized.
Another possible strategy to evade the computational limits of deep learning would be to move to other, perhaps as-yet-undiscovered or underappreciated types of machine learning. As we described, machine-learning systems constructed around the insight of experts can be much more computationally efficient, but their performance can't reach the same heights as deep-learning systems if those experts cannot distinguish all the contributing factors.
Neuro-symbolicmethods and other techniques are being developed to combine the power of expert knowledge and reasoning with the flexibility often found in neural networks.
Like the situation that Rosenblatt faced at the dawn of neural networks, deep learning is today becoming constrained by the available computational tools. Faced with computational scaling that would be economically and environmentally ruinous, we must either adapt how we do deep learning or face a future of much slower progress. Clearly, adaptation is preferable. A clever breakthrough might find a way to make deep learning more efficient or computer hardware more powerful, which would allow us to continue to use these extraordinarily flexible models. If not, the pendulum will likely swing back toward relying more on experts to identify what needs to be learned.
Special Report: The Great AI ReckoningREAD NEXT:How the U.S. Army Is Turning Robots Into Team PlayersOr see thefull reportfor more articles on the future of AI.
Neil C. Thompsonis a research scientist at MIT’s Computer Science and Artificial Intelligence Laboratory.
Very well written article. It matches what I’ve seen over the last few years in scaling deep learning. It feels like a lot of the blood has been squeezed out the turnup with respect to hardware optimization with deep learning optimize hardware that uses low precision arithmetic (16 bit floats) in massively parallel custom chips. So now we’re back to the basic scale of Moore’s law which has been sputtering lately. Instead of the 50-100X improvement every 10 years we were seeing in the 80’s, 90’s and early 2000’s were now getting about 20%/year or about 6x every 10 years, and continuing to slow. So I just don’t see this approach yielding much beyond what we’re currently seeing.
Another approach is to rethink the basic building blocks of ANNs like the formal neuron, which is what progress.ai has successfully done with incredible results that translate to sometimes thousandfold increases in speed and decreases in computational power needs.
Check US patents 9390373 (2016), 9619749 (2017) & 10423694 (2019).
Working to slash server power consumption by half at scale using novel cooling configurations, and combining that with heat reuse cases to further lower the cost and carbon footprint.
Practical Power Beaming Gets RealVideo Friday: Drone in a CageRemembering 1982 IEEE President Robert LarsonAcer Goes Big on Glasses-Free, 3D Monitors—Look Out, VRIs this what’s needed to bring augmented reality to the home office?Matthew S. Smithwrites IEEE Spectrum's Gizmo column and is a freelance consumer-tech journalist. An avid gamer, he is a former staff editor at Digital Trends and is particularly fond of wearables, e-bikes, all things smartphone, and CES, which he has attended every year since 2009.
Content creators are a key target for Acer’s glasses-free 3D.
Acer, the world’s fifth largest PC brand, wants to take the growing AR/VR market by the horns with its SpatialLabs glasses-free stereoscopic 3D displays.
First teased in 2021 in a variant of Acer’s ConceptD 7 laptop, the technology expands this summer in a pair of portable monitors, the SpatialLabs View and View Pro, and select Acer Predator gaming laptops. The launch is paired with artificial-intelligence-powered software for converting existing 2D content into stereoscopic 3D.
“We see a convergence of virtual and reality,” Jane Hsu,head of business Development for SpatialLabs, said in an interview. “It’s a different form for users to start interacting with a virtual world.” Glasses-free stereoscopic 3D isn’t new.
Evolutionary, not revolutionaryThe technology has powered several niche products and prototypes,such as Sony’s Spatial Reality Display, but its most famous debut was Nintendo’s 3DS portable game console.
The 3DS filtered two images through a display layer called a parallax barrier. This barrier controlled the angle an image reached the user’s eyes to create the 3D effect. Because angle was important, the 3DS used cameras that detected the user’s eyes and adjusted the image to compensate for viewing angle.
“The PC in 2022 is encountering a lot of problems.”—Jerry Kao, AcerAcer’s technology is similar. It also displays two images which are filtered through an “optical layer” and has cameras to track and compensate for the user’s viewing angle.
So, what’s different this time?“The fundamental difference is that the computing power is way different, and resolution is way different,” said Hsu. “The Nintendo, that was 800 by 240. In a sense, the technology is the same, but over time it has improved for a crystal-clear, high-resolution experience.”Resolution is important to this form of glasses-free 3D. Because it renders two images to create the 3D effect, the resolution of the display is cut in half on the horizontal axis when 3D is on. The 3DS cut resolution to 400 by 240 when 3D was on and blurry visuals werea common complaint among critics.
Acer’s SpatialLabs laptops and displays are a big improvement. Each provides native 4K (3,840 by 2,160 resolution) in 2D. That’s 43 times the pixel count of Nintendo’s 3DS. Turning 3D on shaves resolution to 1,920 by 2,160, which, while lower, is still sharper than that of a 27-inch 4K monitor.
Hsu says advancements in AI compute are also key. Partners like Nvidia and Intel can now accelerate AI in hardware, a feature that wasn’t common a half decade ago.
Acer has harnessed this for SpatialLabs GO, a software utility that can convert full-screen content from 2D to stereoscopic 3D. This should make SpatialLabs useful with a wider range of content. It can also help creators generate content for use in stereoscopic 3D by importing and converting existing assets.
A new angle on augmented realityAcer was a lead partner in Microsoft’s push for mixed-reality headsets. They were a flop, and their failure taught Acer hard lessons about how people approach AR/VR hardware in the real world.
“Acer spent a lot bringing VR headsets to market, but...it was not very successful,”Acer Co-COO Jerry Kao said in an interview. “There were limitations. It’s not comfortable, or it’s expensive, and you need space around you. So, we wanted to address this.”SpatialLabs is a complementary alternative. Creators can use Spatial Labs to achieve a 3D effect in their home office without pushing aside furniture. The Acer View Pro, meant for commercial use, may have a future in retail displays, a use that headsets can't address.
The View Pro display is built for use in kiosks and retail displays.
AcerMost of the SpatialLabs product line, including the ConceptD 7 laptop and View displays, lean toward creative professionals using programs like Maya and Blender to create 3D content. Acer says its software suite has “out-of-the-box support for all major file formats.” It recently added support for Datasmith, a plug-in used to import assets toEpic’s Unreal Engine.
But the technology is also coming to Predator gaming laptops for glasses-free stereoscopic 3D in select titles likeForza Horizon 5andThe Witcher 3: Wild Hunt. Gaming seems a natural fit given its history in Nintendo’s handheld, and Hsu thinks it will help attract mainstream attention.
“When the Turn 10 team [developer of the Forza Horizon series] saw what we had done withForza Horizon 5, they were like, ‘Wow, this is so great!’ ” said Hsu. “They said, ‘You know what? I think I can build the scene with even more depth.’ And this is just the beginning.”Does glasses-free 3D really stand a chance?SpatialLabs brings gains in resolution and performance, but it’s far from a surefire hit. Acer is the only PC maker currently pursuing the hardware. Going it alone won’t be easy.
“While the tech seems quite appealing, it will likely remain a niche product that’ll be used in rare instances by designers or developers rather than the average consumer,”Jitesh Ubrani, research manager at IDC, said in an email. He thinks Acer could find it difficult to deliver on price and availability, “both of which are tough to do for such a fringe technology.”I asked Hsu how Acer will solve these issues. “In a way he’s right, it is difficult. We’re building this ourselves,” said Hsu. “But also, the hardware is more mature.”Kao chimed in to say SpatialLabs will stand out in what might be weak year for home computers. “The PC in 2022 is encountering a lot of problems,” Kao said. He sees that as a motivation, not a barrier, for novel technology on the PC.
“Intel, Google, Microsoft, and a lot of people, they have technology,” said Kao. “But they don’t know how to leverage that technology in the product and deliver the experience to specific people. That is what Acer is good at.”DARPA Wants a Better, Badder Caspian Sea MonsterLiberty Lifter X-plane will leverage ground effectArguably, the primary job of any military organization is moving enormous amounts of stuff from one place to another as quickly and efficiently as possible. Some of that stuff is weaponry, but the vast majority are things that support that weaponry—fuel, spare parts, personnel, and so on. At the moment, the U.S. military has two options when it comes to transporting large amounts of payload. Option one is boats (a sealift), which are efficient, but also slow and require ports. Option two is planes (an airlift), which are faster by a couple of orders of magnitude, but also expensive and require runways.
To solve this, the Defense Advanced Research Projects Agency (DARPA) wants to combine traditional sealift and airlift with theLiberty Lifter program, which aims to “design, build, and flight test an affordable, innovative, and disruptive seaplane” that “enables efficient theater-range transport of large payloads at speeds far exceeding existing sea lift platforms.”DARPADARPA is asking for a design like this to take advantage of ground effect, which occurs when an aircraft’s wing deflects air downward and proximity to the ground generates a cushioning effect due to the compression of air between the bottom of the wing and the ground. This boosts lift and lowers drag to yield a substantial overall improvement in efficiency. Ground effect works on both water and land, but you can take advantage of it for only so long on land before your aircraft runs into something. Which is why oceans are the ideal place for these aircraft—or ships, depending on your perspective.
During the late 1980s, the Soviets (and later the Russians) leveraged ground effect in the design of a handful of awesomely bizarre ships and aircraft. There’s theVVA-14, which was also an airplane, along with the vehicle shown in DARPA’s video above, theLun-class ekranoplan, which operated until the late 1990s. The video clip really does not do this thing justice, so here’s a better picture, taken a couple of years ago:InstagramTheLun(only one was ever made) had a wingspan of 44 meters and was powered by eight turbojet engines. It flew about 4 meters above the water at speeds of up to 550 kilometers per hour, and could transport almost 100,000 kilograms of cargo for 2,000 km. It was based on an earlier, even larger prototype (the largest aircraft in the world at the time) that the CIA spotted in satellite images in 1967 and which seems to have seriously freaked them out. It was nicknamed the Caspian Sea Monster, and it wasn’t until the 1980s that the West understood what it was and how it worked.
In the mid 1990s, DARPA itself took a serious look at a stupendously large ground-effect vehicle of its own, theAerocon Dash 1.6 wingship. The concept image below is of a 4.5-million-kg vehicle, 175 meters long with a 100-meter wingspan, powered by 20 (!) jet engines:WikipediaWith a range of almost 20,000 km at over 700 km/h, the wingship could have carried 3,000 passengers or 1.4 million kg of cargo. By 1994, though, DARPA had decided that the potential billion-dollar project to build a wingship like this was too risky, and canceled the whole thing.
Less than 10 years later, Boeing’s Phantom Works started exploring an enormous ground-effect aircraft, thePelican Ultra Large Transport Aircraft. The Pelican would have been even larger than the Aerocon wingship, with a wingspan of 152 meters and a payload of 1.2 million kg—that’s about 178 shipping containers’ worth. Unlike the wingship, the Pelican would take advantage of ground effect to boost efficiency only in transit above water, but would otherwise use runways like a normal aircraft and be able to reach flight altitudes of 7,500 meters. Operating as a traditional aircraft and with an optimal payload, the Pelican would have a range of about 12,000 km. In ground effect, however, the range would have increased to 18,500 km, illustrating the appeal of designs like these. But Boeing dropped the project in 2005 to focus on lower cost, less risky options.
We’d be remiss if we didn’t at least briefly mention two other massive aircraft: theH-4 Hercules, the cargo seaplane built by Hughes Aircraft Co. in the 1940s, and theStratolaunch carrier aircraft, which features a twin-fuselage configuration that DARPA seems to be favoring in its concept video for some reason.
From the sound of DARPA’s announcement, they’re looking for something a bit more like the Pelican than the Aerocon Dash or theLun. DARPA wants the Liberty Lifter to be able to sustain flight out of ground effect if necessary, although it’s expected to spend most of its time over water for efficiency. It won’t use runways on land at all, though, and should be able to stay out on the water for 4 to 6 weeks at a time, operating even in rough seas—a significant challenge for ground-effect aircraft.
DARPA is looking for an operational range of 7,500 km, with a maximum payload of at least 90,000 kg, including the ability to launch and recover amphibious vehicles. The hardest thing DARPA is asking for could be that, unlike most other X-planes, the Liberty Lifter should incorporate a “low cost design and construction philosophy” inspired by the mass-produced Liberty ships of World War II.
With US $15 million to be awarded to up to two Liberty Lifter concepts, DARPA is hoping that at least one of those concepts will pass a system-level critical design review in 2025. If everything goes well after that, the first flight of a full-scale prototype vehicle could happen as early as 2027.
Edge Learning: Adapting to a Changing Environment at the EdgeGoing above and beyond the call of duty at the edgeEdge Learning is the capability of an edge device to adapt and learn to new data points/objects that have not been part of its initial training dataset. AlphaICs, a leading AI company developed edge learning PoC with a grant from a US Government Research Organization.
Trending StoriesPractical Power Beaming Gets RealDARPA Wants a Better, Badder Caspian Sea MonsterSimple, Cheap, and Portable: A Filter-Free Desalination System for a Thirsty WorldVideo Friday: Drone in a CageAcer Goes Big on Glasses-Free, 3D Monitors—Look Out, VRAndrew Ng: Unbiggen AIHydrogen Helps Make Topological Insulators PracticalBefore Ships Used GPS, There Was the Fresnel Lens
