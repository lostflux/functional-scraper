old id = 2676
Neural interface translates thoughts into type
2017
https://www.nature.com/articles/d41586-021-00776-8

Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript.
AdvertisementNeural interface translates thoughts into typePavithra Rajeswaran is in the Department of Bioengineering, University of Washington, Seattle, Washington 98195, USA.
You can also search for this author inPubMedGoogle ScholarAmy L. Orsborn is in the Department of Bioengineering, University of Washington, Seattle, Washington 98195, USA; and in the Department of Electrical and Computer Engineering, and at the Washington National Primate Research Center, University of Washington.
You can also search for this author inPubMedGoogle ScholarYou have full access to this article via your institution.
We can think much faster than we can communicate — a fact that many of us feel aware of as we struggle with our smartphone keyboards. For people with severe paralysis, this information bottleneck is much more extreme. Willettet al.
1report ina paper inNaturethe development of a brain–computer interface (BCI) for typing that could eventually let people with paralysis communicate at the speed of their thoughts.
Read the paper: High-performance brain-to-text communication via handwritingCommercially available assistive typing devices predominantly rely on the person using the device being able to make eye movements or deliver voice commands. Eye-tracking keyboards can let people with paralysis type at around 47.5 characters per minute2, slower than the 115-per-minute speeds achieved by people without a comparable injury. However, these technologies do not work for people whose paralysis impairs eye movements or vocalization. And the technology has limitations. For instance, it is hard to reread an e-mail, so that you can compose your reply, while you are typing with your eyes.
By contrast, BCIs restore function by deciphering patterns of brain activity. Such interfaces have successfully restored simple movements — such as reaching for and manipulating large objects — to people with paralysis3–7. By directly tapping into neural processing, BCIs hold the tantalizing promise of seamlessly restoring function to a wide range of people.
But, so far, BCIs for typing have been unable to compete with simpler assistive technologies such as eye-trackers. One reason is that typing is a complex task. In English, we select from 26 letters of the Latin alphabet. Building a classification algorithm to predict which letter a user wants to choose, on the basis of their neural activity, is challenging, so BCIs have solved typing tasks indirectly. For instance, non-invasive BCI spellers present several sequential visual cues to the user and analyse the neural responses to all cues to determine the desired letter8. The most successful invasive BCI (iBCI; one that involves implanting an electrode into the brain) for typing allowed users to control a cursor to select keys, and achieved speeds of 40 characters per minute6. But these iBCIs, like non-invasive eye-trackers, occupy the user’s visual attention and do not provide notably faster typing speeds.
Willett and colleagues developed a different approach, which directly solves the typing task in an iBCI and thereby leapfrogs far beyond past devices, in terms of both performance and functionality. The approach involves decoding letters as users imagine writing at their own pace (Fig. 1).
Figure 1 | A brain–computer interface (BCI) for typing.
Willettet al.
1have developed a BCI that enables a person with paralysis to type, by translating the neural activity produced from imagined attempts at handwriting into text on the computer screen. As a simplified description, electrodes implanted into the brain measure the activity of many neurons as the user imagines writing each letter (lines indicate time points at which each neuron fires). A deep-learning model called a recurrent neural network (RNN) learns the neural activity patterns produced from each character, and analyses how these activity patterns relate across multiple trials, generating cluster plots. This information is used to by an algorithm to predict the letters being imagined by the participant in the current trial, and the prediction is translated into a typographic output. (Figure adapted from Fig. 2a of ref. 1.)Such an approach required a classification algorithm that predicts which of 26 letters or 5 punctuation marks a user with paralysis is trying to write — a challenging feat when the attempts cannot be observed and occur whenever the user chooses. To overcome this challenge, Willettet al.
first repurposed another type of algorithm — a machine-learning algorithm originally developed for speech recognition. This allowed them to estimate, on the basis of neural activity alone, when a user started attempting to write a character. The pattern of neural activity generated each time their study participant imagined a given character was remarkably consistent. From this information, the group produced a labelled data set that contained the neural-activity patterns corresponding to each character. They used this data set to train the classification algorithm.
To achieve accurate classification in such a high-dimensional space, Willett and colleagues’ classification algorithm used current machine-learning methods, along with a type of artificial neural network called a recurrent neural network (RNN), which is especially good at predicting sequential data. Harnessing the power of RNNs requires ample training data, but such data are limited in neural interfaces, because few users want to imagine writing for hours on end. The authors solved this problem using an approach known as data augmentation, in which neural activity patterns previously generated by the participant are used to produce artificial sentences on which to train the RNN. They also expanded their training data by introducing artificial variability into the patterns of neural activity, to mimic changes that occur naturally in the human brain. Such variability can make RNN BCIs more robust9.
Neuroprosthetic device maintains blood pressure after spinal-cord injuryThanks to these methods, Willett and colleagues’ algorithm provided impressively accurate classification, picking the correct character 94.1% of the time. By including predictive-language models (similar to those that drive auto-correct functions on a smartphone), they further improved accuracy to 99.1%. The participant was able to type accurately at a speed of 90 characters per minute — a twofold improvement on his performance with past iBCIs.
This study’s achievements, however, stem from more than machine learning. A decoder’s performance is ultimately only as good as the data that are fed into it. The researchers found that neural data associated with attempted handwriting are particularly well-suited for typing tasks and classification. In fact, handwriting could be classified quite well even with simpler, linear algorithms, suggesting that the neural data themselves played a large part in the success of the authors’ approach.
By simulating how the classification algorithm performed when tested with different types of neural activity, Willettet al.
made a key insight — neural activity during handwriting has more temporal variability between characters than does neural activity when users attempt to draw straight lines, and this variablility actually makes classification easier. This knowledge should inform future BCIs. Perhaps counter-intuitively, it might be advantageous to decode complex behaviours rather than simple ones, particularly for classification tasks.
Brain implants that let you speak your mindWillett and co-workers’ study begins to deliver on the promise of BCI technologies. iBCIs will need to provide tremendous performance and usability benefits to justify the expense and risks associated with implanting electrodes into the brain. Importantly, typing speed is not the only factor that will determine whether the technology is adopted — the longevity and robustness of the approach also require analysis. The authors present promising evidence that their algorithms will perform well with limited training data, but further research will probably be required to enable the device to maintain performance over its lifetime as neural activity patterns change. It will also be crucial to conduct studies to test whether the approach can be generalized for other users, and for settings outside the laboratory.
Another question is how the approach will scale and translate to other languages. Willett and colleagues’ simulations highlight that several characters of the Latin alphabet are written similarly (r, v and u, for instance), and so are harder to classify than are others. One of us (P.R.) speaks Tamil, which has 247, often very closely related, characters, and so might be much harder to classify. And the question of translation is particularly pertinent for languages that are not yet well represented in machine-learning predictive-language models.
Although much work remains to be done, Willett and co-workers’ study is a milestone that broadens the horizon of iBCI applications. Because it uses machine-learning methods that are rapidly improving, plugging in the latest models offers a promising path for future improvements. The team is also making its data set publicly available, which will accelerate advances. The authors’ approach has brought neural interfaces that allow rapid communication much closer to a practical reality.
Nature593, 197-198 (2021)doi: https://doi.org/10.1038/d41586-021-00776-8ReferencesWillett, F. R., Avansino, D. T., Hochberg, L. R., Henderson, J. M. & Shenoy, K. V.
Nature593, 249–254 (2021).
ArticleGoogle ScholarMott, M. E., Williams, S., Wobbrock, J. O. & Morris, M. R. inProc. 2017 CHI Conf. Human Factors in Computing Systems2558–2570 (ACM, 2017).
Google ScholarHochberg, L. R.
et al.
Nature442, 164–171 (2006).
PubMedArticleGoogle ScholarHochberg, L. R.
et al.
Nature485, 372–375 (2012).
PubMedArticleGoogle ScholarCollinger, J. L.
et al.
Lancet381, 557–564 (2013).
PubMedArticleGoogle ScholarPandarinath, C.
et al.
eLife6, e18554 (2017).
PubMedArticleGoogle ScholarAjiboye, A. B.
et al.
Lancet389, 1821–1830 (2017).
PubMedArticleGoogle ScholarRezeika, A.
et al.
Brain Sci.
8, 57 (2018).
ArticleGoogle ScholarSussillo, D., Stavisky, S. D., Kao, J. C., Ryu, S. I. & Shenoy, K. V.
Nature Commun.
7, 13749 (2016).
PubMedArticleGoogle ScholarDownload referencesRelated ArticlesRead the paper: High-performance brain-to-text communication via handwritingNeuroprosthetic device maintains blood pressure after spinal-cord injuryBrain implants that let you speak your mindSee all News & ViewsSubjectsLatest on:Emergent reliability in sensory cortical coding and inter-area communicationArticle19 MAY 22Volatile neurons unite to stabilize visual experienceNews & Views19 MAY 22Exercise spurs the brain to make more mood-boosting hormoneResearch Highlight16 MAY 22Starting a scientific career with narcolepsyCareer Column20 MAY 22Young cerebrospinal fluid improves memory in old miceNews & Views11 MAY 22A switch in neuronal dynamics that helps to initiate movementNews & Views28 APR 22First pig kidneys transplanted into people: what scientists thinkNews19 MAY 22Metabolic diversity drives cancer cell invasionNews & Views18 MAY 22PHGDH heterogeneity potentiates cancer cell dissemination and metastasisArticle18 MAY 22JobsPhD Student in Molecular MR ImagingGerman Cancer Research Center in the Helmholtz Association (DKFZ)Heidelberg, GermanyPhD candidate for protein engineering of imaging tools (f/m/x)Helmholtz MunichNeuherberg (bei München), GermanyPhD candidate for protein engineering of imaging tools (f/m/x)Helmholtz MunichNeuherberg (bei München), GermanyPhD candidate for the development of instrumentation for high-resolution Optoacoustic whole animal imaging (f/m/x)Helmholtz MunichNeuherberg (bei München), GermanyYou have full access to this article via your institution.
Related ArticlesRead the paper: High-performance brain-to-text communication via handwritingNeuroprosthetic device maintains blood pressure after spinal-cord injuryBrain implants that let you speak your mindSee all News & ViewsSubjectsSign up to Nature BriefingAn essential round-up of science news, opinion and analysis, delivered to your inbox every weekday.
Sign up for theNature Briefingnewsletter — what matters in science, free to your inbox daily.
Explore contentAbout the journalPublish with usSearchAdvanced searchQuick linksNature (Nature)ISSN1476-4687(online)ISSN0028-0836(print)nature.com sitemapDiscover contentPublishing policiesAuthor & Researcher servicesLibraries & institutionsAdvertising & partnershipsCareer developmentRegional websitesLegal & Privacy© 2022 Springer Nature Limited
