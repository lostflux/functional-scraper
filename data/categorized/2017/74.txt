old id = 3636
AlphaGo, in context. Update Oct 18, 2017: AlphaGo Zero was… | by Andrej Karpathy | Medium
2017
https://medium.com/@karpathy/alphago-in-context-c47718cb95a5

May 31, 2017AlphaGo, in contextUpdate Oct 18, 2017:AlphaGo Zerowas announced. This post refers to the previous version. 95% of it still applies.
I had a chance to talk to several people about the recentAlphaGo matcheswith Ke Jie and others. In particular, most of the coverage was a mix of popular science + PR so the most common questions I’ve seen were along the lines of “to what extent is AlphaGo a breakthrough?”, “How do researchers in AI see its victories?” and “what implications do the wins have?”. I thought I might as well serialize some of my thoughts into a post.
The cool partsAlphaGo is made up of a number of relatively standard techniques: behavior cloning (supervised learning on human demonstration data), reinforcement learning (REINFORCE), value functions, and Monte Carlo Tree Search (MCTS). However, the way these components are combined is novel and not exactly standard. In particular, AlphaGo uses a SL (supervised learning) policy to initialize the learning of an RL (reinforcement learning) policy that gets perfected with self-play, which they then estimate a value function from, which then plugs into MCTS that (somewhat surprisingly) uses the (worse!, but more diverse) SL policy to sample rollouts. In addition, the policy/value nets are deep neural networks, so getting everything to work properly presents its own unique challenges (e.g. value function is trained in a tricky way to prevent overfitting). On all of these aspects, DeepMind has executed very well. That being said, AlphaGo does not by itself use any fundamental algorithmicbreakthroughsin how we approach RL problems.
On narrownessZooming out, it is also still the case that AlphaGo is a narrow AI system that can play Go and that’s it. The ATARI-playing agents from DeepMind do not use the approach taken with AlphaGo. The Neural Turing Machine has little to do with AlphaGo. The Google datacenter improvements definitely do not use AlphaGo. The Google Search engine is not going to use AlphaGo. Therefore, AlphaGo does not generalize to any problem outside of Go, but the people and the underlying neural network components do, and do so much more effectively than in the days of old AI where each demonstration needed repositories of specialized, explicit code.
Convenient properties of GoI wanted to expand on the narrowness of AlphaGo by explicitly trying to list some of the specific properties that Go has, which AlphaGo benefits a lot from. This can help us think about what settings AlphaGo does or does not generalize to. Go is:Example: AlphaGo applied to robotics?Having enumerated some of the appealing properties of Go, let’s look at a robotics problem and see how well we could apply AlphaGo to, for example, anAmazon Picking Challengerobot. It’s a little comical to even think about.
In short, basically every single assumption that Go satisfies and that AlphaGo takes advantage of are violated, and any successful approach would look extremely different. More generally, some of Go’s properties above are not insurmountable with current algorithms (e.g. 1,2,3), some are somewhat problematic (5,7), but some are quite critical to how AlphaGo is trained but are rarely present in other real-world applications (4,6).
In conclusionWhile AlphaGo does not introduce fundamental breakthroughs in AI algorithmically, and while it is still an example of narrow AI, AlphaGo does symbolize Alphabet’s AI power: in both the quantity/quality of the talent present in the company, the computational resources at their disposal, and the all in focus on AI from the very top.
Alphabet is making a large bet on AI, and it is a safe one. But I’m biased :)EDIT: the goal of this post is, as someone on reddit mentioned, “quelling the ever resilient beliefs of the public that AGI is right down the road”, and the target audience are people outside of AI who were watching AlphaGo and would like a more technical commentary.
--17----17More from Andrej KarpathyI like to train deep neural nets on large datasets.
Love podcasts or audiobooks? Learn on the go with our new app.
Recommended from MediumeBay TechBlogineBayTechJaime DuráninyottabytesSharif NatheirinThe StartupMegan StanleyT.M. Robinsonalok rawatJoanna PurostoDexterinRun DexterAboutHelpTermsPrivacyGet the Medium appAndrej Karpathy44K FollowersI like to train deep neural nets on large datasets.
More from MediumTa-Ying ChenginTowards Data ScienceMostafa IbrahiminThe TechlifeDamin LeeAnishHelpStatusWritersBlogCareersPrivacyTermsAboutKnowable
