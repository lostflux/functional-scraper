old id = 634
An AI app that “undressed” women shows how deepfakes harm the most vulnerable | MIT Technology Review
2016
https://www.technologyreview.com/s/613898/an-ai-app-that-undressed-women-shows-how-deepfakes-harm-the-most-vulnerable

An AI app that “undressed” women shows how deepfakes harm the most vulnerableThe attention around deepfakes and synthetic mediahas growninrecent months. But while the conversation has primarily focused on its potential impact onpolitics, several experts in human rights and tech ethics have warned that another potential harm has been overlooked: the possibly devastating consequences for women and other vulnerable populations who are targeted with the technology but cannot protect themselves.
Now the latest deepfake experiment—an app called DeepNude that “undressed” photos of women—is playing out those nightmares.
First reported by Vice, it usedgenerative adversarial networks, or GANs, to swap the women’s clothes for highly realistic nude bodies. The article quickly inspired a viral backlash, and the app’s creator shut it down.
“The DeepNude app proves our worst fears about the unique way audiovisual tools can be weaponized against women,” says Mutale Nkonde, a fellow at the Data & Society Research Institute, who advised a bill introduced in Congress by Representative Yvette Clarke that would create mechanisms for victims of such malicious deepfakes to seek legal recourse for reputational damage.
The app specifically targeted women. Vice found that the software only generated images of the female body, even when given a picture of a man. The anonymous creator confirmed that he had trained the GAN algorithm only on nude photos of women—in this case more than 10,000 of them—because they were easier to find online. He did, however, also intend to eventually make a male version.
Though the deepfakes didn’t depict the women’s actual bodies—they are completely synthesized by the algorithm—they still had the potential to cause significant emotional and reputational damage. The images could easily be mistaken for the real thing and used as revenge porn or a powerful tool for silencing women. In fact, this has happened before: afemale journalist in Indiahad her face grafted onto a porn video after she began uncovering government corruption. It instantly went viral, subjecting her to intense harassment and rape threats, and she had to go offline for several months.
Deepfakes are not a new threat; manipulated media has been around since long before AI. But the technology has accelerated and broadened existing trends, says Sam Gregory, program director of the human rights nonprofit Witness. Algorithms have made it much easier for far more people to generate ever more convincing fake media. Thus, anything that people used manipulated media to do in the past, such as attack journalists, imply corruption, or obfuscate evidence, will become increasingly commonplace and dangerously difficult to detect.
The app is no different, he says. The image-based sexual abuse of women already existed as a problem. Now deepfakes are adding fuel to the flames.
By the same logic, Nkonde worries that women won’t be the only vulnerable targets of deepfakes. Minorities, LGBTQ folks, and other groups often subject to the most severe online harassment will likely become victims too—though perhaps in different ways. During the 2016 US presidential campaign, for example, Russian operativesused fake African-American personas and related imageryas part of a Facebook disinformation campaign to heighten racial tensions among Americans.
“This was a new way for voter suppression, and it was through misappropriating people’s identities online,” Nkonde says. Deepfake technology would be another natural tool for malicious actors pretending to be people they are not to disrupt communities and otherwise cause harm.
So where do we go from here? Both Nkonde and Gregory haveshared similar recommendationswith MIT Technology Review in the past: companies and researchers who produce tools for deepfakes must also invest in countermeasures, and social-media and search companies should integrate those countermeasures directly into their platforms. Nkonde also urges regulators to act quickly. “Unless government finds a way to protect the rights of consumers, apps like this are going to proliferate,” she says.
“Technology is not neutral,” says Gregory. “This [DeepNude] app is not dual use. It’s single use for a malicious purpose, and it is being created amorally.
“We need to really focus on the ethics of creating and sharing generative media tools,” he adds. “We should repeatedly call this out.”byKaren HaoSharePopularDeep DiveTech policySouth Africa’s private surveillance machine is fueling a digital apartheidAs firms have dumped their AI technologies into the country, it’s created a blueprint for how to surveil citizens and serves as a warning to the world.
A quick guide to the most important AI law you’ve never heard ofThe European Union is planning new legislation aimed at curbing the worst harms associated with artificial intelligence.
Inside the fierce, messy fight over “healthy” sugar techYi-Heng “Percival” Zhang was a leader in rare sugar research. Then things got sticky.
The secret police: Inside the app Minnesota police used to collect data on journalists at protestsIntrepid Response is a little-known but powerful app that lets police quickly upload and share information across agencies. But what happens to the information it collects?Stay connectedGet the latest updates fromMIT Technology ReviewDiscover special offers, top stories, upcoming events, and more.
Thank you for submitting your email!It looks like something went wrong.
We’re having trouble saving your preferences. Try refreshing this page and updating them one more time. If you continue to get this message, reach out to us atcustomer-service@technologyreview.comwith a list of newsletters you’d like to receive.
MIT Technology ReviewOur in-depth reporting reveals what’s going on now to prepare you for what’s coming next.
Subscribeto support our journalism.
AboutHelp© 2022 MIT Technology Review
