old id = 1187
Safety Critical AI - Partnership on AI
2022
https://new.partnershiponai.org/program/safety-critical-ai

Safety Critical AIOverviewHow can we ensure that AI and machine learning technologies are safe? This is an urgent short-term question, with applications in computer security, medicine, transportation, and other domains. It is also a pressing longer-term question, particularly with regard to environments that are uncertain, unanticipated, and potentially adversarial.
ImpactTracking When AI Systems FailOur Safety-Critical AI WorkAt NeurIPS 2020, PAI co-hosted a workshopaddressing open questions about the responsible oversight of novel AI research. Encouraging participants to think critically about how the AI research community can anticipate and mitigate potential negative consequences, the event included panel discussions on impact statements, publication norms, harms of AI, and more.
And in October 2020, PAI releaseda new competitive benchmarkfor SafeLife, a novel AI learning environment that tests the safety of reinforcement learning agents and the algorithms that train them. As reinforcement learning agents get deployed to more complex and safety-critical situations, it’s increasingly important that we are able to measure and improve safety.
UpdatesWhat Responsible AI Can Learn From the Race to Fix Meltdown and SpectreNavigating the Broader Impacts of AI Research: Workshop at NeurIPS 2020What the AI Community Can Learn From Sneezing Ferrets and a Mutant Virus DebateWhen AI Systems Fail: Introducing the AI Incident DatabaseIntroducing the SafeLife Leaderboard: A Competitive Benchmark for Safer AIMultistakeholder Approaches to Explainable Machine LearningExplainable AI in Practice Falls Short of Transparency GoalsIntroducing SafeLife: Safety Benchmarks for Reinforcement LearningProgram WorkstreamsExplainable AI in PracticePublication Norms for Responsible AIAI Incidents DatabaseSafeLife: AI Safety in Complex Environments© 2022 Partnership on AI | All Rights Reserved
