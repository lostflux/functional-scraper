old id = 662
Meta's new learning algorithm can teach AI to multi-task | MIT Technology Review
2022
https://www.technologyreview.com/2022/01/20/1043885/meta-ai-facebook-learning-algorithm-nlp-vision-speech-agi

Meta’s new learning algorithm can teach AI to multi-taskThe single technique for teaching neural networks multiple skills is a step towards general-purpose AI.
If you can recognize a dog by sight, then you can probably recognize a dog when it is described to you in words. Not so for today’sartificial intelligence. Deep neural networks have become very good at identifying objects in photos and conversing in natural language, but not at the same time: there are AI models that excel at one or the other, but not both.
Part of the problem is that these models learn different skills using different techniques. This is a major obstacle for the development of more general-purpose AI,machines that can multi-task and adapt. It also means that advances in deep learning for one skill often do not transfer to others.
Related StoryHuman intelligence emerges from our combination of senses and language abilities. Maybe the same is true for artificial intelligence.
A team at Meta AI (previously Facebook AI Research) wants to change that. The researchers have developed a single algorithm that can be used to train a neural network to recognize images, text, or speech. The algorithm, calledData2vec, not only unifies the learning process but performs at least as well as existing techniques in all three skills. “We hope it will change the way people think about doing this type of work,” says Michael Auli, a researcher at Meta AI.
The research builds on an approach known as self-supervised learning, in which neural networks learn to spot patterns in data sets by themselves, without being guided by labeled examples. This is how large language models like GPT-3 learn from vast bodies of unlabeled text scraped from the internet, and it has driven many of the recent advances in deep learning.
Auli and his colleagues at Meta AI had been working on self-supervised learning for speech recognition. But when they looked at what other researchers were doing with self-supervised learning for images and text, they realized that they were all using different techniques to chase the same goals.
Data2vec uses two neural networks, a student and a teacher. First, the teacher network is trained on images, text, or speech in the usual way, learning an internal representation of this data that allows it to predict what it is seeing when shown new examples. When it is shown a photo of a dog, it recognizes it as a dog.
The twist is that the student network is then trained to predict the internal representations of the teacher. In other words, it is trained not to guess that it is looking at a photo of a dog when shown a dog, but to guess what the teacher sees when shown that image.
Because the student does not try to guess the actual image or sentence but, rather, the teacher’s representation of that image or sentence, the algorithm does not need to be tailored to a particular type of input.
Related StoryA machine that could think like a person has been the guiding vision of AI research since the earliest days—and remains its most divisive idea.
Data2vec is part of a big trend in AI towardmodels that can learn to understand the world in more than one way. “It’s a clever idea,” says Ani Kembhavi at the Allen Institute for AI in Seattle, who works on vision and language. “It’s a promising advance when it comes to generalized systems for learning.”An important caveat is that although the same learning algorithm can be used for different skills, it can only learn one skill at a time. Once it has learned to recognize images, it must start from scratch to learn to recognize speech. Giving an AI multiple skills at once is hard, but that’s something the Meta AI team wants to look at next.
The researchers were surprised to find that their approach actually performed better than existing techniques at recognizing images and speech, and performed as well as leading language models on text understanding.
Mark Zuckerberg is alreadydreaming up potential metaverse applications. "This will all eventually get built into AR glasses with an AI assistant," he posted to Facebook today. "It could help you cook dinner, noticing if you miss an ingredient, prompting you to turn down the heat, or more complex tasks."For Auli, the main takeaway is that researchers should step out of their silos. “Hey, you don’t need to focus on one thing,” he says. “If you have a good idea, it might actually help across the board.”byWill Douglas HeavenSharePopularDeep DiveArtificial intelligenceArtificial intelligence is creating a new colonial world orderAn MIT Technology Review series investigates how AI is enriching a powerful few by dispossessing communities that have been dispossessed before.
Meta has built a massive new language AI—and it’s giving it away for freeFacebook’s parent company is inviting researchers to pore over and pick apart the flaws in its version of GPT-3This horse-riding astronaut is a milestone in AI’s journey to make sense of the worldOpenAI’s latest picture-making AI is amazing—but raises questions about what we mean by intelligence.
How the AI industry profits from catastropheAs the demand for data labeling exploded, an economic catastrophe turned Venezuela into ground zero for a new model of labor exploitation.
Stay connectedGet the latest updates fromMIT Technology ReviewDiscover special offers, top stories, upcoming events, and more.
Thank you for submitting your email!It looks like something went wrong.
We’re having trouble saving your preferences. Try refreshing this page and updating them one more time. If you continue to get this message, reach out to us atcustomer-service@technologyreview.comwith a list of newsletters you’d like to receive.
MIT Technology ReviewOur in-depth reporting reveals what’s going on now to prepare you for what’s coming next.
Subscribeto support our journalism.
AboutHelp© 2022 MIT Technology Review
