old id = 1343
7 outstanding papers at ICLR 2022
2022
https://analyticsindiamag.com/7-outstanding-papers-at-iclr-2022

7 outstanding papers at ICLR 2022AdvertisementThe International Conference on Learning Representations (ICLR) has announced theICLR 2022 Outstanding Paper Awards. The selection committee consisted ofAndreas Krause(ETH-Zurich),Atlas Wang(UT Austin),Been Kim(Google Brain),Bo Li(University of Illinois Urbana-Champaign),Bohyung Han(Seoul National University),He He(New York University), andZaid Harchaoui(University of Washington).
The outstanding papers are listed below:THE BELAMYSign up for your weekly dose of what's up in emerging technology.
Analytic-DPMAuthorsDiffusion probabilistic models (DPMs)– first proposed bySohl-Dickstein et al., 2015–fall under the class of generative models. The problem comes in with the inference of DPMs as it is too expensive since it requires iteration over thousands of timesteps. It needs to estimate the variance in each timestep of the reverse process. Till now, most of the work done on this uses a handcrafted value for all timesteps (Nichol & Dhariwal, 2021).
Here, the researchers of the paper titled,Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models, have proposed Analytic-DPM, a “training-free inference framework that estimates the analytic forms of the variance andKullback–Leibler divergence(KL divergence) using the Monte Carlo method and a pretrained score-based model.” The researchers also said Analytic-DPM applies to various DPMs likeHo et al., 2020;Song et al.,2020a;Nichol & Dhariwal, 2021) in a plug-and-play manner. Analytic-DPM improves the log-likelihood of various DPMs, produces high-quality samples, and also produces 20× to 80× speed.
To read the paper, clickhere.
Hyperparameter Tuning with Renyi Differential PrivacyAuthorsNicolas Papernot– Google Research, Brain TeamThomas Steinke-Google Research, Brain TeamDifferential privacyis a system for publicly sharing information about a dataset by disclosing patterns in the groups but not revealing information about individual entities in the dataset.
Noisy (stochastic) gradient descent is a popular method for ensuring differential privacy (Song et al., 2013;Bassily et al., 2014;Abadi et al., 2016). In the paper titled,Hyperparameter Tuning with Renyi Differential Privacy, the researchers pointed out that DP-SGD differs from the standard gradient.
Due to these differences, it bounds the sensitivity of each update so that the added noise ensures differential privacy. The researchers showed how setting hyperparameters based on non-private training runs could leak private information. The team also provided privacy guarantees for hyperparameter search procedures within the framework ofRenyi Differential Privacy. The results improve on and extend the work ofLiu and Talwar (STOC 2019).
To read the paper, clickhere.
Learning Strides in Convolutional Neural NetworksCNNs have wide applications in image and text classification, speech recognition, translation etc. The paper,Learning Strides in Convolutional Neural Networks, addresses a critical issue while using CNNs – setting the strides in a principled way instead of trials and errors.
What do you mean by Stride?DeepAIdefines “stride” as a neural network’s filter parameter that modifies the amount of movement over the image or video.
Inspired by the work titledSpectral Representations for Convolutional Neural Networks, the researchers proposed “DiffStride, the first downsampling layer with learnable strides”. Instead of cropping with a fixed bounding box controlled by a striding hyperparameter, DiffStride learns the size of its cropping box through backpropagation.
Expressiveness and Approximation Properties of Graph Neural NetworksAuthorsGNN architectures are characterised by the separation power of graph algorithms such as color refinement (CR) andk-dimensional Weisfeiler-Leman tests(k-WL). Understanding the separation power of a given GNN architecture requires complex proofs focused on the specifics of the architecture.
In the paper titled,Expressiveness and Approximation Properties of Graph Neural Networks, the researchers proposed a tensor language-based technique to analyse the separation power of general GNNs. The approach also provides a toolbox with which GNN architecture designers can analyse the separation power of their GNNs without the need to figure out the intricacies of the WL-tests.
Comparing Distributions by Measuring Differences that Affect Decision MakingAuthorsQuantifying the discrepancy between two probability distributions is a huge challenge in machine learning. The paper,Comparing Distributions by Measuring Differences that Affect Decision Making, introduces a new class of discrepancies based on the optimal loss for a decision task. By suitably choosing the decision task, it generalises theJensen Shannon divergenceand the maximum mean discrepancy family.
By applying this approach to two-sample tests and various benchmarks, the team has achieved superior test power compared to competing methods.
Neural Collapse Under MSE LossAuthorsDavid L. Donoho-Stanford UniversityX.Y. Han,- Cornell UniversityVardan Papyan– University of TorontoThe paper titledNeural Collapse Under MSE Loss: Proximity to and Dynamics on the Central Pathsays during the neural collapse, last-layer features collapse to their class-means while the class-means collapse to the sameSimplex Equiangular Tight Frame. The classifier behaviour collapses to the nearest-class-mean decision rule.
The paper proposes a new theoretical construct of “central path”, where the linear classifier stays MSE-optimal for feature activations throughout the dynamics.
Bootstrapped Meta-LearningAuthorsMeta-learning essentially means ‘learning to learn‘. The paper titled,Bootstrapped Meta-Learning, outlines a few challenges that crop up in meta-learning. Meta-learning is challenging because it must first be applied to evaluate an update rule. And it comes with high computational costs. Several challenges in meta-optimisation degrade the performance. The researchers have proposed an algorithm that lets the meta-learner teach itself.
The algorithm first bootstraps a target from the meta-learner and then optimises the meta-learner by minimising the distance to that target under a chosen (pseudo) metric. The bootstrapping mechanism can extend the effective meta-learning horizon without requiring backpropagation through all updates. The researchers achieved new state-of-the-art for model-free agents on the Atari ALE benchmark and showed that it yields both performance and efficiency gains in multi-task meta-learning.
More Great AIM StoriesGuide To OpenPyXL: A Python Module For ExcelImplementing A Recurrent Neural Network (RNN) From ScratchVirtual Real Estate: Stuff Science Fictions Are Made OfNew Innovations In Image Segmentation For Edge devicesHow Google’s Password Manager Keeps Brute Force Attacks At BayInside AUDOIR’s SAM, An AI Tool To Compose SongsOur Upcoming EventsConference, in-person (Bangalore)MachineCon 202224th JunConference, VirtualDeep Learning DevCon 202230th JulConference, in-person (Bangalore)Cypher 202221-23rd Sep3 Ways to Join our CommunityDiscord ServerStay Connected with a larger ecosystem of data science and ML ProfessionalsTelegram ChannelDiscover special offers, top stories, upcoming events, and more.
Subscribe to our newsletterGet the latest updates from AIMMORE FROM AIM7 tricks to speed up the training of a neural networkThere are a few approaches that can be used to reduce the training time time of neural networks.
Why the high accuracy in classification is not always correct?The high accuracy of classification model could be misleading.
YCombinator warns its startups to gear up for turbulent timesThe tech accelerator’s early investments include Dropbox, Coinbase, Airbnb, and Reddit.
LTIMindtree is good. But what about their Analytics businesses?LTI and Mindtree both play in Analytics services businesses, just like most other large IT/ITes service providers. But, what would the analytics services business of the merged entity look like?GitHub now offers math support in markdownGitHub’s math rendering capability uses MathJax; an open-source, JavaScript-based display engine.
WhatsApp launches cloud-based API for freeMeta recently organised messaging event called ‘Conversations.’Wipro announces 40,000 sq.ft. Innovation Studio in TexasThe studio will leverage Wipro’s deep reservoir of IPs, patents, and innovation DNA.
Google’s facial recognition tech to replace smart cards in Bengaluru metro trains￼BMRCL plans to introduce the technology at its automatic fare collection gates.
Data science hiring process at DealShareIn the next few months, DealShare looks to grow its data science team by 15-20 members.
DeepMind’s AlphaFold 2 is half of the storyThe idea was if I give you a sequence of amino acids, can you predict what will be the structure or the shape that it will take in the 3D space?Our mission is to bring about better-informed and more conscious decisions about technology through authoritative, influential, and trustworthy journalism.
Shape The Future of Techrankings & listsResourcesOUR brandsVIDEOSour conferencesAWARDSEVENTSmachinehackNewsletterStay up to date with our latest news, receive exclusive deals, and more.
© Analytics India Magazine Pvt Ltd 2022Terms of usePrivacy PolicyCopyright
