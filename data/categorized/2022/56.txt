old id = 251
Not So Mysterious After All: Researchers Show How to Crack AI’s Black Box
2022
https://singularityhub.com/2021/10/25/not-so-mysterious-after-all-researchers-show-how-to-crack-ais-black-box

Not So Mysterious After All: Researchers Show How to Crack AI’s Black BoxThe deep learning neural networks at the heart of modern artificial intelligence are often described as “black boxes” whose inner workings are inscrutable. But new research calls that idea into question, with significant implications for privacy.
Unlike traditional software whose functions are predetermined by a developer, neural networks learn how to process or analyze data by training on examples. They do this by continually adjusting the strength of the links between their manyneurons.
By the end of this process, the way they make decisions is tied up in a tangled network of connections that can be impossible to follow. As a result, it’s often assumed that even if you have access to the model itself, it’s more or less impossible to work out the data that the system was trained on.
But a pair of recent papers have brought this assumption into question,according toMIT Technology Review,by showing that two very different techniques can be used to identify the data a model was trained on. Thiscould have serious implications forAIsystems trained on sensitive information like health records or financial data.
The first approach takes aim at generative adversarial networks (GANs), the AI systems behinddeepfakeimages. These systemsare increasingly being used to createsynthetic facesthat are supposedly completely unrelated to realpeople.
But researchers from the University of Caen Normandy in France showed that they could easily linkgenerated facesfrom a popular model to real people whose data had been used to train the GAN. They did this by getting a second facial recognition model to compare the generated faces against training samples to spot if they shared the same identity.
The images aren’t an exact match, as the GAN has modified them, but the researchersfoundmultipleexamples where generated faces were clearly linked to images in the training data. Inapaper describing the research,theypoint out that in many cases the generated face is simply the original face in a different pose.
While the approach is specific to face-generation GANs, the researchers point out that similar ideas could be applied to things like biometric data or medical images. Another, more general approach to reverse engineering neural nets could do that straight off the bat, though.
A group from Nvidia has shownthatthey can infer the data the model was trained on without even seeing any examples of the trained data. They used an approach called model inversion, which effectively runs the neural net in reverse. This technique is often used to analyze neural networks, but using it to recover the input data had only been achieved on simple networks under very specific sets of assumptions.
Ina recent paper,the researchers describedhow they were able to scale the approach to large networks by splitting the problem up and carrying out inversions on each of the networks’ layers separately.
With thisapproach, they were able to recreate training data images using nothing but the modelsthemselves.
While carrying out either attack is a complex process that requires intimate access to the model in question, both highlight the fact that AIsmaynotbethe black boxes we thought they were, and determined attackerscouldextract potentiallysensitiveinformation from them.
Given that it’s becoming increasingly easytoreverse engineer someone else’s modelusing your own AI, the requirement to have access to the neural network isn’t even that big of a barrier.
The problem isn’t restricted to image-based algorithms. Last year, researchers froma consortium of tech companies and universitiesshowed that they could extract news headlines, JavaScript code, and personally identifiable information from the large language model GPT-2.
These issues are only going to become more pressing as AI systems push their way into sensitive areas like health, finance, and defense. There aresome solutions on the horizon,such as differential privacy, where models are trained on the statistical features of aggregated data rather than individual data points, or homomorphic encryption, an emerging paradigm that makes it possible to compute directly on encrypted data.
But these approaches are still a long way from being standard practice, so for the time being, entrusting your data to the black box of AI may not be as safe as you think.
Image Credit:Connect world/Shutterstock.comFollow Edd:LatestThis Week’s Awesome Tech Stories From Around the Web (Through May 21)Volvo and DHL Are Partnering on Hub-to-Hub Autonomous TruckingHow a Volcanic Bombardment in Ancient Australia Led to the World’s Greatest Climate CatastropheRELATEDNanomagnetic Computing Could Drastically Cut AI’s Energy UseA Hybrid AI Just Beat Eight World Champions at Bridge—and Explained How It Did ItNVIDIA’s Tiny New AI Transforms Photos Into Full 3D Scenes in Mere SecondsHow the Extinction of Ice Age Mammals May Have Forced Us to Invent CivilizationGet the latest news from Singularity Hub!Singularity University, Singularity Hub, Singularity Summit, SU Labs, Singularity Labs, Exponential Medicine, Exponential Finance and all associated logos and design elements are trademarks and/or service marks of Singularity Education Group.
© 2022 Singularity Education Group. All Rights Reserved.
Singularity University is not a degree granting institution.
