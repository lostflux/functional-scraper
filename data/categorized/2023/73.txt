old id = 767
Safety Critical AI - Partnership on AI
2023
https://new.partnershiponai.org/program/safety-critical-ai

OUR WORK How We Work Programs Inclusive Research & Design AI & Media Integrity AI, Labor, and the Economy FTA & ABOUT ML Safety-Critical AI Workstreams Public Policy Impact Stories RESOURCES Blog Resource Library EVENTS ABOUT US About Us Mission, Vision, & Values Pillars Tenets Team Funding Press Careers PARTNERS OUR WORK How We Work Programs Inclusive Research & Design AI & Media Integrity AI, Labor, and the Economy FTA & ABOUT ML Safety-Critical AI Workstreams Public Policy Impact Stories RESOURCES Blog Resource Library EVENTS ABOUT US About Us Mission, Vision, & Values Pillars Tenets Team Funding Press Careers PARTNERS CONTACT × SEARCH Our Work / Safety Critical AI Overview Impact Our Safety-Critical AI Work Updates & Media Coverage Steering Committee Program Workstreams Sign-up to receive updates on this and other PAI Programs.
STAY INFORMED Given AI’s potential for misuse, how do we develop and deploy algorithmic systems responsibly? Forecast future risks Develop best practices Improve preparedness Create foundations for governance Increasingly, AI systems are being deployed in contexts where safety risks can have widespread consequences, including medicine, finance, transportation, and social media. This makes anticipating and mitigating such risks — in both the near and long term — an urgent societal need.
Our Safety Critical AI Program convenes Partners and other stakeholders to develop best practices that can help us avert likely accidents, misuses, and unintended consequences of AI technologies. We don’t have to wait for such incidents to arise. As our work shows, precaution can be taken as early as the research stage to ensure the development of safe AI systems.
Impact Framework PAI’s Guidance for Safe Foundation Model Deployment PAI Staff Oct 24, 2023 Impact Stories Safety Critical AI/Publication Norms for Responsible AI Recommendations in Practice: Meta Researchers Apply PAI Guidance PAI Staff May 25, 2022 Impact Stories Tracking When AI Systems Fail PAI Staff Jan 30, 2021 Our Safety-Critical AI Work The Safety Critical AI Program has developed norms for responsible publication of AI research, supported the launch of the AI Incident Database , and created SafeLife , a novel AI learning environment for training non-destructive agents.
In 2021, PAI released the white paper “ Managing the Risks of AI Research ,” offering six recommendations for anticipating potential harms when publishing AI research. Following its publication, Nature Machine Intelligence published an editorial endorsing the paper’s recommendations. More recently, Meta AI was informed by and cited the white paper in the release of their large language model OPT-175B.
In 2022, PAI partnered with CIFAR and the Ada Lovelace Institute to bring together recent ML conference organizers and AI ethics researchers to consider how existing ethics review practices like impact statements are faring. “A Culture of Ethical AI,” a report co-authored by all three organizations, synthesizes insights gathered from this convening.
Updates & Media Coverage Blog Safety Critical AI PAI Is Collaboratively Developing Shared Protocols for Large-Scale AI Model Safety PAI Staff Apr 06, 2023 Editorial Publication Norms For Responsible AI Much to discuss in AI ethics Nature Machine Intelligence Dec 19, 2022 Comment Publication Norms For Responsible AI Advancing ethics review practices in AI research Nature Machine Intelligence Dec 14, 2022 Blog Publication Norms For Responsible AI How AI Conferences Can Foster a Responsible Research Culture PAI Staff Aug 03, 2022 Blog Explainable AI in Practice Studying AI Explanations to Improve Healthcare for Underserved Communities￼ Ana Lucic Jun 14, 2022 Podcast Publication Norms For Responsible AI Responsible AI Research with Madhulika Srikumar Machine Ethics Podcast Aug 25, 2021 Podcast Publication Norms For Responsible AI Responsibility, Risk, and Publishing Talking Machines Aug 19, 2021 News Publication Norms For Responsible AI How to be responsible in AI publication Nature Machine Intelligence May 19, 2021 Podcast Publication Norms For Responsible AI Should all AI research be published? Jeremie Harris May 12, 2021 Blog Publication Norms For Responsible AI What Responsible AI Can Learn From the Race to Fix Meltdown and Spectre PAI Staff Feb 26, 2021 Blog Publication Norms For Responsible AI Navigating the Broader Impacts of AI Research: Workshop at NeurIPS 2020 Rosie Campbell Jan 28, 2021 Blog Publication Norms For Responsible AI What the AI Community Can Learn From Sneezing Ferrets and a Mutant Virus Debate PAI Staff Dec 08, 2020 Blog AI Incidents Database When AI Systems Fail: Introducing the AI Incident Database Sean Mcgregor Nov 18, 2020 Blog SafeLife: AI Safety In Complex Environments Introducing the SafeLife Leaderboard: A Competitive Benchmark for Safer AI Carroll Wainwright Oct 09, 2020 Blog Explainable AI In Practice Multistakeholder Approaches to Explainable Machine Learning PAI Staff Jul 16, 2020 Blog Explainable AI In Practice Explainable AI in Practice Falls Short of Transparency Goals PAI Staff Jan 14, 2020 Steering Committee Anthony Aguirre Vice President & Secretary of the Board Future of Life Institute Shahar Avin Senior Research Associate CSER Wafa Ben-Hassine Principal, Responsible Technology Omidyar Network Esha Bhandari Deputy Director ACLU Speech, Privacy, and Technology Project Jack Clark Co-Founder Anthropic Iason Gabriel Staff Research Scientist DeepMind Gillian Hadfield Director Schwartz Reisman Institute for Technology and Society, University of Toronto Tadayoshi Kohno Professor of Computer Science University of Washington Department of Computer Science & Engineering Christina Montgomery Vice President and Chief Privacy & Trust Officer IBM Joelle Pineau Vice President of AI Research Meta Adrian Weller Programme Director for Safe and Ethical AI The Alan Turing Institute Jess Whittlestone Head of AI Policy Centre for Long-Term Resilience × Program Workstreams Program: Safety Critical AI Publication Norms for Responsible AI Project Status Engaging Audiences Program: Safety Critical AI AI Incidents Database Project Status Completed Program: Safety Critical AI Explainable AI in Practice Project Status Completed Program: Safety Critical AI SafeLife: AI Safety in Complex Environments Project Status Completed OUR WORK RESOURCES EVENTS ABOUT US PARTNERS © 2023 Partnership on AI | All Rights Reserved Transparency and Governance Privacy Policy
