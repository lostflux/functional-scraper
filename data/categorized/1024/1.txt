old id = 3889
Deep learning super sampling - Wikipedia
1024
https://en.wikipedia.org/wiki/Deep_learning_super_sampling

Deep learning super samplingDeep learning super sampling(DLSS) is a family ofreal-timedeep learningimage enhancement andupscalingtechnologies developed byNvidiathat are exclusive to itsRTXline ofgraphics cards,[1]and available in a number ofvideo games. The goal of these technologies is to allow the majority of thegraphics pipelineto run at a lowerresolutionfor increased performance, and then infer a higher resolution image from this that contains the same level of detail as if the image had been rendered at this higher resolution. This allows for higher graphical settings and/orframe ratesfor a given output resolution, depending on user preference.
[2]As of May 2022, this technology is available only onGeForce RTX 20andGeForce RTX 30series ofgraphics cardsbased on theTuringandAmperemicroarchitecture, with the GeForce RTX 40 series based on the Ada Lovelace microarchitecture also having the feature, with information being leaked due to adata breachby the hacking groupLapsus$. NVIDIA has also introducedDeep learning dynamic super resolution(DLDSR), a related and opposite technology where the graphics are rendered at a higher resolution, then downsampled to the native display resolution using an AI-assisted downsampling algorithm to achieve higher image quality than rendering at native resolution.
[3]ContentsHistory[edit]Nvidia advertised DLSS as a key feature of theGeForce RTX 20series cards when they launched in September 2018.
[4]At that time, the results were limited to a few video games (namelyBattlefield V[5]andMetro Exodus) because the algorithm had to be trained specifically on each game on which it was applied and the results were usually not as good as simple resolution upscaling.
[6][7]In 2019, the video gameControlshipped withray tracingand an improved version of DLSS, which did not use the Tensor Cores.
[8][9]In April 2020, Nvidia advertised and shipped an improved version of DLSS named DLSS 2.0 withdriverversion 445.75. DLSS 2.0 was available for a few existing games includingControlandWolfenstein: Youngblood, and would later be added to many newly released games andgame enginessuch asUnreal Engine[10]andUnity.
[11]This time Nvidia said that it used the Tensor Cores again, and that the AI did not need to be trained specifically on each game.
[4][12]Despite sharing the DLSS branding, the two iterations of DLSS differ significantly and are not backwards-compatible.
[13][14]Release history[edit]Quality presets[edit]Implementation[edit]DLSS 1.0[edit]The first iteration of DLSS is a predominantly spatial image upscaler with two stages, both relying onconvolutionalauto-encoderneural networks.
[18]The first step is an image enhancement network which uses the current frame and motion vectors to performedge enhancement, andspatial anti-aliasing. The second stage is an image upscaling step which uses the single raw, low-resolution frame to upscale the image to the desired output resolution. Using just a single frame for upscaling means the neural network itself must generate a large amount of new information to produce the high resolution output, this can result in slighthallucinationssuch as leaves that differ in style to the source content.
[13]The neural networks are trained on a per-game basis by generating a "perfect frame" using traditionalsupersamplingto 64 samples per pixel, as well as the motion vectors for each frame. The data collected must be as comprehensive as possible, including as many levels, times of day, graphical settings, resolutions etc. as possible. This data is alsoaugmentedusing common augmentations such as rotations, colour changes, and random noise to help generalize the test data. Training is performed on Nvidia's Saturn V supercomputer.
[14][19]This first iteration received a mixed response, with many criticizing the often soft appearance and artifacting in certain situations;[20][6][5]likely a side effect of the limited data from only using a single frame input to the neural networks which could not be trained to perform optimally in all scenarios andedge-cases.
[13][14]Nvidia also demonstrated the ability for the auto-encoder networks to learn the ability to recreatedepth-of-fieldandmotion blur,[14]although this functionality has never been included in a publicly released product.
[citation needed]DLSS 2.0[edit]DLSS 2.0 is atemporal anti-aliasingupsampling(TAAU) implementation, using data from previous frames extensively through sub-pixel jittering to resolve fine detail and reduce aliasing. The data DLSS 2.0 collects includes: the raw low-resolution input,motion vectors,depth buffers, andexposure/ brightness information.
[13]It can also be used as a simpler TAA implementation where the image is rendered at 100% resolution, rather than being upsampled by DLSS, Nvidia brands this as DLAA (Deep Learning Anti Aliasing).
[21]TAA(U) is used in many modern video games andgame engines,[22]however all previous implementations have used some form of manually writtenheuristicsto prevent temporal artifacts such asghostingandflickering. One example of this is neighborhood clamping which forcefully prevents samples collected in previous frames from deviating too much compared to nearby pixels in newer frames. This helps to identify and fix many temporal artifacts, but deliberately removing fine details in this way is analogous to applying ablur filter, and thus the final image can appear blurry when using this method.
[13]DLSS 2.0 uses aconvolutionalauto-encoderneural network[20]trained to identify and fix temporal artifacts, instead of manually programmed heuristics as mentioned above. Because of this, DLSS 2.0 can generally resolve detail better than other TAA and TAAU implementations, while also removing most temporal artifacts. This is why DLSS 2.0 can sometimes produce a sharper image than rendering at higher, or even native resolutions using traditional TAA. However, no temporal solution is perfect, and artifacts (ghosting in particular) are still visible in some scenarios when using DLSS 2.0.
Because temporal artifacts occur in most art styles and environments in broadly the same way, the neural network that powers DLSS 2.0 does not need to be retrained when being used in different games. Despite this, Nvidia does frequently ship new minor revisions of DLSS 2.0 with new titles,[23]so this could suggest some minor training optimizations may be performed as games are released, although Nvidia does not provide changelogs for these minor revisions to confirm this.
The main advancements compared to DLSS 1.0 include: Significantly improved detail retention, a generalized neural network that does not need to be re-trained per-game, and ~2x less overhead (~1-2ms vs ~2-4ms).
[13]It should also be noted that forms of TAAU such as DLSS 2.0 are notupscalersin the same sense as techniques such as ESRGAN or DLSS 1.0, which attempt to create new information from a low-resolution source; instead TAAU works to recover data from previous frames, rather than creating new data. In practice, this means low resolutiontexturesin games will still appear low-resolution when using current TAAU techniques. This is why Nvidia recommends game developers use higher resolution textures than they would normally for a given rendering resolution by applying a mip-map bias when DLSS 2.0 is enabled.
[13]Anti-Aliasing[edit]DLSS requires and applies its own anti-aliasing method.
It operates on similar principles to TAA. Like TAA, it uses information from past frames to produce the current frame. Unlike TAA, DLSS does not sample every pixel in every frame. Instead, it samples different pixels in different frames and uses pixels sampled in past frames to fill in the unsampled pixels in the current frame. DLSS uses machine learning to combine samples in the current frame and past frames, and it can be thought of as an advanced and superior TAA implementation made possible by the available tensor cores.
[13]Architecture[edit]With the exception of the shader-core version implemented inControl, DLSS is only available onGeForce RTX 20,GeForce RTX 30, andQuadro RTXseries of video cards, using dedicatedAI acceleratorscalledTensor Cores.
[20][24]Tensor Cores are available since the NvidiaVoltaGPUmicroarchitecture, which was first used on theTesla V100line of products.
[25]They are used for doingfused multiply-add(FMA) operations that are used extensively in neural network calculations for applying a large series of multiplications on weights, follows by the addition of a bias. Tensor cores can operate on FP16, INT8, INT4, and INT1 data types. Each core can do 1024 bits of FMA operations per clock, so 1024 INT1, 256 INT4, 128 INT8, and 64 FP16 operations per clock per tensor core, and most Turing GPUs have a few hundred tensor cores.
[26]The Tensor Cores use CUDAWarp-Level Primitives on 32 parallel threads to take advantage of their parallel architecture.
[27]A Warp is a set of 32threadswhich are configured to execute the same instruction.
See also[edit]References[edit]External links[edit]Navigation menuSearch
