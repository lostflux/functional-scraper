old id = 1656
Deepfakes and Synthetic Media: What should we fear? What can we do? - WITNESS Blog
unknown
https://blog.witness.org/2018/07/deepfakes

Connect With UsBlogrollContact|Press KitWITNESS80 Hanson Place, 5th FloorBrooklyn, NY 11217Voice: 718.783.2000Fax: 718.783.1593Privacy PolicyDeepfakes and Synthetic Media: What should we fear? What can we do?For updated information on WITNESS work in this area please visitwit.to/Synthetic-Media-DeepfakesThis is the first in a series of blogs on a new area of focus at WITNESS around the emerging and potential malicious uses of so-called “deepfakes” and other forms of AI-generated “synthetic media” and how we push back to defend evidence, the truth and freedom of expression.
We’ll be sharing expanded elements of the report and further details on the recommendations in separate blogs—the second blog in the series can be foundhere.
This work kicked off with an expert summit—the full report on that is availablehere.
This work is embedded in a broader initiative focused on proactive approaches to protecting and upholding marginal voices and human rights as emerging technologies such as AI intersect with the pressures of disinformation, media manipulation, and rising authoritarianism.
People have started to panic about the increasing possibility of manipulating images, video, and audio, often popularly described as “deepfakes”. In the past decade Hollywood studios have had the capacity to morph faces —from Brad Pitt in “The Curious Case of Benjamin Button” to Princess Leia in “Star Wars’ Rogue One”—and companies and consumers have had tools such as Photoshop to digitally alter images and video in subtler ways. However, now the barriers to entry to create and manipulate audio and video in multiple, more sophisticated ways are beginning to fall, requiring less cost, less technical expertise and drawing on widely available cloud computing power. At the same time, the sophistication of manipulation of social media spaces by bad actors has led to increased opportunities to weaponize these manipulations.
This changing landscape allows for new challenges to human rights and reliable journalism that potentially includes categories of disruption including:Why WITNESS is engagedFor more than 25 years,WITNESShas enabled human rights defenders, and now increasingly anyone, anywhere to use video and technology to protect and defend human rights. Our work and the work of our partners demonstrates the value of images to drive a more diverse personal storytelling and civic journalism, to drive movements around pervasive human rights violations like police violence, and to be critical evidence in war crimes trials. We have also seen the ease in which videos and audio, often crudely edited or even simply recycled and re-contextualized can perpetuate and renew cycles of violence.
WITNESS’ Tech + Advocacy work frequently includes engaging with key social media and video-sharing platforms to develop innovative policy and product responses to challenges facing high-risk users and high public interest content. As the threat of more sophisticated, more personalized audio and video manipulation emerges, we are focused on the critical need to bring together key actorsbeforewe are in the eye-of-the-storm, to push back against apocalyptic narratives on this issue, and identify proactive solutions to ensure we prepare in a more coordinated way.
What are deepfakes and synthetic media?The development of new forms of image and audio synthesis is related to the growth of the subfield of machine learning known as deep learning, which includes using architectures for artificial intelligence similar to human neural networks. Generative Adversarial Networks (GANs) are the technology used in deepfakes. Two neural networks compete to produce and discern high quality faked images. One is the “generator” (which creates images that look like an original image) and the other is the “discriminator” (which tries to figure out if an image is real or simulated). They compete in acat-and-mouse game to make better and better images.
The cost of producing these new forms of synthetic media has decreased significantly in the last few years given increasing amounts of training data, computing power and effective publicly shared approaches and code.
So what to call these manipulations? The terms to describe these advances in video and audio manipulation are not yet well defined. The current conversation is dominated by the term deepfakes, which refers to the result of software that swaps a face between one person and another, and which was initially deployed in contexts such as nonconsensual image manipulation for porn. But a broader range of manipulation (and consequent malicious uses) of audio and video is possible and has been called “synthetic media.”Potential tools susceptible to mal-uses included:An introduction to the “arms” race between synthesis of synthetic media and detection/forensicsThere is an ongoing arms race between manual and automatic synthesis of media, and manual and automatic forensic approaches.
Manual synthesis is characterized by theexplicit modelingof geometry, lighting and physics that we see in Hollywood effects. CGI has been a part of movie industry for 30 years, but it is time-consuming, expensive, and has required domain expertise. On the other hand, automatic synthesis involves use ofimplicit synthesisof texture, lighting or head motion as we have seen for example inLipSync Obama,Deep Video Portraitsor of course, deepfakes. Techniques here often involve a combination of computer vision and computer graphics, and in some cases uses of neural networks. Tools such as LipSync Obama build on a twenty-year research trajectory of exploring how to create 3D face models from existing images. There are a range of positive applications of enhanced ‘synthetic media’ including video and virtual telepresence, VR and AR and content creation, animation and dubbing. There will also be uses in autonomous systems and in human computer/human-robot interaction.
Editing software and manual and automatic synthesis can increasingly create perceptually realistic images that are not visible as manipulated to the naked eye and visual analysis.
Manual forensics does explicit checks of perspectival geometry, lighting, shadows and the ‘physics’ of images, as well as detecting for example copying and splicing between images and evidence of the camera model for a photo. A recent notable example of manual forensics specific to deepfakes is the idea of using a technique known asEulerian Video Magnification, to see the visible pulse rate of real people that would be absent in a deepfake.
An emerging field is automatic forensics. Approaches explored in this include looking at larger datasets and using machine learning to do forensic analysis. Recent experimentation includes:However, most systems are trained on specific databases, and might detect mainly the inconsistencies of specific synthesis techniques, although there is work in progress that addresses these shortcomings. There are also new counter-forensic approaches that use GANs to fight back against forensic analysis – for example, by wiping the forensic traces of multiple cameras and creating an image that appears to have the uniform camera signature of another camera.
Researchers disagree on whether the “arms race” is likely to be won by the forgers or the detectors. Humans are not good at detecting the difference between a real and a fake video (see data inFaceForensics(pdf) which indicated that with low-resolution images humans had approximately 50% accuracy “which is essentially guessing”) but machines are. Detection is currently easier than forgery and for every forgery AI there is a powerful detection model. Provided there is sufficient training data showing new types of faked images, audio and video, the use of GANs might be able to keep up in enabling AI-assisted identification of non-visible faking. There might be a time lag — which will be exploited by bad actors — but detection should keep improving.
What is WITNESS doing?We see the need to:To initiate that on June 11, 2018, WITNESS in collaboration withFirst Draft, a project of the Shorenstein Center on Media, Politics and Public Policy at Harvard Kennedy School, brought together thirty leading independent and company-based technologists, machine learning specialists, academic researchers in synthetic media, human rights researchers, and journalists.
Our goal was to have an open discussion under the Chatham House Rule about pragmatic proactive ways to mitigate the threats that widespread use and commercialization of new tools for AI-generated synthetic media such as deepfakes and facial reenactment potentially pose to public trust, reliable journalism and trustworthy human rights documentation.
Our convening report is availablehereand we’ll be sharing expanded elements of the report and further details on the recommendations in separate blogs including:What do we recommend as next steps?Among the recommendations from the convening:For further information on the project please contact Sam Gregory, sam@witness.org.
Share this:Leave a ReplyCancel replyYour email address will not be published.
Required fields are marked*Comment*Name*Email*WebsiteSave my name, email, and website in this browser for the next time I comment.
Δdocument.getElementById( "ak_js_1" ).setAttribute( "value", ( new Date() ).getTime() );
