old id = 1378
What is Natural Language Processing? Intro to NLP in Machine Learning
unknown
https://www.gyansetu.in/what-is-natural-language-processing

HideWhat is Natural Language Processing? Intro to NLP in Machine LearningWhat is Natural Language Processing? Intro to NLP in Machine LearningNLP is a part of machine learning that deals with understanding, analyzing, and generating the languages that humans use naturally or communicate in order to interface with computers instead of machine language.
Applications of NLPNLP PipelineThere are 3 stages of an NLP pipeline:1.
Text Processing:- Take raw text as input, clean it, normalize it, tokenize it, remove stop words, etc. and convert it into a form that is best for feature extraction.
2.
Feature Extraction:- Extract features that are appropriate for an NLP task you are trying to accomplish and the model you are trying to use.
3.
Modeling:- Design a machine learning model, fit its parameters to the training data, and then use it to make predictions on unseen data.
This process/pipeline isn’t always linear and may require additional steps.
Read also:-Regression vs classification in Machine LearningWhy do we need to process text?To make our raw input text free from any constructs that are not required for the task.
Text ProcessingData CleaningHere we remove special characters, HTML tags, etc. from the raw text as they do not contain any info for the model to learn and are irrelevant or noisy data.
Data NormalizationData normalization involves steps such as case normalization, punctuation removal, etc. so that the text is in a single format for the machine to learn.
Read More:-Is Python Enough for Machine LearningCase NormalizationCar, car, CAR -> they all mean the same thing.
So, convert all capitalization to lower to bring to a common case.
Punctuation RemovalReplace punctuation with space.
TokenizationTokenization is the process of breaking up text documents into individual words called tokens.
So, far we have been using python inbuilt functions for this task.
Tokenization using NLTKStop Word RemovalStop word removal means the removal of non-important words like‘a’, ‘is’, ‘the’, ‘and’, ‘an’, ‘are’, “me”, “i”, etc.
There is an in-built stopword list in NLTK which we can use to remove stop words from text documents. However this is not the standard stopwords list for every problem, we can also define our own set of stop words based on the domain.
Parts of Speech (POS) TaggingGiven a sentence, determine POS tags for each word (e.g.,NOUN, VERB, ADV, ADJ).
You can use an inbuilt part of speech tagger provided in NLTK. There are other more advanced forms of POS tagging that can learn sentence structures and tags from given data, including Hidden Markov Models (HMMs) and Recurrent Neural Networks (RNNs).
Named Entity RecognitionIn information extraction, anamed entityis a real-world object, such as persons, locations, organizations, products, etc., that can be denoted with a propername.
StemmingStemmingis a process of reducing a word to its root form.
branching branched, branchescan all be reduced to branch.
Caching, caches, cachedcan all be reduced to the cache.
1. from nltk.stem.porter import PorterStemmerLemmatizationLemmatizationis another technique for reducing words to it’s normalized form. But in this case, the transformation actually uses a dictionary to map words to their actual form.
change, changing, changes, changed, changer is transformed to change in Stemmed from (stemming does not confirm the root word is also a full actual word), but this is transformed to change in Lemmatized form (lemmatization will have the root word also as an actual meaningful word).
Learn Machine LearningFeature ExtractionFeature Extraction is a way of extracting feature vectors from the text after the text processing step so that it can be used in the machine learning model as input. This extracted feature from the text can be a wordnet of a graph of nodes, a vector representing words (doc2vec, word2vec, sent2vec, glove, etc.)Word Embedding is one such technique where we can represent the text using vectors. The more popular forms of word embeddings are:1.
BoW, which stands for Bag of Words2.
TF-IDF, which stands for Term Frequency-Inverse Document FrequencyBag-of-Words ModelBag of words model, or BoW for short, is a way of extracting features from the text for use in modeling, such as machine learning algorithms. It treats each document as a collection/bag of words.
A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:The intuition is that documents are similar if they have similar content. Further, from the content alone we can learn something about the meaning of the document.
Example:-Corpus is a set of documents.
TF-IDFLet’s first put a formal definition around TF-IDF. Here’s how Wikipedia puts it:“Term frequency-inverse document frequency is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.”Term FrequencyLet’s first understand Term Frequent (TF). It is a measure of how frequently a term, t, appears in a document, dHere, in the numerator, n is the number of times the term “t” appears in the document “d”. Thus, each document and term would have its own TF value.
Inverse Document Frequency (IDF)IDF is a measure of how important a term is. We need the IDF because computing just the TF alone is not sufficient to understand the importance of words.
ModelingThe final stage of the NLP pipeline ismodeling, which includes designing a statistical or machine learning model, fitting its parameters to training data, using an optimization procedure, and then using it to make predictions about unseen data.
The nice thing about working with numerical features is that it allows you to choose from all machine learning models or even a combination of them.
Some of themachine learning algorithmsused here are:-1.
Linear Regression2.
Logistic Regression3.
Decision Trees4.
Random Forest Classifier5.
Naive Bayes Classifier6.
Support Vector Machines (SVM)7.
Neural Networks (RNN, LSTM, Bi-LSTM)8.
Ensemble Methods – Adaboost, XGBoost, Gradient Boost9.
Gradient Boosting Machine10.
Nearest Neighbors (K-Nearest Neighbors, Approximate Nearest Neighbors, etc.)11.
Clustering (K-means, Hierarchical, DBSCAN, Gaussian Mixture Models, etc.)12.
Many more….
Once you have a working model, you can deploy it as a web app, mobile app, or integrate it with other products and services. The possibilities are endless!Recent PostsJob Profiles & Responsibilities of Data Scientist at Microsoft, Google, AmazonMachine Learning Project Ideas for 202215 Machine Learning Interview Questions (with Answers) for Data ScientistsTop NLP (Natural Language Processing) Interview Question AnswersBlog CategoriesEnroll NowStructure your learning and get a certificate to prove it.
Discover Top CategoriesiClass Gyansetu is one of the fastest growing professionally managed company in Technology Learning & Consulting. The core team comprises of highly qualified and skilled professionals with global exposure in diverse areas. We offer high quality, cost-effective professional training with delivery par excellence in cutting edge technologies.
Useful linksContact with UsNewsletterEnquire nowIf you have any query please submit a requestApply for TrainerIf you want to apply as trainer please fill the form below
