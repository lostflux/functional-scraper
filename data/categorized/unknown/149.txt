old id = 1161
OpenAI Five
unknown
https://openai.com/blog/openai-five

OpenAI FiveOur team of five neural networks, OpenAI Five, has started to defeat amateur human teams at Dota 2.
Our team of five neural networks, OpenAI Five, has started todefeatamateur human teams atDota 2. While today we play withrestrictions, we aim to beat a team of top professionals atThe Internationalin August subject only to a limited set of heroes. We may not succeed: Dota 2 is one of the most popular andcomplexesports games in the world, with creative and motivated professionals whotrainyear-round to earn part of Dota's annual $40Mprize pool(the largest of any esports game).
OpenAI Five plays 180 years worth of games against itself every day, learning via self-play. It trains using a scaled-up version ofProximal Policy Optimizationrunning on 256 GPUs and 128,000 CPU cores — a larger-scale version of the system we built to play the much-simplersolo variantof the game last year. Using a separateLSTMfor each hero and no human data, it learns recognizable strategies. This indicates thatreinforcement learningcan yield long-term planning with large but achievable scale — without fundamental advances, contrary to our own expectations upon starting the project.
To benchmark our progress, we'll host a match versus top players on August 5th.
Followus on Twitch to view the live broadcast, orrequestan invite to attend in person!OpenAI Five playing the best OpenAI employee team. The match was commentated by professional commentatorBlitzand OpenAI Dota team member Christy Dennison, and observed by a crowd from the community.
The problemOne AI milestone is to exceed human capabilities in a complex video game likeStarCraftor Dota. Relative to previous AI milestones likeChessorGo, complex video games start to capture the messiness and continuous nature of the real world. The hope is that systems which solve complex video games will be highly general, with applications outside of games.
Dota 2 is a real-time strategy game played between two teams of five players, with each player controlling a character called a "hero". A Dota-playing AI must master the following:The Dota rules are also very complex — the game has been actively developed for over a decade, with game logic implemented in hundreds of thousands of lines of code. This logic takes milliseconds per tick to execute, versus nanoseconds for Chess or Go engines. The game also gets an update about once every two weeks, constantly changing the environment semantics.
Our approachOur system learns using a massively-scaled version ofProximal Policy Optimization. Both OpenAI Five and our earlier1v1 botlearn entirely from self-play. They start with random parameters and do not usesearchor bootstrap from human replays.
RL researchers (including ourselves) have generallybelievedthat long time horizons would require fundamentally new advances, such ashierarchicalreinforcementlearning. Our results suggest that we haven't been giving today's algorithms enough credit — at least when they're run at sufficient scale and with a reasonable way ofexploring.
Our agent is trained to maximize the exponentially decayed sum of future rewards, weighted by an exponential decay factor calledγ. During the latest training run of OpenAI Five, we annealedγfrom0.998(valuing future rewards with a half-life of 46 seconds) to0.9997(valuing future rewards with a half-life of five minutes). For comparison, the longest horizon in thePPOpaper was a half-life of 0.5 seconds, the longest in theRainbowpaper was a half-life of 4.4 seconds, and theObserve and Look Furtherpaper used a half-life of 46 seconds.
While the current version of OpenAI Five is weak atlast-hitting(observing our test matches, the professional Dota commentatorBlitzestimated it around median for Dota players), itsobjective prioritizationmatches a common professional strategy. Gaining long-term rewards such as strategic map control often requires sacrificing short-term rewards such as gold gained fromfarming, since grouping up to attack towers takes time. This observation reinforces our belief that the system is truly optimizing over a long horizon.
Model structureEach ofOpenAI Five's networkscontain a single-layer, 1024-unitLSTMthat sees the current game state (extracted from Valve'sBot API) and emits actions through several possible action heads. Each head has semantic meaning, for example, the number of ticks to delay this action, which action to select, the X or Y coordinate of this action in a grid around the unit, etc. Action heads are computed independently.
Interactive demonstration of the observation space and action space used by OpenAI Five. OpenAI Five views the world as a list of 20,000 numbers, and takes an action by emitting a list of 8 enumeration values. Select different actions and targets to understand how OpenAI Five encodes each action, and how it observes the world. The image shows the scene as a human would see it.
OpenAI Five can react to missing pieces of state that correlate with what it does see. For example, until recently OpenAI Five's observations did not includeshrapnelzones (areas where projectiles rain down on enemies), which humans see on screen. However, we observed OpenAI Five learning to walk out of (though not avoid entering) active shrapnel zones, since it could see its health decreasing.
ExplorationGiven a learning algorithm capable of handling long horizons, we still need to explore the environment. Even with ourrestrictions, there are hundreds of items, dozens of buildings, spells, and unit types, and a long tail of game mechanics to learn about — many of which yield powerful combinations. It's not easy to explore this combinatorially-vast space efficiently.
OpenAI Five learns from self-play (starting from random weights), which provides a natural curriculum for exploring the environment. To avoid "strategy collapse", the agent trains 80% of its games against itself and the other 20% against its past selves. In the first games, the heroes walk aimlessly around the map. After several hours of training, concepts such aslaning,farming, or fighting overmidemerge. After several days, they consistently adopt basic human strategies: attempt to stealBountyrunes from their opponents, walk to theirtier onetowers to farm, and rotate heroes around the map to gain lane advantage. And with further training, they become proficient at high-level strategies like5-hero push.
In March 2017, our firstagentdefeated bots but got confused against humans. To force exploration in strategy space, during training (and only during training) we randomized the properties (health, speed, start level, etc.) of the units, and it began beating humans. Later on, when a test player was consistently beating our 1v1 bot, we increased our training randomizations and the test player started to lose. (Our robotics team concurrently applied similar randomization techniques tophysicalrobotsto transfer from simulation to the real world.)OpenAI Five uses the randomizations we wrote for our 1v1 bot. It also uses a new "lane assignment" one. At the beginning of each training game, we randomly "assign" each hero to some subset oflanesand penalize it for straying from those lanes until a randomly-chosen time in the game.
Exploration is also helped by a good reward.
Our rewardconsists mostly of metrics humans track to decide how they're doing in the game: net worth, kills, deaths, assists, last hits, and the like. We postprocess each agent's reward by subtracting the other team's average reward to prevent the agents from finding positive-sum situations.
We hardcode item and skill builds (originally written for ourscriptedbaseline), and choose which of the builds to use at random.
Couriermanagement is also imported from the scripted baseline.
CoordinationOpenAI Five does not contain an explicit communication channel between the heroes' neural networks. Teamwork is controlled by a hyperparameter we dubbed "team spirit". Team spirit ranges from 0 to 1, putting a weight on how much each of OpenAI Five's heroes should care about its individual reward function versus the average of the team's reward functions. We anneal its value from 0 to 1 over training.
RapidOur system is implemented as a general-purpose RL training system called Rapid, which can be applied to anyGymenvironment. We've used Rapid to solve other problems at OpenAI, includingCompetitive Self-Play.
The training system is separated intorolloutworkers, which run a copy of the game and an agent gathering experience, andoptimizernodes, which perform synchronous gradient descent across a fleet of GPUs. The rollout workers sync their experience through Redis to the optimizers. Each experiment also contains workers evaluating the trained agent versus reference agents, as well as monitoring software such asTensorBoard,Sentry, andGrafana.
During synchronous gradient descent, each GPU computes a gradient on its part of the batch, and then the gradients are globally averaged. We originally usedMPI'sallreducefor averaging, but now use our ownNCCL2wrappers that parallelize GPU computations and network data transfer.
The latencies for synchronizing 58MB of data (size of OpenAI Five's parameters) across different numbers of GPUs are shown on the right. The latency is low enough to be largely masked by GPU computation which runs in parallel with it.
We've implemented Kubernetes, Azure, and GCP backends for Rapid.
The gamesThus far OpenAI Five has played (with ourrestrictions) versus each of these teams:The April 23rd version of OpenAI Five was the first to beat our scripted baseline. The May 15th version of OpenAI Five was evenly matched versus team 1, winning one game and losing another. The June 6th version of OpenAI Five decisively won all its games versus teams 1-3. We set up informalscrimswith teams 4 & 5, expecting to lose soundly, but OpenAI Five won two of its first three games versus both.
The teamwork aspect of the bot was just overwhelming. It feels like five selfless players that know a good general strategy.
— BlitzWe observed that OpenAI Five:Differences versus humansOpenAI Five is given access to the same information as humans, but instantly sees data like positions, healths, and item inventories that humans have to check manually. Our method isn't fundamentally tied to observing state, but just rendering pixels from the game would require thousands of GPUs.
OpenAI Five averages around 150-170 actions per minute (and has a theoretical maximum of 450 due to observing every 4th frame). Frame-perfect timing, whilepossiblefor skilled players, is trivial for OpenAI Five. OpenAI Five has an average reaction time of 80ms, which is faster than humans.
These differences matter most in 1v1 (where our bot had a reaction time of 67ms), but the playing field is relatively equitable as we've seen humans learn from and adapt to the bot. Dozens ofprofessionalsusedour 1v1 bot fortrainingin the months after last year'sTI. According to Blitz, the 1v1 bot has changed the way people think about 1v1s (the bot adopted a fast-paced playstyle, and everyone has now adapted to keep up).
Surprising findingsA subset of the OpenAI Dota team, holding the laptop thatdefeatedthe world's top professionals at Dota 1v1 at The International last year.*What's nextOur team is focused on making our August goal. We don't know if it will be achievable, but we believe that with hard work (and some luck) we have a real shot.
This post described a snapshot of our system as of June 6th. We'll release updates along the way to surpassing human performance and write a report on our final system once we complete the project. Please join us on August 5thvirtuallyorin person, when we'll play a team of top players!Our underlying motivation reaches beyond Dota. Real-world AI deployments will need to deal with thechallengesraised by Dota which are not reflected in Chess, Go, Atari games, or Mujoco benchmark tasks. Ultimately, we will measure the success of our Dota system in its application to real-world tasks. If you'd like to be part of what comes next, we'rehiring!The hero set restriction makes the game very different from how Dota is played at world-elite level (i.e.
Captains Modedrafting from all 100+ heroes). However, the difference from regular "public" games (All Pick/Random Draft) is smaller.
Most of the restrictions come from remaining aspects of the game we haven't integrated yet. Some restrictions, in particular wards and Roshan, are central components of professional-level play. We're working to add these as soon as possible.
You can cite this blog post with the following BibTeX:
