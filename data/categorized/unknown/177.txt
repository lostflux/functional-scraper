old id = 1607
Gradient descent - Wikipedia
unknown
https://en.wikipedia.org/wiki/Gradient_descent

Gradient descentIn mathematicsgradient descent(also often calledsteepest descent) is afirst-orderiterativeoptimizationalgorithmfor finding alocal minimumof adifferentiable function. The idea is to take repeated steps in the opposite direction of thegradient(or approximate gradient) of the function at the current point, because this is the direction of steepest descent. Conversely, stepping in the direction of the gradient will lead to alocal maximumof that function; the procedure is then known asgradient ascent.
Gradient descent is generally attributed toCauchy, who first suggested it in 1847.
[1]Hadamardindependently proposed a similar method in 1907.
[2][3]Its convergence properties for non-linear optimization problems were first studied byHaskell Curryin 1944,[4]with the method becoming increasingly well-studied and used in the following decades.
[5][6]ContentsDescription[edit]Gradient descent is based on the observation that if themulti-variable functionF(x){\displaystyle F(\mathbf {x} )}isdefinedanddifferentiablein a neighborhood of a pointa{\displaystyle \mathbf {a} }, thenF(x){\displaystyle F(\mathbf {x} )}decreasesfastestif one goes froma{\displaystyle \mathbf {a} }in the direction of the negativeGradientofF{\displaystyle F}ata,−∇F(a){\displaystyle \mathbf {a} ,-\nabla F(\mathbf {a} )}. It follows that, iffor a small enough step size orlearning rateγ∈R+{\displaystyle \gamma \in \mathbb {R} _{+}}, thenF(an)≥F(an+1){\displaystyle F(\mathbf {a_{n}} )\geq F(\mathbf {a_{n+1}} )}. In other words, the termγ∇F(a){\displaystyle \gamma \nabla F(\mathbf {a} )}is subtracted froma{\displaystyle \mathbf {a} }because we want to move against the gradient, toward the local minimum. With this observation in mind, one starts with a guessx0{\displaystyle \mathbf {x} _{0}}for a local minimum ofF{\displaystyle F}, and considers the sequencex0,x1,x2,…{\displaystyle \mathbf {x} _{0},\mathbf {x} _{1},\mathbf {x} _{2},\ldots }such thatWe have amonotonicsequenceso, hopefully, the sequence(xn){\displaystyle (\mathbf {x} _{n})}converges to the desired local minimum. Note that the value of thestep sizeγ{\displaystyle \gamma }is allowed to change at every iteration. With certain assumptions on the functionF{\displaystyle F}(for example,F{\displaystyle F}convexand∇F{\displaystyle \nabla F}Lipschitz) and particular choices ofγ{\displaystyle \gamma }(e.g., chosen either via aline searchthat satisfies theWolfe conditions, or the Barzilai–Borwein method[7][8]shown as following),convergenceto a local minimum can be guaranteed. When the functionF{\displaystyle F}isconvex, all local minima are also global minima, so in this case gradient descent can converge to the global solution.
This process is illustrated in the adjacent picture. Here,F{\displaystyle F}is assumed to be defined on the plane, and that its graph has abowlshape. The blue curves are thecontour lines, that is, the regions on which the value ofF{\displaystyle F}is constant. A red arrow originating at a point shows the direction of the negative gradient at that point. Note that the (negative) gradient at a point isorthogonalto the contour line going through that point. We see that gradientdescentleads us to the bottom of the bowl, that is, to the point where the value of the functionF{\displaystyle F}is minimal.
An analogy for understanding gradient descent[edit]The basic intuition behind gradient descent can be illustrated by a hypothetical scenario. A person is stuck in the mountains and is trying to get down (i.e., trying to find the global minimum). There is heavy fog such that visibility is extremely low. Therefore, the path down the mountain is not visible, so they must use local information to find the minimum. They can use the method of gradient descent, which involves looking at the steepness of the hill at their current position, then proceeding in the direction with the steepest descent (i.e., downhill). If they were trying to find the top of the mountain (i.e., the maximum), then they would proceed in the direction of steepest ascent (i.e., uphill). Using this method, they would eventually find their way down the mountain or possibly get stuck in some hole (i.e., local minimum orsaddle point), like a mountain lake. However, assume also that the steepness of the hill is not immediately obvious with simple observation, but rather it requires a sophisticated instrument to measure, which the person happens to have at the moment. It takes quite some time to measure the steepness of the hill with the instrument, thus they should minimize their use of the instrument if they wanted to get down the mountain before sunset. The difficulty then is choosing the frequency at which they should measure the steepness of the hill so not to go off track.
In this analogy, the person represents the algorithm, and the path taken down the mountain represents the sequence of parameter settings that the algorithm will explore. The steepness of the hill represents theslopeof the error surface at that point. The instrument used to measure steepness isdifferentiation(the slope of the error surface can be calculated by taking thederivativeof the squared error function at that point). The direction they choose to travel in aligns with thegradientof the error surface at that point. The amount of time they travel before taking another measurement is the step size.
Examples[edit]Gradient descent has problems withpathologicalfunctions such as theRosenbrock functionshown here.
The Rosenbrock function has a narrow-curved valley that contains the minimum. The bottom of the valley is very flat. Because of the curved flat valley, the optimization is zigzagging slowly with small step sizes towards the minimum. TheWhiplash gradient descent, which is a closed-loop algorithm, successfully minimizes this function.
[9]The zigzagging nature of the method is also evident below, where the gradient ascent method is applied toChoosing the step size and descent direction[edit]Since using a step sizeγ{\displaystyle \gamma }that is too small would slow convergence, and aγ{\displaystyle \gamma }too large would lead to divergence, finding a good setting ofγ{\displaystyle \gamma }is an important practical problem.
Philip Wolfealso advocated for using "clever choices of the [descent] direction" in practice.
[10]Whilst using a direction that deviates from the steepest descent direction may seem counter-intuitive, the idea is that the smaller slope may be compensated for by being sustained over a much longer distance.
To reason about this mathematically, let's use a directionpn{\displaystyle \mathbf {p} _{n}}and step sizeγn{\displaystyle \gamma _{n}}and consider the more general update:Finding good settings ofpn{\displaystyle \mathbf {p} _{n}}andγn{\displaystyle \gamma _{n}}requires a little thought. First of all, we would like the update direction to point downhill. Mathematically, lettingθn{\displaystyle \theta _{n}}denote the angle between∇F(an){\displaystyle \nabla F(\mathbf {a_{n}} )}andpn{\displaystyle \mathbf {p} _{n}}, this requires thatcos⁡θn>0.
{\displaystyle \cos \theta _{n}>0.}To say more, we need more information about the objective function that we are optimising. Under the fairly weak assumption thatF{\displaystyle F}is continuously differentiable, we may prove that:[11](1)This inequality implies that the amount by which we can be sure the functionF{\displaystyle F}is decreased depends on a trade off between the two terms in square brackets. The first term in square brackets measures the angle between the descent direction and the negative gradient. The second term measures how quickly the gradient changes along the descent direction.
In principle inequality (1) could be optimized overpn{\displaystyle \mathbf {p} _{n}}andγn{\displaystyle \gamma _{n}}to choose an optimal step size and direction. The problem is that evaluating the second term in square brackets requires evaluating∇F(an−tγnpn){\displaystyle \nabla F(\mathbf {a} _{n}-t\gamma _{n}\mathbf {p} _{n})}, and extra gradient evaluations are generally expensive and undesirable. Some ways around this problem are:Usually by following one of the recipes above,convergenceto a local minimum can be guaranteed. When the functionF{\displaystyle F}isconvex, all local minima are also global minima, so in this case gradient descent can converge to the global solution.
Solution of a linear system[edit]Gradient descent can be used to solve a system of linear equationsreformulated as a quadratic minimization problem. If the system matrixA{\displaystyle A}is realsymmetricandpositive-definite, an objective function is defined as the quadratic function, with minimization ofso thatFor a general real matrixA{\displaystyle A},linear least squaresdefineIn traditional linear least squares for realA{\displaystyle A}andb{\displaystyle \mathbf {b} }theEuclidean normis used, in which caseTheline searchminimization, finding the locally optimal step sizeγ{\displaystyle \gamma }on every iteration, can be performed analytically for quadratic functions, and explicit formulas for the locally optimalγ{\displaystyle \gamma }are known.
[5][13]For example, for realsymmetricandpositive-definitematrixA{\displaystyle A}, a simple algorithm can be as follows,[5]To avoid multiplying byA{\displaystyle A}twice per iteration, we note thatx:=x+γr{\displaystyle \mathbf {x} :=\mathbf {x} +\gamma \mathbf {r} }impliesr:=r−γAr{\displaystyle \mathbf {r} :=\mathbf {r} -\gamma \mathbf {Ar} }, which gives the traditional algorithm,[14]The method is rarely used for solving linear equations, with theconjugate gradient methodbeing one of the most popular alternatives. The number of gradient descent iterations is commonly proportional to the spectralcondition numberκ(A){\displaystyle \kappa (A)}of the system matrixA{\displaystyle A}(the ratio of the maximum to minimumeigenvaluesofATA{\displaystyle A^{T}A}), while the convergence ofconjugate gradient methodis typically determined by a square root of the condition number, i.e., is much faster. Both methods can benefit frompreconditioning, where gradient descent may require less assumptions on the preconditioner.
[14]Solution of a non-linear system[edit]Gradient descent can also be used to solve a system ofnonlinear equations. Below is an example that shows how to use the gradient descent to solve for three unknown variables,x1,x2, andx3. This example shows one iteration of the gradient descent.
Consider the nonlinear system of equationsLet us introduce the associated functionwhereOne might now define the objective functionwhich we will attempt to minimize. As an initial guess, let us useWe know thatwhere theJacobian matrixJG{\displaystyle J_{G}}is given byWe calculate:ThusandNow, a suitableγ0{\displaystyle \gamma _{0}}must be found such thatThis can be done with any of a variety ofline searchalgorithms. One might also simply guessγ0=0.001,{\displaystyle \gamma _{0}=0.001,}which givesEvaluating the objective function at this value, yieldsThe decrease fromF(0)=58.456{\displaystyle F(\mathbf {0} )=58.456}to the next step's value ofis a sizable decrease in the objective function. Further steps would reduce its value further, until an approximate solution to the system was found.
Comments[edit]Gradient descent works in spaces of any number of dimensions, even in infinite-dimensional ones. In the latter case, the search space is typically afunction space, and one calculates theFréchet derivativeof the functional to be minimized to determine the descent direction.
[6]That gradient descent works in any number of dimensions (finite number at least) can be seen as a consequence of theCauchy-Schwarz inequality. That article proves that the magnitude of the inner (dot) product of two vectors of any dimension is maximized when they are colinear. In the case of gradient descent, that would be when the vector of independent variable adjustments is proportional to the gradient vector of partial derivatives.
The gradient descent can take many iterations to compute a local minimum with a requiredaccuracy, if thecurvaturein different directions is very different for the given function. For such functions,preconditioning, which changes the geometry of the space to shape the function level sets likeconcentric circles, cures the slow convergence. Constructing and applying preconditioning can be computationally expensive, however.
The gradient descent can be combined with aline search, finding the locally optimal step sizeγ{\displaystyle \gamma }on every iteration. Performing the line search can be time-consuming. Conversely, using a fixed smallγ{\displaystyle \gamma }can yield poor convergence.
Methods based onNewton's methodand inversion of theHessianusingconjugate gradienttechniques can be better alternatives.
[15][16]Generally, such methods converge in fewer iterations, but the cost of each iteration is higher. An example is theBFGS methodwhich consists in calculating on every step a matrix by which the gradient vector is multiplied to go into a "better" direction, combined with a more sophisticatedline searchalgorithm, to find the "best" value ofγ.
{\displaystyle \gamma .}For extremely large problems, where the computer-memory issues dominate, a limited-memory method such asL-BFGSshould be used instead of BFGS or the steepest descent.
Gradient descent can be viewed as applyingEuler's methodfor solvingordinary differential equationsx′(t)=−∇f(x(t)){\displaystyle x'(t)=-\nabla f(x(t))}to agradient flow. In turn, this equation may be derived as an optimal controller[17]for the control systemx′(t)=u(t){\displaystyle x'(t)=u(t)}withu(t){\displaystyle u(t)}given in feedback formu(t)=−∇f(x(t)){\displaystyle u(t)=-\nabla f(x(t))}. However, don't let this analogy to mean that the convergence of the gradient descent in the discrete setting follows from results known for the gradient flow: while the gradient flow has very good convergence guarantees, the situation in the discrete setting is more complicated and one has to be careful in choosing a good learning rate. More generally, it is misleading to think that the good properties of some ODE will automatically guarantee good properties of a corresponding discrete version.
Modifications[edit]Gradient descent can converge to a local minimum and slow down in a neighborhood of asaddle point. Even for unconstrained quadratic minimization, gradient descent develops a zig-zag pattern of subsequent iterates as iterations progress, resulting in slow convergence. Multiple modifications of gradient descent have been proposed to address these deficiencies.
Fast gradient methods[edit]Yurii Nesterovhas proposed[18]a simple modification that enables faster convergence for convex problems and has been since further generalized. For unconstrained smooth problems the method is called thefast gradient method(FGM) or theaccelerated gradient method(AGM). Specifically, if the differentiable functionF{\displaystyle F}is convex and∇F{\displaystyle \nabla F}isLipschitz, and it is not assumed thatF{\displaystyle F}isstrongly convex, then the error in the objective value generated at each stepk{\displaystyle k}by the gradient descent method will bebounded byO(1k){\textstyle {\mathcal {O}}\left({\tfrac {1}{k}}\right)}. Using the Nesterov acceleration technique, the error decreases atO(1k2){\textstyle {\mathcal {O}}\left({\tfrac {1}{k^{2}}}\right)}.
[19]It is known that the rateO(k−2){\displaystyle {\mathcal {O}}\left({k^{-2}}\right)}for the decrease of thecost functionis optimal for first-order optimization methods. Nevertheless, there is the opportunity to improve the algorithm by reducing the constant factor. Theoptimized gradient method(OGM)[20]reduces that constant by a factor of two and is an optimal first-order method for large-scale problems.
[21]For constrained or non-smooth problems, Nesterov's FGM is called thefast proximal gradient method(FPGM), an acceleration of theproximal gradient method.
Momentum orheavy ballmethod[edit]Trying to break the zig-zag pattern of gradient descent, themomentum or heavy ball methoduses a momentum term in analogy to a heavy ball sliding on the surface of values of the function being minimized,[5]or to mass movement inNewtonian dynamicsthrough aviscousmedium in a conservative force field.
[22]Gradient descent with momentum remembers the solution update at each iteration, and determines the next update as alinear combinationof the gradient and the previous update. For unconstrained quadratic minimization, a theoretical convergence rate bound of the heavy ball method is asymptotically the same as that for the optimalconjugate gradient method.
[5]This technique is used instochastic gradient descentand as an extension to thebackpropagationalgorithms used to trainartificial neural networks.
[23][24]In the direction of updating, stochastic gradient descent adds a stochastic property. The weights can be used to calculate the derivatives.
Extensions[edit]Gradient descent can be extended to handleconstraintsby including aprojectiononto the set of constraints. This method is only feasible when the projection is efficiently computable on a computer. Under suitable assumptions, this method converges. This method is a specific case of theforward-backward algorithmfor monotone inclusions (which includesconvex programmingandvariational inequalities).
[25]Gradient descent is a special case ofmirror descentusing the squared Euclidean distance as the givenBregman divergence.
[26]See also[edit]References[edit]Further reading[edit]External links[edit]Navigation menuSearch
