old id = 1144
Statistical Machine Learning Group
unknown
https://sml-group.cc

SearchStatistical Machine Learning GroupResearch groupUniversity College LondonWe are a research group at UCL’sCentre for Artificial Intelligence. Our research expertise is in data-efficient machine learning, probabilistic modeling, and autonomous decision making. Applications focus on robotics, climate science, and sustainable development.
If you are interested in joining the team, please check out ouropenings.
Meet the TeamPrincipal InvestigatorsMarc DeisenrothDeepMind Chair of Machine Learning and Artificial IntelligenceMachine learning, Gaussian processes, Reinforcement learning, Robotics, Meta learningAdministratorsSophie OstlerAdministratorResearch FellowsSo TakaoSenior Research FellowMachine learning, Climate science, Fluid mechanics, Geometric mechanicsYasemin BekiroğluSenior Research FellowMachine learning, RoboticsPhD StudentsDaniel Ramos Macedo Antunes De SouzaPhD StudentMachine learning, Gaussian processesJackie KayPhD StudentMachine learning, Robotics, Fairness, Ethical AI, Reinforcement LearningJacob MenickPhD StudentMachine learning, Generative models, Large-scale deep learning, Variational inference, Information theory, SparsityJake CunninghamPhD StudentMachine learningJames WilsonPhD StudentMachine learning, Gaussian processes, Bayesian optimization, Practical approximate inferenceJanith PetangodaPhD StudentMachine learning, Meta learning, Differential geometry, Reinforcement learningMihaela RoscaPhD StudentGenerative models, Reinforcement learning, Natural language processing, Scalable and safe machine learning.
Samuel CohenPhD StudentMachine learning, Optimal transport, Gaussian processesSicelukwanda ZwanePhD StudentMachine learning, Robotics, Transfer Learning, Reinforcement LearningYicheng LuoPhD StudentMeta-learning, Probabilistic Programming, Reinforcement Learning, Deep Generative ModelsProject StudentsBengt LofgrenMSc Project StudentChristopher TanMEng Project StudentMaria KaprosMEng Project StudentRares-Ioan IordanMSc Project StudentRonald MacEachernMSc Project StudentSean NassimihaMSc Project StudentWilliam BankesMSc Project StudentAffiliatesChristina WinklerPhD Student @ TU MunichFabian PaischerPhD Student @ JKU LinzK. S. Sesh KumarResearch AssociateMachine learning, Discrete optimization, Differential privacy, SubmodularityMirgahney H. MohamedPhD StudentComputer vision, Uncertainty estimationOscar KeyPhD StudentProbabilistic modeling, Approximate inference, Machine learning, Climate scienceRendani MbuvhaLecturerAlumniAlexander TereninPhD (10/2018-11/2021)Machine learning, Bayesian theory, Geometric machine learningBenjamin ChamberlainPhD (10/2014-08/2018)Machine learning, Community detection, Representation of graphs, Hyperbolic embeddingsHugh SalimbeniPhD (10/2015-10/2019)Machine learning, Deep probabilistic models, Approximate inferenceK. S. Sesh KumarResearch AssociateMachine learning, Discrete optimization, Differential privacy, SubmodularityRiccardo MoriconiPhD (10/2016-02/2021)Machine learning, Gaussian processes, Bayesian optimizationSanket KamthePhD (10/2016-03/2021)Machine learning, Reinforcement learning, Optimal control, CopulasSimon OlofssonPhD (06/2016-03/2020)Machine learning, Bayesian optimization, Mechanistic models, Model discriminationSteindór SæmundssonPhD (11/2016-11/2021)Machine learning, Gaussian processes, Meta learning, Structural priors, Variational inferenceRecent Blog PostsGaussian processes are a model class for learning unknown functions from data. They are particularly of interest in statistical decision-making systems, due to their ability to quantify and propagate uncertainty. In this work, we study analogs of the popular Matérn class where the domain of the Gaussian process is replaced by a weighted undirected graph.
Recent NewsOur group got three papers accepted at ICML 2021. Very well done to everyone and congratulations to some great work!Congratulations toJackie Kayfor an accepted paper at AIES. Great work!Recent PublicationsRecent & Upcoming TalksFeatured PublicationsVector-valued Gaussian Processes on Riemannian Manifolds via Gauge Independent Projected KernelsAs Gaussian processes are used to answer increasingly complex questions, analytic solutions become scarcer and scarcer. Monte Carlo methods act as a convenient bridge for connecting intractable mathematical expressions with actionable estimates via sampling. Conventional approaches for simulating Gaussian process posteriors view samples as draws from marginal distributions of process values at finite sets of input locations. This distribution-centric characterization leads to generative strategies that scale cubically in the size of the desired random vector. These methods are prohibitively expensive in cases where we would, ideally, like to draw high-dimensional vectors or even continuous sample paths. In this work, we investigate a different line of reasoning: rather than focusing on distributions, we articulate Gaussian conditionals at the level of random variables. We show how this pathwise interpretation of conditioning gives rise to a general family of approximations that lend themselves to efficiently sampling Gaussian process posteriors. Starting from first principles, we derive these methods and analyze the approximation errors they introduce. We, then, ground these results by exploring the practical implications of pathwise conditioning in various applied settings, such as global optimization and reinforcement learning.
Dynamic time warping (DTW) is a useful method for aligning, comparing and combining time series, but it requires them to live in comparable spaces. In this work, we consider a setting in which time series live on different spaces without a sensible ground metric, causing DTW to become ill-defined. To alleviate this, we propose Gromov dynamic time warping (GDTW), a distance between time series on potentially incomparable spaces that avoids the comparability requirement by instead considering intra-relational geometry. We demonstrate its effectiveness at aligning, combining and comparing time series living on incomparable spaces. We further propose a smoothed version of GDTW as a differentiable loss and assess its properties in a variety of settings, including barycentric averaging, generative modeling and imitation learning.
Learning physically structured representations of dynamical systems that include contact between different objects is an important problem for deep learning based approaches in robotics. Black-box neural networks can learn to approximately represent discontinuous dynamics, but typically require impractical quantities of data, and often suffer from pathological behaviour when forecasting for longer time horizons. In this work, we use connections between deep neural networks and differential equations to design a family of deep network architectures for representing contact dynamics between objects. We show that these networks can learn discontinuous contact events in a data-efficient manner from noisy observations in settings which are traditionally difficult for black-box approaches and recent physics inspired neural networks. Our results indicate that an idealised form of touch feedback—which is heavily relied upon by biological systems—is a key component of making this learning problem tractable. Together with the inductive biases introduced through the network architectures, our techniques enable accurate learning of contact dynamics from physical data.
Gaussian processes are a versatile framework for learning unknown functions in a manner that permits one to utilize prior information about their properties. Although many different Gaussian process models are readily available when the input space is Euclidean, the choice is much more limited for Gaussian processes whose input space is an undirected graph. In this work, we leverage the stochastic partial differential equation characterization of Matérn Gaussian processes—a widely-used model class in the Euclidean setting—to study their analog for undirected graphs. We show that the resulting Gaussian processes inherit various attractive properties of their Euclidean and Riemannian analogs, and provide techniques that allow them to be trained using standard methods, such as inducing points. This enables graph Matérn Gaussian processes to be employed in mini-batch, online, and non-conjugate settings, thereby making them more accessible to practitioners and easier to deploy within larger learning frameworks.
Cite{{snippet}}
