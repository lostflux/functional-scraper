old id = 1116
"CBLL, Research Projects, Computational and Biological Learning Lab, Courant Institute, NYU"
unknown
http://www.cs.nyu.edu/~yann/research/norb

NORB: Generic Object Recognition in ImagesThe recognition of generic object categories with invariance to pose, lighting, diverse backgrounds, and the presence of clutter is one of the major challenges of Computer Vision.
We are developing learning systems that can recognize generic object purely from their shape, independently of pose, illumination, and surrounding clutter.
The NORB dataset (NYU Object Recognition Benchmark) contains stereo image pairs of 50 uniform-colored toys under 36 azimuths, 9 elevations, and 6 lighting conditions (for a total of 194,400 individual images).
The objects were 10 instances of 5 generic categories:four-legged animals, human figures, airplanes, trucks, and cars. Five instances of each category were used for training, and the other 5 for testing.
Low-resolution grayscale images of the objects with various amounts of variability and surrounding clutter were used to train and test nearest neighbor methods, Support Vector Machines, and Convolutional Networks, operating on raw pixels or on PCA-derived features.
The NORB DatasetExperiments were conducted with four datasets generated from the normalized object images. The first two datasets were for pure categorization experiments (a somewhat unrealistic task), while the last two were for simultaneous detection/segmentation/recognition experiments.
All datasets used 5 instances of each category for training and the 5 remaining instances for testing. In thenormalizeddataset, 972 images of each instance were used: 9elevations, 18 azimuths (0 to 340 degrees every 20 degrees), and 6 illuminations, for a total of 24,300 training samples and 24,300 test samples. In the variousjittereddatasets, each of the 972 images of each instance were used to generate additional examples by randomly perturbing the position ([-3, +3] pixels), scale (ratio in [0.8, 1.1]), image-plane angle ([-5, 5] degrees), brightness ([-20, 20] shifts of gray levels), contrast ([0.8, 1.3] gain) of the objects during the compositing process. Ten drawings of these random parameters were drawn to generate training sets, and one or two drawings to generate test sets.
[click picture to enlarge]Image capturing setup.
In thetexturedandcluttereddatasets, the objects were placed on randomly picked background images. In those experiments, a 6-th category was added: background images with no objects (results are reported for this 6-way classification). In thetexturedset, the backgrounds were placed at a fixed disparity, akin to a back wall orthogonal to the camera axis at a fixed distance. In thecluttereddatasets, the disparities were adjusted and randomly picked so that the objects appeared placed on highly textured horizontal surfaces at small random distance from that surface. In addition, a randomly picked ``distractor'' object from the training set was placed at the periphery of the image.
Examples of the various lighting conditions for two elevations)normalized-uniform set: 5 classes, centered, unperturbed objects on uniform backgrounds. 24,300 training samples, 24,300 testing samples.
jittered-uniform set: 5 classes, random perturbations, uniform backgrounds. 243,000 training samples (10 drawings) and 24,300 test samples (1 drawing)jittered-textured set: 6 classes (including one background class) random perturbation, natural background textures at fixed disparity. 291,600 training samples (10 drawings), 58,320 testing samples (2 drawings).
jittered-cluttered set: 6 classes (including one background class), random perturbation, highly cluttered background images at random disparities, and randomly placed distractor objects around the periphery. 291,600 training samples (10 drawings), 58,320 testing samples (2 drawings).
[click picture to enlarge]Compositing process. top left: raw image; top right: chroma-keyed object mask; bottom left: cast shadow coefficient mask; bottom right: composite image with cast shadow.
Occlusions of the central object by the distractor occur occasionally in the jittered cluttered set. Most experiments were performed in binocular mode (using left and right images), but some were performed in monocular mode. In monocular experiments, the training set and test set were composed of all left and right images used in the corresponding binocular experiment. Therefore, while the number of training samples was twice higher, the total amount of training data was identical. Examples from thejittered-textured and jittered-cluttered training setare shown belowMethodsOn the Normalized-Uniform DatasetClassifierError RateLinear Classifier, binocular30.2% errorK-Nearest Neighbors on raw stereo images18.4% errorK-Nearest Neighbors on 95 PCA features16.6 errorPairwise Support Vector Machine on raw stereo imagesNO CONVERGENCEPairwise SVM on 48x48 monocular images13.9% errorPairwise SVM on 32x32 monocular images12.6% errorPairwise SVM on 95 PCA features13.3 errorConvolutional Network "LeNet7"6.6% errorConvolutional Network "LeNet7" with pose manifold6.2% errorThe first 60 principal components extracted from the normalized-uniform training set. Unlike with eigen-faces these "eigen-toys" are not recognizable and have symmetries because the objects are seen from every angle in the training set.
On the Jittered-Cluttered DatasetClassifierError RateConvolutional Network "LeNet7", binocular7.8% errorConvolutional Network "LeNet7", monocular20.8% error[click picture to enlarge]Architecture of the convolutional net "LeNet 7". This network has 90,857 trainable parameters and 4.66 Million connections. Each output unit is influenced by a receptive field of 96x96 pixels on the input.
Learned kernels from the first layer of the binocular convolutional network.
Learned kernels from the third layer of the binocular convolutional network.
Results and ExamplesThe convolutional network can be very efficiently applied to all locations on a large input image. For example, applying LeNet 7 to a single 96x96 window requires 4.66 Million multiply-accumulate operations. But applying LeNet 7 to every 96x96 windows, shifted every 12 pixels, over a 240x240 image (169 windows) requires only 47.5 Million multiply-accumulate operations. Applying a non-convolutional classifier with the same complexity to every such 96x96 window would consume 788 Million operations (4.66 million times 169).
The network can be applied to images at multiple scales to ensure scale invariance.
A system was built around LeNet 7, that can detect and recognize objects in natural images. Thesystem runs in real time(a few frames per second) on a laptop connected to a USB camera. Examples of outputs from that system are shown below.
Scenes with objects from the NORB datasetVarious scenes with other objectsNatural ScenesNOTE: The system was not trained on natural images.
A few mistakesExamples with the Internal State of the Convolutional Network
