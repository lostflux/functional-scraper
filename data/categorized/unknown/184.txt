old id = 1655
Kullback–Leibler divergence - Wikipedia
unknown
https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence

Kullback–Leibler divergenceInmathematical statistics, theKullback–Leibler divergence,DKL(P∥Q){\displaystyle D_{\text{KL}}(P\parallel Q)}(also calledrelative entropyandI-divergence[1]), is astatistical distance: a measure of how oneprobability distributionPis different from a second, reference probability distributionQ.
[2][3]A simpleinterpretationof the divergence of P from Q is theexpectedexcesssurprisefrom usingQas a model when the actual distribution isP. While it is a distance, it is not ametric, the most familiar type of distance: it isasymmetricin the two distributions (in contrast tovariation of information), and does not satisfy thetriangle inequality. Instead, in terms ofinformation geometry, it is adivergence,[4]a generalization ofsquareddistance, and for certain classes of distributions (notably anexponential family), it satisfies a generalizedPythagorean theorem(which applies to squared distances).
[5]In the simple case, a relative entropy of 0 indicates that the two distributions in question have identical quantities of information. Relative entropy is a nonnegative function of two distributions or measures. It has diverse applications, both theoretical, such as characterizing the relative(Shannon) entropyin information systems, randomness in continuoustime-series, and information gain when comparing statistical models ofinference; and practical, such as applied statistics,fluid mechanics,neuroscienceandbioinformatics.
ContentsIntroduction and context[edit]Consider two probability distributionsP{\displaystyle P}andQ{\displaystyle Q}. Usually,P{\displaystyle P}represents the data, the observations, or a measured probability distribution. DistributionQ{\displaystyle Q}represents instead a theory, a model, a description or an approximation ofP{\displaystyle P}. The Kullback–Leibler divergence is then interpreted as the average difference of the number of bits required for encoding samples ofP{\displaystyle P}using a code optimized forQ{\displaystyle Q}rather than one optimized forP{\displaystyle P}. Note that the roles ofP{\displaystyle P}andQ{\displaystyle Q}can be reversed in some situations where that is easier to compute, such as with theExpectation–maximization (EM) algorithmandEvidence lower bound (ELBO)computations.
Etymology[edit]The relative entropy was introduced bySolomon KullbackandRichard LeiblerinKullback & Leibler (1951)as "the mean information for discrimination betweenH1{\displaystyle H_{1}}andH2{\displaystyle H_{2}}per observation fromμ1{\displaystyle \mu _{1}}",[6]where one is comparing two probability measuresμ1,μ2{\displaystyle \mu _{1},\mu _{2}}, andH1,H2{\displaystyle H_{1},H_{2}}are the hypotheses that one is selecting from measureμ1,μ2{\displaystyle \mu _{1},\mu _{2}}(respectively). They denoted this byI(1:2){\displaystyle I(1:2)}, and defined the "'divergence' betweenμ1{\displaystyle \mu _{1}}andμ2{\displaystyle \mu _{2}}" as the symmetrized quantityJ(1,2)=I(1:2)+I(2:1){\displaystyle J(1,2)=I(1:2)+I(2:1)}, which had already been defined and used byHarold Jeffreysin 1948.
[7]InKullback (1959), the symmetrized form is again referred to as the "divergence", and the relative entropies in each direction are referred to as a "directed divergences" between two distributions;[8]Kullback preferred the termdiscrimination information.
[9]The term "divergence" is in contrast to a distance (metric), since the symmetrized divergence does not satisfy the triangle inequality.
[10]Numerous references to earlier uses of the symmetrized divergence and to otherstatistical distancesare given inKullback (1959, pp. 6–7, 1.3 Divergence). The asymmetric "directed divergence" has come to be known as the Kullback–Leibler divergence, while the symmetrized "divergence" is now referred to as theJeffreys divergence.
Definition[edit]Fordiscrete probability distributionsP{\displaystyle P}andQ{\displaystyle Q}defined on the sameprobability space,X{\displaystyle {\mathcal {X}}}, the relative entropy fromQ{\displaystyle Q}toP{\displaystyle P}is defined[11]to bewhich is equivalent toIn other words, it is theexpectationof the logarithmic difference between the probabilitiesP{\displaystyle P}andQ{\displaystyle Q}, where the expectation is taken using the probabilitiesP{\displaystyle P}.
Relative entropy is defined so only if for allx{\displaystyle x},Q(x)=0{\displaystyle Q(x)=0}impliesP(x)=0{\displaystyle P(x)=0}(absolute continuity). Else it is often defined as+∞{\displaystyle +\infty },[1]but the value+∞{\displaystyle +\infty }is possible even ifQ(x)≠0{\displaystyle Q(x)\neq 0}everywhere,[12][13]provided thatX{\displaystyle {\mathcal {X}}}is infinite. Analogous comments apply to the continuous and general measure cases defined below.
WheneverP(x){\displaystyle P(x)}is zero the contribution of the corresponding term is interpreted as zero becauseFor distributionsP{\displaystyle P}andQ{\displaystyle Q}of acontinuous random variable, relative entropy is defined to be the integral:[14]: p. 55wherep{\displaystyle p}andq{\displaystyle q}denote theprobability densitiesofP{\displaystyle P}andQ{\displaystyle Q}.
More generally, ifP{\displaystyle P}andQ{\displaystyle Q}are probabilitymeasuresover a setX{\displaystyle {\mathcal {X}}}, andP{\displaystyle P}isabsolutely continuouswith respect toQ{\displaystyle Q}, then the relative entropy fromQ{\displaystyle Q}toP{\displaystyle P}is defined aswheredPdQ{\displaystyle {\frac {dP}{dQ}}}is theRadon–Nikodym derivativeofP{\displaystyle P}with respect toQ{\displaystyle Q}, and provided the expression on the right-hand side exists. Equivalently (by thechain rule), this can be written aswhich is theentropyofP{\displaystyle P}relative toQ{\displaystyle Q}. Continuing in this case, ifμ{\displaystyle \mu }is any measure onX{\displaystyle {\mathcal {X}}}for which the densitiesp=dPdμ{\displaystyle p={\frac {dP}{d\mu }}}andq=dQdμ{\displaystyle q={\frac {dQ}{d\mu }}}exist (meaning thatP{\displaystyle P}andQ{\displaystyle Q}are absolutely continuous with respect toμ{\displaystyle \mu }), then the relative entropy fromQ{\displaystyle Q}toP{\displaystyle P}is given asNote that there is no loss of generality in assuming the existence of densities, sinceμ{\displaystyle \mu }can always taken to beμ=12(P+Q){\displaystyle \mu ={\frac {1}{2}}\left(P+Q\right)}. The logarithms in these formulae are taken tobase2 if information is measured in units ofbits, or to basee{\displaystyle e}if information is measured innats. Most formulas involving relative entropy hold regardless of the base of the logarithm.
Various conventions exist for referring toDKL(P∥Q){\displaystyle D_{\text{KL}}(P\parallel Q)}in words. Often it is referred to as the divergencebetweenP{\displaystyle P}andQ{\displaystyle Q}, but this fails to convey the fundamental asymmetry in the relation. Sometimes, as in this article, it may be described as the divergence ofP{\displaystyle P}fromQ{\displaystyle Q}or as the divergencefromQ{\displaystyle Q}toP{\displaystyle P}. This reflects theasymmetryinBayesian inference, which startsfromapriorQ{\displaystyle Q}and updatestotheposteriorP{\displaystyle P}. Another common way to refer toDKL(P∥Q){\displaystyle D_{\text{KL}}(P\parallel Q)}is as the relative entropy ofP{\displaystyle P}with respect toQ{\displaystyle Q}.
Basic example[edit]Kullback[3]gives the following example (Table 2.1, Example 2.1). LetPandQbe the distributions shown in the table and figure.
Pis the distribution on the left side of the figure, abinomial distributionwithN=2{\displaystyle N=2}andp=0.4{\displaystyle p=0.4}.
Qis the distribution on the right side of the figure, a discrete uniform distribution with the three possible outcomesx={\displaystyle x=}0,1,2(i.e.
X={0,1,2}{\displaystyle {\mathcal {X}}=\{0,1,2\}}), each with probabilityp=1/3{\displaystyle p=1/3}.
Relative entropiesDKL(P∥Q){\displaystyle D_{\text{KL}}(P\parallel Q)}andDKL(Q∥P){\displaystyle D_{\text{KL}}(Q\parallel P)}are calculated as follows. This example uses thenatural logwith basee, designatedlnto get results innats(seeunits of information).
Interpretations[edit]The relative entropy fromQ{\displaystyle Q}toP{\displaystyle P}is often denotedDKL(P∥Q){\displaystyle D_{\text{KL}}(P\parallel Q)}.
In the context ofmachine learning,DKL(P∥Q){\displaystyle D_{\text{KL}}(P\parallel Q)}is often called theinformation gainachieved ifP{\displaystyle P}would be used instead ofQ{\displaystyle Q}which is currently used. By analogy with information theory, it is called therelative entropyofP{\displaystyle P}with respect toQ{\displaystyle Q}. In the context ofcoding theory,DKL(P∥Q){\displaystyle D_{\text{KL}}(P\parallel Q)}can be constructed by measuring the expected number of extrabitsrequired tocodesamples fromP{\displaystyle P}using a code optimized forQ{\displaystyle Q}rather than the code optimized forP{\displaystyle P}.
Expressed in the language ofBayesian inference,DKL(P∥Q){\displaystyle D_{\text{KL}}(P\parallel Q)}is a measure of the information gained by revising one's beliefs from theprior probability distributionQ{\displaystyle Q}to theposterior probability distributionP{\displaystyle P}. In other words, it is the amount of information lost whenQ{\displaystyle Q}is used to approximateP{\displaystyle P}.
[15]In applications,P{\displaystyle P}typically represents the "true" distribution of data, observations, or a precisely calculated theoretical distribution, whileQ{\displaystyle Q}typically represents a theory, model, description, orapproximationofP{\displaystyle P}. In order to find a distributionQ{\displaystyle Q}that is closest toP{\displaystyle P}, we can minimize KL divergence and compute aninformation projection.
While it is astatistical distance, it is not ametric, the most familiar type of distance, but instead it is adivergence.
[4]While metrics are symmetric and generalizelineardistance, satisfying thetriangle inequality, divergences are asymmetric and generalizesquareddistance, in some cases satisfying a generalizedPythagorean theorem. In generalDKL(P∥Q){\displaystyle D_{\text{KL}}(P\parallel Q)}does not equalDKL(Q∥P){\displaystyle D_{\text{KL}}(Q\parallel P)}, and the asymmetry is an important part of the geometry.
[4]Theinfinitesimalform of relative entropy, specifically itsHessian, gives ametric tensorthat equals theFisher information metric; see§ Fisher information metric. Relative entropy satisfies a generalized Pythagorean theorem forexponential families(geometrically interpreted asdually flat manifolds), and this allows one to minimize relative entropy by geometric means, for example byinformation projectionand inmaximum likelihood estimation.
[5]Relative entropy is a special case of a broader class ofstatistical divergencescalledf-divergencesas well as the class ofBregman divergences, and it is the only such divergence over probabilities that is a member of both classes.
Arthur Hobson proved that relative entropy is the only measure of difference between probability distributions that satisfies some desired properties, which are the canonical extension to those appearing in a commonly usedcharacterization of entropy.
[16]Consequently,mutual informationis the only measure of mutual dependence that obeys certain related conditions, since it can be definedin terms of Kullback–Leibler divergence.
Motivation[edit]In information theory, theKraft–McMillan theoremestablishes that any directly decodable coding scheme for coding a message to identify one valuexi{\displaystyle x_{i}}out of a set of possibilitiesX{\displaystyle X}can be seen as representing an implicit probability distributionq(xi)=2−ℓi{\displaystyle q(x_{i})=2^{-\ell _{i}}}overX{\displaystyle X}, whereℓi{\displaystyle \ell _{i}}is the length of the code forxi{\displaystyle x_{i}}in bits. Therefore, relative entropy can be interpreted as the expected extra message-length per datum that must be communicated if a code that is optimal for a given (wrong) distributionQ{\displaystyle Q}is used, compared to using a code based on the true distributionP{\displaystyle P}: it is theexcessentropy.
whereH(P,Q){\displaystyle \mathrm {H} (P,Q)}is thecross entropyofP{\displaystyle P}andQ{\displaystyle Q}, andH(P){\displaystyle \mathrm {H} (P)}is theentropyofP{\displaystyle P}(which is the same as the cross-entropy of P with itself).
The relative entropyDKL(P∥Q){\displaystyle D_{\text{KL}}(P\parallel Q)}can be thought of geometrically as astatistical distance, a measure of how far the distributionQis from the distributionP. Geometrically it is adivergence: an asymmetric, generalized form of squared distance. The cross-entropyH(P,Q){\displaystyle H(P,Q)}is itself such a measurement (formally aloss function), but it cannot be thought of as a distance, sinceH(P,P)=:H(P){\displaystyle H(P,P)=:H(P)}isn't zero. This can be fixed by subtractingH(P){\displaystyle H(P)}to makeDKL(P∥Q){\displaystyle D_{\text{KL}}(P\parallel Q)}agree more closely with our notion of distance, as theexcessloss. The resulting function is asymmetric, and while this can be symmetrized (see§ Symmetrised divergence), the asymmetric form is more useful. See§ Interpretationsfor more on the geometric interpretation.
Relative entropy relates to "rate function" in the theory oflarge deviations.
[17][18]Properties[edit]Duality formula for variational inference[edit]The following result, due to Donsker and Varadhan,[21]is known asDonsker and Varadhan's variational formula.
Theorem [Duality Formula for Variational Inference]LetΘ{\displaystyle \Theta }be a set endowed with an appropriateσ{\displaystyle \sigma }-fieldF{\displaystyle {\mathcal {F}}}, and two probability measuresP{\displaystyle P}andQ{\displaystyle Q}, which formulate twoprobability spaces(Θ,F,P){\displaystyle (\Theta ,{\mathcal {F}},P)}and(Θ,F,Q){\displaystyle (\Theta ,{\mathcal {F}},Q)}withQ≪P{\displaystyle Q\ll P}. (Q≪P{\displaystyle Q\ll P}indicates thatQ{\displaystyle Q}is absolutely continuous with respect toP{\displaystyle P}.) Leth{\displaystyle h}be a real-valued integrablerandom variableon(Θ,F,P){\displaystyle (\Theta ,{\mathcal {F}},P)}. Then the following equality holdsFurther, the supremum on the right-hand side is attained if and only if it holdsalmost surely with respect to probability measureP{\displaystyle P}, wheredQ(θ)dP(θ){\displaystyle {\frac {dQ(\theta )}{dP(\theta )}}}denotes the Radon-Nikodym derivative ofQ{\displaystyle Q}with respect toP{\displaystyle P}.
Proof.
For a short proof assuming integrability ofexp⁡(h){\displaystyle \exp(h)}with respect toP{\displaystyle P}, letQ∗{\displaystyle Q^{*}}haveP{\displaystyle P}-densityexp⁡h(θ)EP[exp⁡h]{\displaystyle {\frac {\exp h(\theta )}{E_{P}[\exp h]}}}. ThenTherefore,where the last inequality follows fromDKL(Q∥Q∗)≥0{\displaystyle D_{\text{KL}}(Q\parallel Q^{*})\geq 0}, for which equality occurs if and only ifQ=Q∗{\displaystyle Q=Q^{*}}. The conclusion follows.
For alternative proof usingmeasure theory, see[22].
Examples[edit]Multivariate normal distributions[edit]Suppose that we have twomultivariate normal distributions, with meansμ0,μ1{\displaystyle \mu _{0},\mu _{1}}and with (non-singular)covariance matricesΣ0,Σ1.
{\displaystyle \Sigma _{0},\Sigma _{1}.}If the two distributions have the same dimension,k{\displaystyle k}, then the relative entropy between the distributions is as follows:[23]: p. 13Thelogarithmin the last term must be taken to baseesince all terms apart from the last are base-elogarithms of expressions that are either factors of the density function or otherwise arise naturally. The equation therefore gives a result measured innats. Dividing the entire expression above byln⁡(2){\displaystyle \ln(2)}yields the divergence inbits.
In a numerical implementation, it is helpful to express the result in terms of the Cholesky decompositionsL0,L1{\displaystyle L_{0},L_{1}}such thatΣ0=L0L0T{\displaystyle \Sigma _{0}=L_{0}L_{0}^{T}}andΣ1=L1L1T{\displaystyle \Sigma _{1}=L_{1}L_{1}^{T}}. Then withM{\displaystyle M}andy{\displaystyle y}solutions to the triangular linear systemsL1M=L0{\displaystyle L_{1}M=L_{0}}, andL1y=μ1−μ0{\displaystyle L_{1}y=\mu _{1}-\mu _{0}},A special case, and a common quantity invariational inference, is the relative entropy between a diagonal multivariate normal, and a standard normal distribution (with zero mean and unit variance):Relation to metrics[edit]While relative entropy is astatistical distance, it is not ametricon the space of probability distributions, but instead it is adivergence.
[4]While metrics are symmetric and generalizelineardistance, satisfying thetriangle inequality, divergences are asymmetric in general and generalizesquareddistance, in some cases satisfying a generalizedPythagorean theorem. In generalDKL(P∥Q){\displaystyle D_{\text{KL}}(P\parallel Q)}does not equalDKL(Q∥P){\displaystyle D_{\text{KL}}(Q\parallel P)}, and while this can be symmetrized (see§ Symmetrised divergence), the asymmetry is an important part of the geometry.
[4]It generates atopologyon the space ofprobability distributions. More concretely, if{P1,P2,…}{\displaystyle \{P_{1},P_{2},\ldots \}}is a sequence of distributions such thatthen it is said thatPinsker's inequalityentails thatwhere the latter stands for the usual convergence intotal variation.
Fisher information metric[edit]Relative entropy is directly related to theFisher information metric. This can be made explicit as follows. Assume that the probability distributionsP{\displaystyle P}andQ{\displaystyle Q}are both parameterized by some (possibly multi-dimensional) parameterθ{\displaystyle \theta }. Consider then two close by values ofP=P(θ){\displaystyle P=P(\theta )}andQ=P(θ0){\displaystyle Q=P(\theta _{0})}so that the parameterθ{\displaystyle \theta }differs by only a small amount from the parameter valueθ0{\displaystyle \theta _{0}}. Specifically, up to first order one has (using theEinstein summation convention)withΔθj=(θ−θ0)j{\displaystyle \Delta \theta _{j}=(\theta -\theta _{0})_{j}}a small change ofθ{\displaystyle \theta }in thej{\displaystyle j}direction, andPj(θ0)=∂P∂θj(θ0){\displaystyle P_{j}\left(\theta _{0}\right)={\frac {\partial P}{\partial \theta _{j}}}(\theta _{0})}the corresponding rate of change in the probability distribution. Since relative entropy has an absolute minimum 0 forP=Q{\displaystyle P=Q}, i.e.
θ=θ0{\displaystyle \theta =\theta _{0}}, it changes only tosecondorder in the small parametersΔθj{\displaystyle \Delta \theta _{j}}. More formally, as for any minimum, the first derivatives of the divergence vanishand by theTaylor expansionone has up to second orderwhere theHessian matrixof the divergencemust bepositive semidefinite. Lettingθ0{\displaystyle \theta _{0}}vary (and dropping the subindex 0) the Hessiangjk(θ){\displaystyle g_{jk}(\theta )}defines a (possibly degenerate)Riemannian metricon theθparameter space, called the Fisher information metric.
Fisher information metric theorem[edit]Whenp(x,ρ){\displaystyle p_{(x,\rho )}}satisfies the following regularity conditions:whereξis independent ofρthen:Variation of information[edit]Another information-theoretic metric isvariation of information, which is roughly a symmetrization ofconditional entropy. It is a metric on the set ofpartitionsof a discreteprobability space.
Relation to other quantities of information theory[edit]Many of the other quantities of information theory can be interpreted as applications of relative entropy to specific cases.
Self-information[edit]Theself-information, also known as theinformation contentof a signal, random variable, oreventis defined as the negative logarithm of theprobabilityof the given outcome occurring.
When applied to adiscrete random variable, the self-information can be represented as[citation needed]is the relative entropy of the probability distributionP(i){\displaystyle P(i)}from aKronecker deltarepresenting certainty thati=m{\displaystyle i=m}— i.e. the number of extra bits that must be transmitted to identifyi{\displaystyle i}if only the probability distributionP(i){\displaystyle P(i)}is available to the receiver, not the fact thati=m{\displaystyle i=m}.
Mutual information[edit]Themutual information,is the relative entropy of the productP(X)P(Y){\displaystyle P(X)P(Y)}of the twomarginal probabilitydistributions from thejoint probability distributionP(X,Y){\displaystyle P(X,Y)}— i.e. the expected number of extra bits that must be transmitted to identifyX{\displaystyle X}andY{\displaystyle Y}if they are coded using only their marginal distributions instead of the joint distribution. Equivalently, if the joint probabilityP(X,Y){\displaystyle P(X,Y)}isknown, it is the expected number of extra bits that must on average be sent to identifyY{\displaystyle Y}if the value ofX{\displaystyle X}is not already known to the receiver.
Shannon entropy[edit]TheShannon entropy,is the number of bits which would have to be transmitted to identifyX{\displaystyle X}fromN{\displaystyle N}equally likely possibilities,lessthe relative entropy of the uniform distribution on therandom variatesofX{\displaystyle X},PU(X){\displaystyle P_{U}(X)}, from the true distributionP(X){\displaystyle P(X)}— i.e.
lessthe expected number of bits saved, which would have had to be sent if the value ofX{\displaystyle X}were coded according to the uniform distributionPU(X){\displaystyle P_{U}(X)}rather than the true distributionP(X){\displaystyle P(X)}.
Conditional entropy[edit]Theconditional entropy[24],is the number of bits which would have to be transmitted to identifyX{\displaystyle X}fromN{\displaystyle N}equally likely possibilities,lessthe relative entropy of the product distributionPU(X)P(Y){\displaystyle P_{U}(X)P(Y)}from the true joint distributionP(X,Y){\displaystyle P(X,Y)}— i.e.
lessthe expected number of bits saved which would have had to be sent if the value ofX{\displaystyle X}were coded according to the uniform distributionPU(X){\displaystyle P_{U}(X)}rather than the conditional distributionP(X|Y){\displaystyle P(X|Y)}ofX{\displaystyle X}givenY{\displaystyle Y}.
Cross entropy[edit]When we have a set of possible events, coming from the distributionp, we can encode them (with alossless data compression) usingentropy encoding. This compresses the data by replacing each fixed-length input symbol with a corresponding unique, variable-length,prefix-free code(e.g.: the events (A, B, C) with probabilities p = (1/2, 1/4, 1/4) can be encoded as the bits (0, 10, 11)). If we know the distributionpin advance, we can devise an encoding that would be optimal (e.g.: usingHuffman coding). Meaning the messages we encode will have the shortest length on average (assuming the encoded events are sampled fromp), which will be equal toShannon's Entropyofp(denoted asH(p){\displaystyle \mathrm {H} (p)}). However, if we use a different probability distribution (q) when creating the entropy encoding scheme, then a larger number ofbitswill be used (on average) to identify an event from a set of possibilities. This new (larger) number is measured by thecross entropybetweenpandq.
Thecross entropybetween twoprobability distributions(pandq) measures the average number ofbitsneeded to identify an event from a set of possibilities, if a coding scheme is used based on a given probability distributionq, rather than the "true" distributionp. The cross entropy for two distributionspandqover the sameprobability spaceis thus defined as follows.
For explicit derivation of this, see theMotivationsection above.
Under this scenario, relative entropies (kl-divergence) can be interpreted as the extra number of bits, on average, that are needed (beyondH(p){\displaystyle \mathrm {H} (p)}) for encoding the events because of usingqfor constructing the encoding scheme instead ofp.
Bayesian updating[edit]InBayesian statistics, relative entropy can be used as a measure of the information gain in moving from aprior distributionto aposterior distribution:p(x)→p(x∣I){\displaystyle p(x)\to p(x\mid I)}. If some new factY=y{\displaystyle Y=y}is discovered, it can be used to update the posterior distribution forX{\displaystyle X}fromp(x∣I){\displaystyle p(x\mid I)}to a new posterior distributionp(x∣y,I){\displaystyle p(x\mid y,I)}usingBayes' theorem:This distribution has a newentropy:which may be less than or greater than the original entropyH(p(x∣I)){\displaystyle \mathrm {H} (p(x\mid I))}. However, from the standpoint of the new probability distribution one can estimate that to have used the original code based onp(x∣I){\displaystyle p(x\mid I)}instead of a new code based onp(x∣y,I){\displaystyle p(x\mid y,I)}would have added an expected number of bits:to the message length. This therefore represents the amount of useful information, or information gain, aboutX{\displaystyle X}, that has been learned by discoveringY=y{\displaystyle Y=y}.
If a further piece of data,Y2=y2{\displaystyle Y_{2}=y_{2}}, subsequently comes in, the probability distribution forx{\displaystyle x}can be updated further, to give a new best guessp(x∣y1,y2,I){\displaystyle p(x\mid y_{1},y_{2},I)}. If one reinvestigates the information gain for usingp(x∣y1,I){\displaystyle p(x\mid y_{1},I)}rather thanp(x∣I){\displaystyle p(x\mid I)}, it turns out that it may be either greater or less than previously estimated:and so the combined information gain doesnotobey the triangle inequality:All one can say is that onaverage, averaging usingp(y2∣y1,x,I){\displaystyle p(y_{2}\mid y_{1},x,I)}, the two sides will average out.
Bayesian experimental design[edit]A common goal inBayesian experimental designis to maximise the expected relative entropy between the prior and the posterior.
[25]When posteriors are approximated to be Gaussian distributions, a design maximising the expected relative entropy is calledBayes d-optimal.
Discrimination information[edit]Relative entropyDKL(p(x∣H1)∥p(x∣H0)){\textstyle D_{\text{KL}}{\bigl (}p(x\mid H_{1})\parallel p(x\mid H_{0}){\bigr )}}can also be interpreted as the expecteddiscrimination informationforH1{\displaystyle H_{1}}overH0{\displaystyle H_{0}}: the mean information per sample for discriminating in favor of a hypothesisH1{\displaystyle H_{1}}against a hypothesisH0{\displaystyle H_{0}}, when hypothesisH1{\displaystyle H_{1}}is true.
[26]Another name for this quantity, given to it byI. J. Good, is the expected weight of evidence forH1{\displaystyle H_{1}}overH0{\displaystyle H_{0}}to be expected from each sample.
The expected weight of evidence forH1{\displaystyle H_{1}}overH0{\displaystyle H_{0}}isnotthe same as the information gain expected per sample about the probability distributionp(H){\displaystyle p(H)}of the hypotheses,Either of the two quantities can be used as autility functionin Bayesian experimental design, to choose an optimal next question to investigate: but they will in general lead to rather different experimental strategies.
On the entropy scale ofinformation gainthere is very little difference between near certainty and absolute certainty—coding according to a near certainty requires hardly any more bits than coding according to an absolute certainty. On the other hand, on thelogitscale implied by weight of evidence, the difference between the two is enormous – infinite perhaps; this might reflect the difference between being almost sure (on a probabilistic level) that, say, theRiemann hypothesisis correct, compared to being certain that it is correct because one has a mathematical proof. These two different scales ofloss functionfor uncertainty arebothuseful, according to how well each reflects the particular circumstances of the problem in question.
Principle of minimum discrimination information[edit]The idea of relative entropy as discrimination information led Kullback to propose the Principle of.mw-parser-output .vanchor>:target~.vanchor-text{background-color:#b1d2ff}Minimum Discrimination Information(MDI): given new facts, a new distributionf{\displaystyle f}should be chosen which is as hard to discriminate from the original distributionf0{\displaystyle f_{0}}as possible; so that the new data produces as small an information gainDKL(f∥f0){\displaystyle D_{\text{KL}}(f\parallel f_{0})}as possible.
For example, if one had a prior distributionp(x,a){\displaystyle p(x,a)}overx{\displaystyle x}anda{\displaystyle a}, and subsequently learnt the true distribution ofa{\displaystyle a}wasu(a){\displaystyle u(a)}, then the relative entropy between the new joint distribution forx{\displaystyle x}anda{\displaystyle a},q(x∣a)u(a){\displaystyle q(x\mid a)u(a)}, and the earlier prior distribution would be:i.e. the sum of the relative entropy ofp(a){\displaystyle p(a)}the prior distribution fora{\displaystyle a}from the updated distributionu(a){\displaystyle u(a)}, plus the expected value (using the probability distributionu(a){\displaystyle u(a)}) of the relative entropy of the prior conditional distributionp(x∣a){\displaystyle p(x\mid a)}from the new conditional distributionq(x∣a){\displaystyle q(x\mid a)}. (Note that often the later expected value is called theconditional relative entropy(orconditional Kullback-Leibler divergence) and denoted byDKL(q(x∣a)∥p(x∣a)){\displaystyle D_{\text{KL}}(q(x\mid a)\parallel p(x\mid a))}[3][24]: p. 22) This is minimized ifq(x∣a)=p(x∣a){\displaystyle q(x\mid a)=p(x\mid a)}over the whole support ofu(a){\displaystyle u(a)}; and we note that this result incorporates Bayes' theorem, if the new distributionu(a){\displaystyle u(a)}is in fact a δ function representing certainty thata{\displaystyle a}has one particular value.
MDI can be seen as an extension ofLaplace'sPrinciple of Insufficient Reason, and thePrinciple of Maximum EntropyofE.T. Jaynes. In particular, it is the natural extension of the principle of maximum entropy from discrete to continuous distributions, for which Shannon entropy ceases to be so useful (seedifferential entropy), but the relative entropy continues to be just as relevant.
In the engineering literature, MDI is sometimes called thePrinciple of Minimum Cross-Entropy(MCE) orMinxentfor short. Minimising relative entropy fromm{\displaystyle m}top{\displaystyle p}with respect tom{\displaystyle m}is equivalent to minimizing the cross-entropy ofp{\displaystyle p}andm{\displaystyle m}, sincewhich is appropriate if one is trying to choose an adequate approximation top{\displaystyle p}. However, this is just as oftennotthe task one is trying to achieve. Instead, just as often it ism{\displaystyle m}that is some fixed prior reference measure, andp{\displaystyle p}that one is attempting to optimise by minimisingDKL(p∥m){\displaystyle D_{\text{KL}}(p\parallel m)}subject to some constraint. This has led to some ambiguity in the literature, with some authors attempting to resolve the inconsistency by redefining cross-entropy to beDKL(p∥m){\displaystyle D_{\text{KL}}(p\parallel m)}, rather thanH(p,m){\displaystyle \mathrm {H} (p,m)}.
Relationship to available work[edit]Surprisals[27]add where probabilities multiply. The surprisal for an event of probabilityp{\displaystyle p}is defined ass=kln⁡(1/p){\displaystyle s=k\ln(1/p)}. Ifk{\displaystyle k}is{1,1/ln⁡2,1.38×10−23}{\displaystyle \left\{1,1/\ln 2,1.38\times 10^{-23}\right\}}then surprisal is in{{\displaystyle \{}nats, bits, orJ/K}{\displaystyle J/K\}}so that, for instance, there areN{\displaystyle N}bits of surprisal for landing all "heads" on a toss ofN{\displaystyle N}coins.
Best-guess states (e.g. for atoms in a gas) are inferred by maximizing theaverage surprisalS{\displaystyle S}(entropy) for a given set of control parameters (like pressureP{\displaystyle P}or volumeV{\displaystyle V}). This constrainedentropy maximization, both classically[28]and quantum mechanically,[29]minimizesGibbsavailability in entropy units[30]A≡−kln⁡(Z){\displaystyle A\equiv -k\ln(Z)}whereZ{\displaystyle Z}is a constrained multiplicity orpartition function.
When temperatureT{\displaystyle T}is fixed, free energy (T×A{\displaystyle T\times A}) is also minimized. Thus ifT,V{\displaystyle T,V}and number of moleculesN{\displaystyle N}are constant, theHelmholtz free energyF≡U−TS{\displaystyle F\equiv U-TS}(whereU{\displaystyle U}is energy andS{\displaystyle S}is entropy) is minimized as a system "equilibrates." IfT{\displaystyle T}andP{\displaystyle P}are held constant (say during processes in your body), theGibbs free energyG=U+PV−TS{\displaystyle G=U+PV-TS}is minimized instead. The change in free energy under these conditions is a measure of availableworkthat might be done in the process. Thus available work for an ideal gas at constant temperatureTo{\displaystyle T_{o}}and pressurePo{\displaystyle P_{o}}isW=ΔG=NkToΘ(V/Vo){\displaystyle W=\Delta G=NkT_{o}\Theta (V/V_{o})}whereVo=NkTo/Po{\displaystyle V_{o}=NkT_{o}/P_{o}}andΘ(x)=x−1−ln⁡x≥0{\displaystyle \Theta (x)=x-1-\ln x\geq 0}(see alsoGibbs inequality).
More generally[31]thework availablerelative to some ambient is obtained by multiplying ambient temperatureTo{\displaystyle T_{o}}by relative entropy ornet surprisalΔI≥0,{\displaystyle \Delta I\geq 0,}defined as the average value ofkln⁡(p/po){\displaystyle k\ln(p/p_{o})}wherepo{\displaystyle p_{o}}is the probability of a given state under ambient conditions. For instance, the work available in equilibrating a monatomic ideal gas to ambient values ofVo{\displaystyle V_{o}}andTo{\displaystyle T_{o}}is thusW=ToΔI{\displaystyle W=T_{o}\Delta I}, where relative entropyThe resulting contours of constant relative entropy, shown at right for a mole of Argon at standard temperature and pressure, for example put limits on the conversion of hot to cold as in flame-powered air-conditioning or in the unpowered device to convert boiling-water to ice-water discussed here.
[32]Thus relative entropy measures thermodynamic availability in bits.
Quantum information theory[edit]Fordensity matricesP{\displaystyle P}andQ{\displaystyle Q}on aHilbert space, thequantum relative entropyfromQ{\displaystyle Q}toP{\displaystyle P}is defined to beInquantum information sciencethe minimum ofDKL(P∥Q){\displaystyle D_{\text{KL}}(P\parallel Q)}over all separable statesQ{\displaystyle Q}can also be used as a measure ofentanglementin the stateP{\displaystyle P}.
Relationship between models and reality[edit]Just as relative entropy of "actual from ambient" measures thermodynamic availability, relative entropy of "reality from a model" is also useful even if the only clues we have about reality are some experimental measurements. In the former case relative entropy describesdistance to equilibriumor (when multiplied by ambient temperature) the amount ofavailable work, while in the latter case it tells you about surprises that reality has up its sleeve or, in other words,how much the model has yet to learn.
Although this tool for evaluating models against systems that are accessible experimentally may be applied in any field, its application to selecting astatistical modelviaAkaike information criterionare particularly well described in papers[33]and a book[34]by Burnham and Anderson. In a nutshell the relative entropy of reality from a model may be estimated, to within a constant additive term, by a function of the deviations observed between data and the model's predictions (like themean squared deviation) . Estimates of such divergence for models that share the same additive term can in turn be used to select among models.
When trying to fit parametrized models to data there are various estimators which attempt to minimize relative entropy, such asmaximum likelihoodandmaximum spacingestimators.
[citation needed]Symmetrised divergence[edit]Kullback & Leibler (1951)also considered the symmetrized function:[6]which they referred to as the "divergence", though today the "KL divergence" refers to the asymmetric function (see§ Etymologyfor the evolution of the term). This function is symmetric and nonnegative, and had already been defined and used byHarold Jeffreysin 1948;[7]it is accordingly called theJeffreys divergence.
This quantity has sometimes been used forfeature selectioninclassificationproblems, whereP{\displaystyle P}andQ{\displaystyle Q}are the conditionalpdfsof a feature under two different classes. In the Banking and Finance industries, this quantity is referred to asPopulation Stability Index(PSI), and is used to assess distributional shifts in model features through time.
An alternative is given via theλ{\displaystyle \lambda }divergence,which can be interpreted as the expected information gain aboutX{\displaystyle X}from discovering which probability distributionX{\displaystyle X}is drawn from,P{\displaystyle P}orQ{\displaystyle Q}, if they currently have probabilitiesλ{\displaystyle \lambda }and1−λ{\displaystyle 1-\lambda }respectively.
[clarification needed][citation needed]The valueλ=0.5{\displaystyle \lambda =0.5}gives theJensen–Shannon divergence, defined bywhereM{\displaystyle M}is the average of the two distributions,DJS{\displaystyle D_{JS}}can also be interpreted as the capacity of a noisy information channel with two inputs giving the output distributionsP{\displaystyle P}andQ{\displaystyle Q}. The Jensen–Shannon divergence, like allf-divergences, islocallyproportional to theFisher information metric. It is similar to theHellinger metric(in the sense that it induces the same affine connection on astatistical manifold).
Furthermore, the Jensen-Shannon divergence can be generalized using abstract statistical M-mixtures relying on an abstract mean M.
[35][36]Relationship to other probability-distance measures[edit]There are many other important measures ofprobability distance. Some of these are particularly connected with relative entropy. For example:Other notable measures of distance include theHellinger distance,histogram intersection,Chi-squared statistic,quadratic form distance,match distance,Kolmogorov–Smirnov distance, andearth mover's distance.
[39]Data differencing[edit]Just asabsoluteentropy serves as theoretical background fordatacompression,relativeentropy serves as theoretical background fordatadifferencing– the absolute entropy of a set of data in this sense being the data required to reconstruct it (minimum compressed size), while the relative entropy of a target set of data, given a source set of data, is the data required to reconstruct the targetgiventhe source (minimum size of apatch).
See also[edit]References[edit]External links[edit]Navigation menuSearch
