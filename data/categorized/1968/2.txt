old id = 1139
Convolutional neural network - Wikipedia
1968
https://en.wikipedia.org/wiki/Convolutional_neural_network

Convolutional neural networkIndeep learning, aconvolutional neural network(CNN, orConvNet) is a class ofartificial neural network(ANN), most commonly applied to analyze visual imagery.
[1]CNNs are also known asShift InvariantorSpace Invariant Artificial Neural Networks(SIANN), based on the shared-weight architecture of the convolution kernels or filters that slide along input features and provide translation-equivariantresponses known as feature maps.
[2][3]Counter-intuitively, most convolutional neural networks are onlyequivariant, as opposed toinvariant, to translation.
[4]They have applications inimage and video recognition,recommender systems,[5]image classification,image segmentation,medical image analysis,natural language processing,[6]brain–computer interfaces,[7]and financialtime series.
[8]CNNs areregularizedversions ofmultilayer perceptrons. Multilayer perceptrons usually mean fully connected networks, that is, each neuron in onelayeris connected to all neurons in the nextlayer. The "full connectivity" of these networks make them prone tooverfittingdata. Typical ways of regularization, or preventing overfitting, include: penalizing parameters during training (such as weight decay) or trimming connectivity (skipped connections, dropout, etc.) CNNs take a different approach towards regularization: they take advantage of the hierarchical pattern in data and assemble patterns of increasing complexity using smaller and simpler patterns embossed in their filters. Therefore, on a scale of connectivity and complexity, CNNs are on the lower extreme.
Convolutional networks wereinspiredbybiologicalprocesses[9][10][11][12]in that the connectivity pattern betweenneuronsresembles the organization of the animalvisual cortex. Individualcortical neuronsrespond to stimuli only in a restricted region of thevisual fieldknown as thereceptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.
CNNs use relatively little pre-processing compared to otherimage classification algorithms. This means that the network learns to optimize thefilters(or kernels) through automated learning, whereas in traditional algorithms these filters arehand-engineered. This independence from prior knowledge and human intervention in feature extraction is a major advantage.
ContentsDefinition[edit]Convolutional neural networks are a specialized type of artificial neural networks that use a mathematical operation calledconvolutionin place of general matrix multiplication in at least one of their layers.
[13]They are specifically designed to process pixel data and are used in image recognition and processing.
Architecture[edit]A convolutional neural network consists of an input layer,hidden layersand an output layer. In any feed-forward neural network, any middle layers are called hidden because their inputs and outputs are masked by the activation function and finalconvolution. In a convolutional neural network, the hidden layers include layers that perform convolutions. Typically this includes a layer that performs adot productof the convolution kernel with the layer's input matrix. This product is usually theFrobenius inner product, and its activation function is commonlyReLU. As the convolution kernel slides along the input matrix for the layer, the convolution operation generates a feature map, which in turn contributes to the input of the next layer. This is followed by other layers such as pooling layers, fully connected layers, and normalization layers.
Convolutional layers[edit]In a CNN, the input is atensorwith a shape: (number of inputs) x (input height) x (input width) x (inputchannels). After passing through a convolutional layer, the image becomes abstracted to a feature map, also called an activation map, with shape: (number of inputs) x (feature map height) x (feature map width) x (feature mapchannels).
Convolutional layers convolve the input and pass its result to the next layer. This is similar to the response of a neuron in the visual cortex to a specific stimulus.
[14]Each convolutional neuron processes data only for itsreceptive field. Althoughfully connected feedforward neural networkscan be used to learn features and classify data, this architecture is generally impractical for larger inputs such as high-resolution images. It would require a very high number of neurons, even in a shallow architecture, due to the large input size of images, where each pixel is a relevant input feature. For instance, a fully connected layer for a (small) image of size 100 x 100 has 10,000 weights foreachneuron in the second layer. Instead, convolution reduces the number of free parameters, allowing the network to be deeper.
[15]For example, regardless of image size, using a 5 x 5 tiling region, each with the same shared weights, requires only 25 learnable parameters. Using regularized weights over fewer parameters avoids the vanishing gradients and exploding gradients problems seen duringbackpropagationin traditional neural networks.
[16][17]Furthermore, convolutional neural networks are ideal for data with a grid-like topology (such as images) as spatial relations between separate features are taken into account during convolution and/or pooling.
Pooling layers[edit]Convolutional networks may include local and/or global pooling layers along with traditional convolutional layers. Pooling layers reduce the dimensions of data by combining the outputs of neuron clusters at one layer into a single neuron in the next layer. Local pooling combines small clusters, tiling sizes such as 2 x 2 are commonly used. Global pooling acts on all the neurons of the feature map.
[18][19]There are two common types of pooling in popular use: max and average.
Max poolinguses the maximum value of each local cluster of neurons in the feature map,[20][21]whileaverage poolingtakes the average value.
Fully connected layers[edit]Fully connected layers connect every neuron in one layer to every neuron in another layer. It is the same as a traditionalmultilayer perceptronneural network (MLP). The flattened matrix goes through a fully connected layer to classify the images.
Receptive field[edit]In neural networks, each neuron receives input from some number of locations in the previous layer. In a convolutional layer, each neuron receives input from only a restricted area of the previous layer called the neuron'sreceptive field. Typically the area is a square (e.g. 5 by 5 neurons). Whereas, in a fully connected layer, the receptive field is theentire previous layer. Thus, in each convolutional layer, each neuron takes input from a larger area in the input than previous layers. This is due to applying the convolution over and over, which takes into account the value of a pixel, as well as its surrounding pixels. When using dilated layers, the number of pixels in the receptive field remains constant, but the field is more sparsely populated as its dimensions grow when combining the effect of several layers.
Weights[edit]Each neuron in a neural network computes an output value by applying a specific function to the input values received from the receptive field in the previous layer. The function that is applied to the input values is determined by a vector of weights and a bias (typically real numbers). Learning consists of iteratively adjusting these biases and weights.
The vector of weights and the bias are calledfiltersand represent particularfeaturesof the input (e.g., a particular shape). A distinguishing feature of CNNs is that many neurons can share the same filter. This reduces thememory footprintbecause a single bias and a single vector of weights are used across all receptive fields that share that filter, as opposed to each receptive field having its own bias and vector weighting.
[22]History[edit]CNN are often compared to the way the brain achieves vision processing in livingorganisms.
[23][citation needed]Receptive fields in the visual cortex[edit]Work byHubelandWieselin the 1950s and 1960s showed that catvisual corticescontain neurons that individually respond to small regions of thevisual field. Provided the eyes are not moving, the region of visual space within which visual stimuli affect the firing of a single neuron is known as itsreceptive field.
[24]Neighboring cells have similar and overlapping receptive fields.
[citation needed]Receptive field size and location varies systematically across the cortex to form a complete map of visual space.
[citation needed]The cortex in each hemisphere represents the contralateralvisual field.
[citation needed]Their 1968 paper identified two basic visual cell types in the brain:[10]Hubel and Wiesel also proposed a cascading model of these two types of cells for use in pattern recognition tasks.
[25][24]Neocognitron, origin of the CNN architecture[edit]The "neocognitron"[9]was introduced byKunihiko Fukushimain 1980.
[11][21][26]It was inspired by the above-mentioned work of Hubel and Wiesel. The neocognitron introduced the two basic types of layers in CNNs: convolutional layers, and downsampling layers. A convolutional layer contains units whose receptive fields cover a patch of the previous layer. The weight vector (the set of adaptive parameters) of such a unit is often called a filter. Units can share filters. Downsampling layers contain units whose receptive fields cover patches of previous convolutional layers. Such a unit typically computes the average of the activations of the units in its patch. This downsampling helps to correctly classify objects in visual scenes even when the objects are shifted.
In a variant of the neocognitron called the cresceptron, instead of using Fukushima's spatial averaging, J. Weng et al. introduced a method called max-pooling where a downsampling unit computes the maximum of the activations of the units in its patch.
[27]Max-pooling is often used in modern CNNs.
[28]Several supervised and unsupervised learning algorithms have been proposed over the decades to train the weights of a neocognitron.
[9]Today, however, the CNN architecture is usually trained throughbackpropagation.
Theneocognitronis the first CNN which requires units located at multiple network positions to have shared weights.
Convolutional neural networks were presented at the Neural Information Processing Workshop in 1987, automatically analyzing time-varying signals by replacing learned multiplication with convolution in time, and demonstrated for speech recognition.
[29]Time delay neural networks[edit]Thetime delay neural network(TDNN) was introduced in 1987 byAlex Waibelet al. and was one of the first convolutional networks, as it achieved shift invariance.
[30]It did so by utilizing weight sharing in combination withbackpropagationtraining.
[31]Thus, while also using a pyramidal structure as in the neocognitron, it performed a global optimization of the weights instead of a local one.
[30]TDNNs are convolutional networks that share weights along the temporal dimension.
[32]They allow speech signals to be processed time-invariantly. In 1990 Hampshire and Waibel introduced a variant which performs a two dimensional convolution.
[33]Since these TDNNs operated on spectrograms, the resulting phoneme recognition system was invariant to both shifts in time and in frequency. This inspiredtranslation invariancein image processing with CNNs.
[31]The tiling of neuron outputs can cover timed stages.
[34]TDNNs now achieve the best performance in far distance speech recognition.
[35]Max pooling[edit]In 1990 Yamaguchi et al. introduced the concept of max pooling, which is a fixed filtering operation that calculates and propagates the maximum value of a given region. They did so by combining TDNNs with max pooling in order to realize a speaker independent isolated word recognition system.
[20]In their system they used several TDNNs per word, one for eachsyllable. The results of each TDNN over the input signal were combined using max pooling and the outputs of the pooling layers were then passed on to networks performing the actual word classification.
Image recognition with CNNs trained by gradient descent[edit]A system to recognize hand-writtenZIP Codenumbers[36]involved convolutions in which the kernel coefficients had been laboriously hand designed.
[37]Yann LeCunet al. (1989)[37]used back-propagation to learn the convolution kernel coefficients directly from images of hand-written numbers. Learning was thus fully automatic, performed better than manual coefficient design, and was suited to a broader range of image recognition problems and image types.
This approach became a foundation of moderncomputer vision.
LeNet-5[edit]LeNet-5, a pioneering 7-level convolutional network byLeCunet al. in 1998,[38]that classifies digits, was applied by several banks to recognize hand-written numbers on checks (British English:cheques) digitized in 32x32 pixel images. The ability to process higher-resolution images requires larger and more layers of convolutional neural networks, so this technique is constrained by the availability of computing resources.
Shift-invariant neural network[edit]Similarly, a shift invariant neural network was proposed by W. Zhang et al. for image character recognition in 1988.
[2][3]The architecture and training algorithm were modified in 1991[39]and applied for medical image processing[40]and automatic detection of breast cancer inmammograms.
[41]A different convolution-based design was proposed in 1988[42]for application to decomposition of one-dimensionalelectromyographyconvolved signals via de-convolution. This design was modified in 1989 to other de-convolution-based designs.
[43][44]Neural abstraction pyramid[edit]The feed-forward architecture of convolutional neural networks was extended in the neural abstraction pyramid[45]by lateral and feedback connections. The resulting recurrent convolutional network allows for the flexible incorporation of contextual information to iteratively resolve local ambiguities. In contrast to previous models, image-like outputs at the highest resolution were generated, e.g., for semantic segmentation, image reconstruction, and object localization tasks.
GPU implementations[edit]Although CNNs were invented in the 1980s, their breakthrough in the 2000s required fast implementations ongraphics processing units(GPUs).
In 2004, it was shown by K. S. Oh and K. Jung that standard neural networks can be greatly accelerated on GPUs. Their implementation was 20 times faster than an equivalent implementation onCPU.
[46][28]In 2005, another paper also emphasised the value ofGPGPUformachine learning.
[47]The first GPU-implementation of a CNN was described in 2006 by K. Chellapilla et al. Their implementation was 4 times faster than an equivalent implementation on CPU.
[48]Subsequent work also used GPUs, initially for other types of neural networks (different from CNNs), especially unsupervised neural networks.
[49][50][51][52]In 2010, Dan Ciresan et al. atIDSIAshowed that even deep standard neural networks with many layers can be quickly trained on GPU by supervised learning through the old method known asbackpropagation. Their network outperformed previous machine learning methods on theMNISThandwritten digits benchmark.
[53]In 2011, they extended this GPU approach to CNNs, achieving an acceleration factor of 60, with impressive results.
[18]In 2011, they used such CNNs on GPU to win an image recognition contest where they achieved superhuman performance for the first time.
[54]Between May 15, 2011 and September 30, 2012, their CNNs won no less than four image competitions.
[55][28]In 2012, they also significantly improved on the best performance in the literature for multiple imagedatabases, including theMNIST database, the NORB database, the HWDB1.0 dataset (Chinese characters) and theCIFAR10 dataset(dataset of 60000 32x32 labeledRGB images).
[21]Subsequently, a similar GPU-based CNN by Alex Krizhevsky et al. won theImageNet Large Scale Visual Recognition Challenge2012.
[56]A very deep CNN with over 100 layers by Microsoft won the ImageNet 2015 contest.
[57]Intel Xeon Phi implementations[edit]Compared to the training of CNNs usingGPUs, not much attention was given to theIntel Xeon Phicoprocessor.
[58]A notable development is a parallelization method for training convolutional neural networks on the Intel Xeon Phi, named Controlled Hogwild with Arbitrary Order of Synchronization (CHAOS).
[59]CHAOS exploits both the thread- andSIMD-level parallelism that is available on the Intel Xeon Phi.
Distinguishing features[edit]In the past, traditionalmultilayer perceptron(MLP) models were used for image recognition.
[example needed]However, the full connectivity between nodes caused thecurse of dimensionality, and was computationally intractable with higher-resolution images. A 1000×1000-pixel image withRGB colorchannels has 3 million weights per fully-connected neuron, which is too high to feasibly process efficiently at scale.
For example, inCIFAR-10, images are only of size 32×32×3 (32 wide, 32 high, 3 color channels), so a single fully connected neuron in the first hidden layer of a regular neural network would have 32*32*3 = 3,072 weights. A 200×200 image, however, would lead to neurons that have 200*200*3 = 120,000 weights.
Also, such network architecture does not take into account the spatial structure of data, treating input pixels which are far apart in the same way as pixels that are close together. This ignoreslocality of referencein data with a grid-topology (such as images), both computationally and semantically. Thus, full connectivity of neurons is wasteful for purposes such as image recognition that are dominated byspatially localinput patterns.
Convolutional neural networks are variants of multilayer perceptrons, designed to emulate the behavior of avisual cortex. These models mitigate the challenges posed by the MLP architecture by exploiting the strong spatially local correlation present in natural images. As opposed to MLPs, CNNs have the following distinguishing features:Together, these properties allow CNNs to achieve better generalization onvision problems. Weight sharing dramatically reduces the number offree parameterslearned, thus lowering the memory requirements for running the network and allowing the training of larger, more powerful networks.
Building blocks[edit]A CNN architecture is formed by a stack of distinct layers that transform the input volume into an output volume (e.g. holding the class scores) through a differentiable function. A few distinct types of layers are commonly used. These are further discussed below.
Convolutional layer[edit]The convolutional layer is the core building block of a CNN. The layer's parameters consist of a set of learnablefilters(orkernels), which have a small receptive field, but extend through the full depth of the input volume. During the forward pass, each filter isconvolvedacross the width and height of the input volume, computing thedot productbetween the filter entries and the input, producing a 2-dimensionalactivation mapof that filter. As a result, the network learns filters that activate when it detects some specific type offeatureat some spatial position in the input.
[62][nb 1]Stacking the activation maps for all filters along the depth dimension forms the full output volume of the convolution layer. Every entry in the output volume can thus also be interpreted as an output of a neuron that looks at a small region in the input and shares parameters with neurons in the same activation map.
Local connectivity[edit]When dealing with high-dimensional inputs such as images, it is impractical to connect neurons to all neurons in the previous volume because such a network architecture does not take the spatial structure of the data into account. Convolutional networks exploit spatially local correlation by enforcing asparse local connectivitypattern between neurons of adjacent layers: each neuron is connected to only a small region of the input volume.
The extent of this connectivity is ahyperparametercalled thereceptive fieldof the neuron. The connections arelocal in space(along width and height), but always extend along the entire depth of the input volume. Such an architecture ensures that the learnt filters produce the strongest response to a spatially local input pattern.
Spatial arrangement[edit]Threehyperparameterscontrol the size of the output volume of the convolutional layer: the depth,stride, and padding size:The spatial size of the output volume is a function of the input volume sizeW{\displaystyle W}, the kernel field sizeK{\displaystyle K}of the convolutional layer neurons, the strideS{\displaystyle S}, and the amount of zero paddingP{\displaystyle P}on the border. The number of neurons that "fit" in a given volume is then:If this number is not aninteger, then the strides are incorrect and the neurons cannot be tiled to fit across the input volume in asymmetricway. In general, setting zero padding to beP=(K−1)/2{\textstyle P=(K-1)/2}when the stride isS=1{\displaystyle S=1}ensures that the input volume and output volume will have the same size spatially. However, it is not always completely necessary to use all of the neurons of the previous layer. For example, a neural network designer may decide to use just a portion of padding.
Parameter sharing[edit]A parameter sharing scheme is used in convolutional layers to control the number of free parameters. It relies on the assumption that if a patch feature is useful to compute at some spatial position, then it should also be useful to compute at other positions. Denoting a single 2-dimensional slice of depth as adepth slice, the neurons in each depth slice are constrained to use the same weights and bias.
Since all neurons in a single depth slice share the same parameters, the forward pass in each depth slice of the convolutional layer can be computed as aconvolutionof the neuron's weights with the input volume.
[nb 2]Therefore, it is common to refer to the sets of weights as a filter (or akernel), which is convolved with the input. The result of this convolution is anactivation map, and the set of activation maps for each different filter are stacked together along the depth dimension to produce the output volume. Parameter sharing contributes to thetranslation invarianceof the CNN architecture.
[4]Sometimes, the parameter sharing assumption may not make sense. This is especially the case when the input images to a CNN have some specific centered structure; for which we expect completely different features to be learned on different spatial locations. One practical example is when the inputs are faces that have been centered in the image: we might expect different eye-specific or hair-specific features to be learned in different parts of the image. In that case it is common to relax the parameter sharing scheme, and instead simply call the layer a "locally connected layer".
Pooling layer[edit]Another important concept of CNNs is pooling, which is a form of non-lineardown-sampling. There are several non-linear functions to implement pooling, wheremax poolingis the most common. Itpartitionsthe input image into a set of rectangles and, for each such sub-region, outputs the maximum.
Intuitively, the exact location of a feature is less important than its rough location relative to other features. This is the idea behind the use of pooling in convolutional neural networks. The pooling layer serves to progressively reduce the spatial size of the representation, to reduce the number of parameters,memory footprintand amount of computation in the network, and hence to also controloverfitting. This is known as down-sampling. It is common to periodically insert a pooling layer between successive convolutional layers (each one typically followed by an activation function, such as aReLU layer) in a CNN architecture.
[62]: 460–461While pooling layers contribute to local translation invariance, they do not provide global translation invariance in a CNN, unless a form of global pooling is used.
[4][61]The pooling layer commonly operates independently on every depth, or slice, of the input and resizes it spatially. A very common form of max pooling is a layer with filters of size 2×2, applied with a stride of 2, which subsamples every depth slice in the input by 2 along both width and height, discarding 75% of the activations:In addition to max pooling, pooling units can use other functions, such asaveragepooling orℓ2-normpooling. Average pooling was often used historically but has recently fallen out of favor compared to max pooling, which generally performs better in practice.
[64]Due to the effects of fast spatial reduction of the size of the representation,[which?]there is a recent trend towards using smaller filters[65]or discarding pooling layers altogether.
[66]"Region of Interest" pooling (also known as RoI pooling) is a variant of max pooling, in which output size is fixed and input rectangle is a parameter.
[67]Pooling is a downsampling method and an important component of convolutional neural networks forobject detectionbased on the Fast R-CNN[68]architecture.
ReLU layer[edit]ReLU is the abbreviation ofrectified linear unit, which applies the non-saturatingactivation functionf(x)=max(0,x){\textstyle f(x)=\max(0,x)}.
[56]It effectively removes negative values from an activation map by setting them to zero.
[69]It introducesnonlinearitiesto thedecision functionand in the overall network without affecting the receptive fields of the convolution layers.
Other functions can also be used to increase nonlinearity, for example the saturatinghyperbolic tangentf(x)=tanh⁡(x){\displaystyle f(x)=\tanh(x)},f(x)=|tanh⁡(x)|{\displaystyle f(x)=|\tanh(x)|}, and thesigmoid functionσ(x)=(1+e−x)−1{\textstyle \sigma (x)=(1+e^{-x})^{-1}}. ReLU is often preferred to other functions because it trains the neural network several times faster without a significant penalty togeneralizationaccuracy.
[70]Fully connected layer[edit]After several convolutional and max pooling layers, the final classification is done via fully connected layers. Neurons in a fully connected layer have connections to all activations in the previous layer, as seen in regular (non-convolutional)artificial neural networks. Their activations can thus be computed as anaffine transformation, withmatrix multiplicationfollowed by a bias offset (vector additionof a learned or fixed bias term).
[citation needed]Loss layer[edit]The "loss layer", or "loss function", specifies howtrainingpenalizes the deviation between the predicted output of the network, and thetruedata labels (during supervised learning). Variousloss functionscan be used, depending on the specific task.
TheSoftmaxloss function is used for predicting a single class ofKmutually exclusive classes.
[nb 3]Sigmoidcross-entropyloss is used for predictingKindependent probability values in[0,1]{\displaystyle [0,1]}.
Euclideanloss is used forregressingtoreal-valuedlabels(−∞,∞){\displaystyle (-\infty ,\infty )}.
Hyperparameters[edit]Hyperparameters are various settings that are used to control the learning process. CNNs use morehyperparametersthan a standard multilayer perceptron (MLP).
Kernel size[edit]The kernel is the number of pixels processed together. It is typically expressed as the kernel's dimensions, e.g., 2x2, or 3x3.
Padding[edit]Padding is the addition of (typically) 0-valued pixels on the borders of an image. This is done so that the border pixels are not undervalued (lost) from the output because they would ordinarily participate in only a single receptive field instance. The padding applied is typically one less than the corresponding kernel dimension. For example, a convolutional layer using 3x3 kernels would receive a 2-pixel pad on all sides of the image.
[71]Stride[edit]The stride is the number of pixels that the analysis window moves on each iteration. A stride of 2 means that each kernel is offset by 2 pixels from its predecessor.
Number of filters[edit]Since feature map size decreases with depth, layers near the input layer tend to have fewer filters while higher layers can have more. To equalize computation at each layer, the product of feature valuesvawith pixel position is kept roughly constant across layers. Preserving more information about the input would require keeping the total number of activations (number of feature maps times number of pixel positions) non-decreasing from one layer to the next.
The number of feature maps directly controls the capacity and depends on the number of available examples and task complexity.
Filter size[edit]Common filter sizes found in the literature vary greatly, and are usually chosen based on the data set.
The challenge is to find the right level of granularity so as to create abstractions at the proper scale, given a particular data set, and withoutoverfitting.
Pooling type and size[edit]Max poolingis typically used, often with a 2x2 dimension. This implies that the input is drasticallydownsampled, reducing processing cost.
Large input volumes may warrant 4×4 pooling in the lower layers.
[72]Greater poolingreduces the dimensionof the signal, and may result in unacceptableinformation loss. Often, non-overlapping pooling windows perform best.
[64]Dilation[edit]Dilation involves ignoring pixels within a kernel. This reduces processing/memory potentially without significant signal loss. A dilation of 2 on a 3x3 kernel expands the kernel to 7x7, while still processing 9 (evenly spaced) pixels. Accordingly, dilation of 4 expands the kernel to 15x15.
[73]Translation equivariance and aliasing[edit]It is commonly assumed that CNNs are invariant to shifts of the input. Convolution or pooling layers within a CNN that do not have a stride greater than one are indeedequivariantto translations of the input.
[61]However, layers with a stride greater than one ignore theNyquist-Shannon sampling theoremand might lead toaliasingof the input signal[61]While, in principle, CNNs are capable of implementing anti-aliasing filters, it has been observed that this does not happen in practice[74]and yield models that are not equivariant to translations. Furthermore, if a CNN makes use of fully connected layers, translation equivariance does not imply translation invariance, as the fully connected layers are not invariant to shifts of the input.
[75][4]One solution for complete translation invariance is avoiding any down-sampling throughout the network and applying global average pooling at the last layer.
[61]Additionally, several other partial solutions have been proposed, such asanti-aliasingbefore downsampling operations,[76]spatial transformer networks,[77]data augmentation, subsampling combined with pooling,[4]andcapsule neural networks.
[78]Evaluation[edit]The accuracy of the final model based on a sub-part of the dataset set apart at the start, often called a test-set. Other times methods such ask-fold cross-validationare applied. Other strategies include usingconformal prediction.
[79][80]Regularization methods[edit]Regularizationis a process of introducing additional information to solve anill-posed problemor to preventoverfitting. CNNs use various types of regularization.
Empirical[edit]Dropout[edit]Because a fully connected layer occupies most of the parameters, it is prone to overfitting. One method to reduce overfitting isdropout.
[81][82]At each training stage, individual nodes are either "dropped out" of the net (ignored) with probability1−p{\displaystyle 1-p}or kept with probabilityp{\displaystyle p}, so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed. Only the reduced network is trained on the data in that stage. The removed nodes are then reinserted into the network with their original weights.
In the training stages,p{\displaystyle p}is usually 0.5; for input nodes, it is typically much higher because information is directly lost when input nodes are ignored.
At testing time after training has finished, we would ideally like to find a sample average of all possible2n{\displaystyle 2^{n}}dropped-out networks; unfortunately this is unfeasible for large values ofn{\displaystyle n}. However, we can find an approximation by using the full network with each node's output weighted by a factor ofp{\displaystyle p}, so theexpected valueof the output of any node is the same as in the training stages. This is the biggest contribution of the dropout method: although it effectively generates2n{\displaystyle 2^{n}}neural nets, and as such allows for model combination, at test time only a single network needs to be tested.
By avoiding training all nodes on all training data, dropout decreases overfitting. The method also significantly improves training speed. This makes the model combination practical, even fordeep neural networks. The technique seems to reduce node interactions, leading them to learn more robust features[clarification needed]that better generalize to new data.
DropConnect[edit]DropConnect is the generalization of dropout in which each connection, rather than each output unit, can be dropped with probability1−p{\displaystyle 1-p}. Each unit thus receives input from a random subset of units in the previous layer.
[83]DropConnect is similar to dropout as it introduces dynamic sparsity within the model, but differs in that the sparsity is on the weights, rather than the output vectors of a layer. In other words, the fully connected layer with DropConnect becomes a sparsely connected layer in which the connections are chosen at random during the training stage.
Stochastic pooling[edit]A major drawback to Dropout is that it does not have the same benefits for convolutional layers, where the neurons are not fully connected.
In stochastic pooling,[84]the conventionaldeterministicpooling operations are replaced with a stochastic procedure, where the activation within each pooling region is picked randomly according to amultinomial distribution, given by the activities within the pooling region. This approach is free of hyperparameters and can be combined with other regularization approaches, such as dropout anddata augmentation.
An alternate view of stochastic pooling is that it is equivalent to standard max pooling but with many copies of an input image, each having small localdeformations. This is similar to explicitelastic deformationsof the input images,[85]which delivers excellent performance on theMNIST data set.
[85]Using stochastic pooling in a multilayer model gives an exponential number of deformations since the selections in higher layers are independent of those below.
Artificial data[edit]Because the degree of model overfitting is determined by both its power and the amount of training it receives, providing a convolutional network with more training examples can reduce overfitting. Because these networks are usually trained with all available data, one approach is to either generate new data from scratch (if possible) or perturb existing data to create new ones. For example, input images can be cropped, rotated, or rescaled to create new examples with the same labels as the original training set.
[86]Explicit[edit]Early stopping[edit]One of the simplest methods to prevent overfitting of a network is to simply stop the training before overfitting has had a chance to occur. It comes with the disadvantage that the learning process is halted.
Number of parameters[edit]Another simple way to prevent overfitting is to limit the number of parameters, typically by limiting the number of hidden units in each layer or limiting network depth. For convolutional networks, the filter size also affects the number of parameters. Limiting the number of parameters restricts the predictive power of the network directly, reducing the complexity of the function that it can perform on the data, and thus limits the amount of overfitting. This is equivalent to a "zero norm".
Weight decay[edit]A simple form of added regularizer is weight decay, which simply adds an additional error, proportional to the sum of weights (L1 norm) or squared magnitude (L2 norm) of the weight vector, to the error at each node. The level of acceptable model complexity can be reduced by increasing the proportionality constant('alpha' hyperparameter), thus increasing the penalty for large weight vectors.
L2 regularization is the most common form of regularization. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective. The L2 regularization has the intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors. Due to multiplicative interactions between weights and inputs this has the useful property of encouraging the network to use all of its inputs a little rather than some of its inputs a lot.
L1 regularization is also common. It makes the weight vectors sparse during optimization. In other words, neurons with L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the noisy inputs. L1 with L2 regularization can be combined; this is calledelastic net regularization.
Max norm constraints[edit]Another form of regularization is to enforce an absolute upper bound on the magnitude of the weight vector for every neuron and useprojected gradient descentto enforce the constraint. In practice, this corresponds to performing the parameter update as normal, and then enforcing the constraint by clamping the weight vectorw→{\displaystyle {\vec {w}}}of every neuron to satisfy‖w→‖2<c{\displaystyle \|{\vec {w}}\|_{2}<c}. Typical values ofc{\displaystyle c}are order of 3–4. Some papers report improvements[87]when using this form of regularization.
Hierarchical coordinate frames[edit]Pooling loses the precise spatial relationships between high-level parts (such as nose and mouth in a face image). These relationships are needed for identity recognition. Overlapping the pools so that each feature occurs in multiple pools, helps retain the information. Translation alone cannot extrapolate the understanding of geometric relationships to a radically new viewpoint, such as a different orientation or scale. On the other hand, people are very good at extrapolating; after seeing a new shape once they can recognize it from a different viewpoint.
[88]An earlier common way to deal with this problem is to train the network on transformed data in different orientations, scales, lighting, etc. so that the network can cope with these variations. This is computationally intensive for large data-sets. The alternative is to use a hierarchy of coordinate frames and use a group of neurons to represent a conjunction of the shape of the feature and its pose relative to theretina. The pose relative to the retina is the relationship between the coordinate frame of the retina and the intrinsic features' coordinate frame.
[89]Thus, one way to represent something is to embed the coordinate frame within it. This allows large features to be recognized by using the consistency of the poses of their parts (e.g. nose and mouth poses make a consistent prediction of the pose of the whole face). This approach ensures that the higher-level entity (e.g. face) is present when the lower-level (e.g. nose and mouth) agree on its prediction of the pose. The vectors of neuronal activity that represent pose ("pose vectors") allow spatial transformations modeled as linear operations that make it easier for the network to learn the hierarchy of visual entities and generalize across viewpoints. This is similar to the way the humanvisual systemimposes coordinate frames in order to represent shapes.
[90]Applications[edit]Image recognition[edit]CNNs are often used inimage recognitionsystems. In 2012 anerror rateof 0.23% on theMNIST databasewas reported.
[21]Another paper on using CNN for image classification reported that the learning process was "surprisingly fast"; in the same paper, the best published results as of 2011 were achieved in the MNIST database and the NORB database.
[18]Subsequently, a similar CNN calledAlexNet[91]won theImageNet Large Scale Visual Recognition Challenge2012.
When applied tofacial recognition, CNNs achieved a large decrease in error rate.
[92]Another paper reported a 97.6% recognition rate on "5,600 still images of more than 10 subjects".
[12]CNNs were used to assessvideo qualityin an objective way after manual training; the resulting system had a very lowroot mean square error.
[34]TheImageNet Large Scale Visual Recognition Challengeis a benchmark in object classification and detection, with millions of images and hundreds of object classes. In the ILSVRC 2014,[93]a large-scale visual recognition challenge, almost every highly ranked team used CNN as their basic framework. The winnerGoogLeNet[94](the foundation ofDeepDream) increased the mean averageprecisionof object detection to 0.439329, and reduced classification error to 0.06656, the best result to date. Its network applied more than 30 layers. That performance of convolutional neural networks on the ImageNet tests was close to that of humans.
[95]The best algorithms still struggle with objects that are small or thin, such as a small ant on a stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters, an increasingly common phenomenon with modern digital cameras. By contrast, those kinds of images rarely trouble humans. Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained categories such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this.
[citation needed]In 2015 a many-layered CNN demonstrated the ability to spot faces from a wide range of angles, including upside down, even when partially occluded, with competitive performance. The network was trained on a database of 200,000 images that included faces at various angles and orientations and a further 20 million images without faces. They used batches of 128 images over 50,000 iterations.
[96]Video analysis[edit]Compared to image data domains, there is relatively little work on applying CNNs to video classification. Video is more complex than images since it has another (temporal) dimension. However, some extensions of CNNs into the video domain have been explored. One approach is to treat space and time as equivalent dimensions of the input and perform convolutions in both time and space.
[97][98]Another way is to fuse the features of two convolutional neural networks, one for the spatial and one for the temporal stream.
[99][100][101]Long short-term memory(LSTM)recurrentunits are typically incorporated after the CNN to account for inter-frame or inter-clip dependencies.
[102][103]Unsupervised learningschemes for training spatio-temporal features have been introduced, based on Convolutional Gated RestrictedBoltzmann Machines[104]and Independent Subspace Analysis.
[105]Natural language processing[edit]CNNs have also been explored fornatural language processing. CNN models are effective for various NLP problems and achieved excellent results insemantic parsing,[106]search query retrieval,[107]sentence modeling,[108]classification,[109]prediction[110]and other traditional NLP tasks.
[111]Compared to traditional language processing methods such asrecurrent neural networks, CNNs can represent different contextual realities of language that do not rely on a series-sequence assumption, while RNNs are better suitable when classical time serie modeling is required[112][113][114][115]Anomaly Detection[edit]A CNN with 1-D convolutions was used on time series in the frequency domain (spectral residual) by an unsupervised model to detect anomalies in the time domain.
[116]Drug discovery[edit]CNNs have been used indrug discovery. Predicting the interaction between molecules and biologicalproteinscan identify potential treatments. In 2015, Atomwise introduced AtomNet, the first deep learning neural network forstructure-based drug design.
[117]The system trains directly on 3-dimensional representations of chemical interactions. Similar to how image recognition networks learn to compose smaller, spatially proximate features into larger, complex structures,[118]AtomNet discovers chemical features, such asaromaticity,sp3carbons, andhydrogen bonding. Subsequently, AtomNet was used to predict novel candidatebiomoleculesfor multiple disease targets, most notably treatments for theEbola virus[119]andmultiple sclerosis.
[120]Health risk assessment and biomarkers of aging discovery[edit]CNNs can be naturally tailored to analyze a sufficiently large collection oftime seriesdata representing one-week-long human physical activity streams augmented by the rich clinical data (including the death register, as provided by, e.g., theNHANESstudy). A simple CNN was combined with Cox-Gompertzproportional hazards modeland used to produce a proof-of-concept example of digitalbiomarkers of agingin the form of all-causes-mortality predictor.
[121]Checkers game[edit]CNNs have been used in the game ofcheckers. From 1999 to 2001,Fogeland Chellapilla published papers showing how a convolutional neural network could learn to playcheckerusing co-evolution. The learning process did not use prior human professional games, but rather focused on a minimal set of information contained in the checkerboard: the location and type of pieces, and the difference in number of pieces between the two sides. Ultimately, the program (Blondie24) was tested on 165 games against players and ranked in the highest 0.4%.
[122][123]It also earned a win against the programChinookat its "expert" level of play.
[124]Go[edit]CNNs have been used incomputer Go. In December 2014, Clark andStorkeypublished a paper showing that a CNN trained by supervised learning from a database of human professional games could outperformGNU Goand win some games againstMonte Carlo tree searchFuego 1.1 in a fraction of the time it took Fuego to play.
[125]Later it was announced that a large 12-layer convolutional neural network had correctly predicted the professional move in 55% of positions, equalling the accuracy of a6 danhuman player. When the trained convolutional network was used directly to play games of Go, without any search, it beat the traditional search programGNU Goin 97% of games, and matched the performance of theMonte Carlo tree searchprogram Fuego simulating ten thousand playouts (about a million positions) per move.
[126]A couple of CNNs for choosing moves to try ("policy network") and evaluating positions ("value network") driving MCTS were used byAlphaGo, the first to beat the best human player at the time.
[127]Time series forecasting[edit]Recurrent neural networks are generally considered the best neural network architectures for time series forecasting (and sequence modeling in general), but recent studies show that convolutional networks can perform comparably or even better.
[128][8]Dilated convolutions[129]might enable one-dimensional convolutional neural networks to effectively learn time series dependences.
[130]Convolutions can be implemented more efficiently than RNN-based solutions, and they do not suffer from vanishing (or exploding) gradients.
[131]Convolutional networks can provide an improved forecasting performance when there are multiple similar time series to learn from.
[132]CNNs can also be applied to further tasks in time series analysis (e.g., time series classification[133]or quantile forecasting[134]).
Cultural Heritage and 3D-datasets[edit]As archaeological findings likeclay tabletswithcuneiform writingare increasingly acquired using3D scannersfirst benchmark datasets are becoming available likeHeiCuBeDa[135]providing almost 2.000 normalized 2D- and 3D-datasets prepared with theGigaMesh Software Framework.
[136]Socurvature-based measures are used in conjunction with Geometric Neural Networks (GNNs) e.g. for period classification of those clay tablets being among the oldest documents of human history.
[137][138]Fine-tuning[edit]For many applications, the training data is less available. Convolutional neural networks usually require a large amount of training data in order to avoidoverfitting. A common technique is to train the network on a larger data set from a related domain. Once the network parameters have converged an additional training step is performed using the in-domain data to fine-tune the network weights, this is known astransfer learning. Furthermore, this technique allows convolutional network architectures to successfully be applied to problems with tiny training sets.
[139]Human interpretable explanations[edit]End-to-end training and prediction are common practice incomputer vision. However, human interpretable explanations are required forcritical systemssuch as aself-driving cars.
[140]With recent advances invisual salience,spatial attention, andtemporal attention, the most critical spatial regions/temporal instants could be visualized to justify the CNN predictions.
[141][142]Related architectures[edit]Deep Q-networks[edit]A deep Q-network (DQN) is a type of deep learning model that combines a deep neural network withQ-learning, a form ofreinforcement learning. Unlike earlier reinforcement learning agents, DQNs that utilize CNNs can learn directly from high-dimensional sensory inputs via reinforcement learning.
[143]Preliminary results were presented in 2014, with an accompanying paper in February 2015.
[144]The research described an application toAtari 2600gaming. Other deep reinforcement learning models preceded it.
[145]Deep belief networks[edit]Convolutional deep belief networks (CDBN) have structure very similar to convolutional neural networks and are trained similarly to deep belief networks. Therefore, they exploit the 2D structure of images, like CNNs do, and make use of pre-training likedeep belief networks. They provide a generic structure that can be used in many image and signal processing tasks. Benchmark results on standard image datasets like CIFAR[146]have been obtained using CDBNs.
[147]Notable libraries[edit]See also[edit]Notes[edit]References[edit]External links[edit]Navigation menuSearch
