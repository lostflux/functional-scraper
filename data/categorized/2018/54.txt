old id = 871
35 Innovators Under 35: Max Shulaker | MIT Technology Review
2018
https://www.technologyreview.com/innovator/max-shulaker

PioneersThey’re making crucial advances in fusion power, computing, biosensors, and robotics.
Full listCategoriesPast YearsMax ShulakerHis work with carbon nanotubes could lead to the next generation of computers.
Max Shulakerhas built the world’s first functionalcomputer using carbon nanotubes, and he has also designed systems that combine computing, memory, and sensing directly on top of one another on a single chip. Together, these new technologies could increase energy efficiency in computers up to 1,000-fold and make possible a whole world of new devices like low-cost medical sensors.
Carbon nanotubes are “basically a straw that is one carbon atom thin,” says Shulaker. For 20 years, researchers have talked about using them to replace traditional silicon chips. But turning carbon-nanotube transistors and wires into actual devices has proved difficult, and Shulaker has solved several problems to make them work. He developed a way to remove poorly formed carbon nanotubes during production, devised new processes to create wafers of nanotube-based transistors using regular industrial fabrication plants, and invented a new design ensuring that chips built with a certain number of defective tubes are guaranteed to work.
These breakthroughs represent a significant step toward next-generation computer systems far more energy efficient than anything built to date.
Shulaker’s drive led him to work on another feat: monolithic, three-dimensional nanosystems. These fuse microprocessor, memory, and additional functional layers directly on top of one another using carbon nanotubes. Traditional designs have the microchip and memory on separate chips connected by wires. But moving massive quantities of data between those chips leads to slowdowns and wasted energy—a problem known in the industry as “the memory wall.” Shulaker’s 3D nanosystems solve it.
byRuss JuskalianShareGeorge BoatengHe built a smartphone-based platform to teach young people to code— and tackle Africa’s IT skills gap in the process.
George Boateng’s venture,SuaCode.ai, emerged largely by accident. In 2013, as an undergraduate at Dartmouth College, he’d teamed up with a group of friends to launch a summer innovation boot camp for high school students in their native Ghana. When the donated laptops they’d gotten for the course broke down a few years later, they were in a fix: only a quarter of the students had laptops of their own, and buying more would overwhelm their budget. All the students, however, had smartphones—so Boateng and his colleagues redesigned the coding module to fit a five-inch screen.
The experience went so well that it hatched a spinoff: in 2018, Boateng and cofounder Victor Kumbol ran their first pilot of SuaCode, an eight-weeksmartphone-­based course. The course, which teaches Processing, a Java-based language, now has more than 600 graduates from two dozen countries. Boateng, currently a doctoral candidate in applied machine learning at ETH Zurich, also engineered an English- and French-speaking AI-­powered teaching assistant named Kwame—a nod to Ghana’s first president, Kwame Nkrumah. “His pan-­Africanist vision resonates with our goal of empowering youth across the continent,” Boateng says.
Boateng’s hope is that the automated nature of the course will help it reach far more students—providing early exposure to coding that will serve as a bedrock for further education and ultimately help them land well-­paying jobs in tech.
byJonathan W. RosenShareAnna GoldieShe uses AI to design microchips much more quickly than humans can.
Anna Goldiedesignscomputer chips using reinforcement learning, an AI technique that works by repeatedly generating solutions from an artificial neural network. The system then provides feedback to the network, “reinforcing” pathways that lead to successful outcomes and weakening pathways that don’t.
Building on this branch of machine learning, which also underlies the most successful methods for teaching computers to play games like chess or Go, has allowed Goldie and her team to speed up the process of chip design.
Modern chips are composed of millions or even billions of components. Some perform computations; others store data in short-term memory. Figuring out the best way to place all the components in a chip’s layout can take engineers weeks or even months—they must try to minimize power consumption and area but also maximize performance, all while making sure that traffic between components doesn’t get too congested.
Goldie’s AI can, in under six hours, come up with solutions that match—or even outperform—the ones that people were able to develop.
In early 2021, Goldie collaborated with Google engineers to produce physical versions of her layouts for Google’s latest artificial-intelligence chip. By using AI to design better hardware faster, she hopes to pave the way for AI advances that further improve and accelerate hardware design, creating a symbiotic loop between hardware and artificial intelligence.
“It generates these very strange, alien-looking layouts,” she says. “The chip designers were like: What if it goes wrong?” It didn’t.
byWill Douglas HeavenShareAdnan MehonicMemristors can be a new and more efficient building block of modern computers.
Memristorsare a novel type of electric circuit element that were first theorized to exist in 1971. In 2008, researchers at Hewlett-Packard identified them for the first time, in nano­devices made from titanium dioxide, but the technology has not replaced flash memory as initially predicted.
Resistors are elements of a circuit that control the flow of electric current. A memristor, as its name suggests, is like an adjustable resistor with memory. Turn the power off, and a memristor “remembers” the most recent resistance it had. That holds the promise of faster, more efficient chips that integrate memory with logic.
Adnan Mehonicis developing memristors out of silicon oxide, the material most commonly used in computer chips. His most straightforward goal is to make dense, low-power, high-speed memory. More ambitiously, he is using the physics of memristors to implement in-memory computing and brain-like functionalities for future neuromorphic systems.
Among other applications, memristors could greatly improve the energy efficiency of AI systems, proponents say. “Crossbar arrays” of memristors, says Mehonic, could perform deep-learning tasks using one-500th as much energy as current hardware. A startup he cofounded concluded a $1.9 million financing round in March.
byPatrick Howell O'NeillShareMarc MiskinHe figured out how to give motion to microscopic robots.
Marc Miskinhas given life to a technology that’s eluded the world’s top nanoscientists for decades: robots too small to see. Miskin’s tiny bots piggyback on more than 50 years of electronics innovation, making it possible to build silicon chips smaller than the width of a human hair. The challenge was getting these circuits, which function as the robots’ brains, to move: previous approaches to connecting them to a pair of microscopic legs required too much voltage to work at such a tiny scale.
His technique fabricates legs from sheets of platinum a dozen or so atoms thick, capped on one side with an even smaller layer of titanium. When activated with a current—generated by solar cells attached to the robot brain—the platinum bends, causing the bot to march forward. Miskin’s initial prototype, which he developed as a postdoctoral researcher at Cornell University, requires only one-fifth of a volt to move and measures just 40 by 40 microns—smaller than many single-celled microorganisms. It’s recognized byGuinness World Recordsas the smallest ever walking robot, and a million of them at a time can be fabricated on a single 10-centimeter wafer.
For now, Miskin’s robot does little more than prance under a microscope, but his lab at the University of Pennsylvania, where he’s a professor of electrical and systems engineering, is fabricating limbs for a “smart bot” with programmable memory, developed with researchers at the University of Michigan. In the longer term, Miskin envisions tiny bots being used to engineer new materials, rid crops of pests, or even act as microscopic surgeons, programmed to eliminate cancer cells one by one.
byJonathan W. RosenShareNako NakatsukaHer miniature biosensors could give scientists better insight into depression and dementia.
Nako Nakatsukais building tiny sensors that can detect chemical changes in the brain and other parts of the body more precisely than ever before. Scientists can use such information to help them understand and treat conditions like depression and dementia. Compared with earlier sensors, Nakatsuka’s are better at differentiating between structurally similar chemicals, like neurotransmitters and their precursors and metabolites.
For now, her sensors are used to take measurements on samples in the lab, but the technology is being refined to work directly in the body and on a wider range of chemicals.
Nakatsuka built her sensors using molecules called aptamers, which can be designed to have strong affinity for specific targets. She first used an aptamer constructed from DNA that changes its shape in the presence of serotonin, a neurotransmitter that plays an important role in bodily functions like sleep and appetite, and in conditions like depression and obsessive-compulsive disorder.
Later she developed a way to attach the aptamer to the opening of a tiny pipette, just 10 nanometers in diameter, hooked up to an electrical circuit. As the aptamer changes its shape in the presence of serotonin, it alters the electrical current. The sensor can measure samples in brain fluid or tissue, or potentially directly next to individual neurons in a lab dish or in the brain.
“This might help us to better understand Parkinson’s and other diseases,” Nakatsuka says. Her sensors could be used to monitor how neurons in or from a patient with such a disease function in real time. And since aptamers can be used in all sorts of tests, Nakatsuka’s technology could lead to faster, cheaper, and more accurate detection for all kinds of medical conditions and infections.
byRuss JuskalianShareMoses NamaraWorking to break down the barriers keeping young Black people from careers in AI.
Moses Namaraknew two fundamental truths: first, that misuses of AI disproportionately harm Black communities around the world, and second, that Black people are underrepresented in university AI programs. Just1.8% of studentsenrolled in computer science PhD programs in the United States were Black in the 2018-2019 school year, and the numbers were only marginally better for master’s students.
Namara knew something else, too: that the barriers to entry are often rooted in resources, and that some of those resources were things a mentorship network could provide. “One is just information,” he says. For example: applicants need to know which research opportunities to pursue as undergrads, which university programs and professors best suit their interests, and what resources might be out there to help with the expensive process of actually applying. “If you don’t know where to look for the information, then that’s the number one step that you’re going to fail,” he says.
So in 2018 Namara co-­created theBlack in Artificial Intelligence graduate application mentoring programto help students applying to graduate school. The program, run through the resource group Black in AI, has mentored 400 applicants, 200 of whom have been accepted to competitive AI programs. It provides an array of resources: mentorship from current PhD students and professors, CV evaluations, and advice on where to apply. Namara now sees the mentorship system evolving to the next logical step: helping Black PhD and master’s students find that first job.
byAbby OhlheiserShareDavid RolnickHe's employing artificial intelligence in the fight against climate change.
In 2019, as a postdoctoral researcher at the University of Pennsylvania,David Rolnickwas lead author of an influentialreportthat described various ways machine learning could reduce greenhouse-gas emissions and help society adapt to climate change, from predicting energy needs to managing forests to modeling planet-scale weather systems. His coauthors included DeepMind cofounderDemis Hassabisand Turing Award winnerYoshua Bengio. That year, Rolnick was a lead organizer for the first workshops on climate change at three leading AI conferences, and lead organizer of an event on AI at the United Nations Climate Change Conference.
“David Rolnick has been hugely influential in convening AI practitioners to work on climate change,” saysAndrew Ng, a cofounder of Google Brain and former chief scientist at Baidu. “By helping shape a vision of how AI could help climate change and tirelessly organizing a community around it, he has catalyzed a significant amount of activity on this important topic.”Rolnick now leads a group at McGill University that uses different AI techniques to attack problems related to climate.
For example, data relevant to climate change—records of infrastructure spending or greenhouse-gas emissions or simply weather patterns—varies enormously between countries. And yet climate needs to be understood at a global level.
“In the Global South there can be less information on infrastructure,” says Rolnick. “So policymakers may have less to go on when it comes to making decisions about energy requirements or managing coastal flood risk.” Countries also have different regulations about what does and does not get recorded. Germany gathers information on where its solar panels are, for example, but the US does not, so researchers are using machine learning to identify solar panels in the US from satellite imagery. Machine learning can also be used to forecast energy demand more accurately than is possible with existing techniques, Rolnick says. This allows energy providers to manage their electricity grids more efficiently.
Rolnick and his colleagues are trying to come up with new machine-learning techniques that could be applied to the study of climate change as well.
For instance, they are building algorithms for transfer learning, which involves training an AI on one set of examples and then transferring what it’s learned to new situations. They are also researching meta-learning, a set of techniques that make AI better at learning from small or incomplete data sets. Rolnick thinks these methods are especially useful for modeling biodiversity because sources of real-world data are so patchy.
Rolnick is also involved in projects that combine machine learning with climate models to simulate complex physical and atmospheric processes like cloud formation. The precise means by which clouds form, and how much they reflect or absorb sunlight, is one of the largest sources of uncertainty in existing climate models—partly because simulating clouds in climate models is computationally intensive. Using machine learning to find patterns in when and where clouds form and how reflective they tend to be—without trying to understand the underlying atmospheric chemistry—allows scientists to run models more quickly.
Rolnick and his collaborators are convinced AI will be a crucial tool in fighting climate change. All the same, there are growing concerns thatmachine learning itself is part of the problem. He acknowledges that training today’s largest AI models consumes large amounts of energy, but he points out that this contributes a tiny fraction of global emissions—and that the real climate risks from AI arguably have more to do with its uses in areas such as oil and gas exploration. “I’m much more worried about negative applications of machine learning than I am about its energy use,” he says.
byWill Douglas HeavenShareJinxing ZhengHe created new physics models for controlling fusion reactions and hot plasma.
Jinxing Zhenghas devised better ways to model the use of powerful magnets for controlling plasma at extreme temperatures, a major advance for fusion-based energy. Zheng’s work is helping China leapfrog the rest of the world and design the largest fusion reactor to date, called theChina Fusion Engineering Test Reactor. CFETR is expected to finish construction and go online before 2035, though it may take five to 10 years to reach full power.
Fusion reactors, based on the energy released when atoms are combined, have great potential for creating clean energy and are inherently safer than existing nuclear power based on fission reactions. But no one has built a practical one, in part because it’s so challenging to contain the necessary plasma, which can reach temperatures of hundreds of millions of degrees Celsius.
Zheng’s innovation amounts to having discovered new theoretical models for understanding how multiple large superconducting magnets can rapidly change their magnetic fields to keep plasma in one place while fusion reactions occur. In 2018, with the help of Zheng’s models, a fusion reactor in Hefei, China, called theExperimental Advanced Superconducting Tokamak—nicknamed “the artificial sun”—controlled plasma at a record temperature of 50 million˚C for 102 seconds.
China’s future CFETR is intended to operate at over 1 gigawatt of power sometime in the 2030s. That’s double the power ofITER, a fusion reactor currently being completed in the south of France with cooperation from countries around the world.
byRuss JuskalianShareMIT Technology ReviewOur in-depth reporting reveals what’s going on now to prepare you for what’s coming next.
Subscribeto support our journalism.
AboutHelp© 2022 MIT Technology Review
