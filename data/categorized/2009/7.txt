old id = 1608
IBM Blue Gene - Wikipedia
2009
https://en.wikipedia.org/wiki/IBM_Blue_Gene

IBM Blue GeneBlue Geneis anIBMproject aimed at designing supercomputers that can reach operating speeds in thepetaFLOPS (PFLOPS)range, with low power consumption.
The project created three generations of supercomputers,Blue Gene/L,Blue Gene/P, andBlue Gene/Q. During their deployment, Blue Gene systems often led theTOP500[1]andGreen500[2]rankings of the most powerful and most power efficient supercomputers, respectively. Blue Gene systems have also consistently scored top positions in theGraph500list.
[3]The project was awarded the 2009National Medal of Technology and Innovation.
[4]As of 2015, IBM seems to have ended the development of the Blue Gene family[5]though no public announcement has been made. IBM's continuing efforts of the supercomputer scene seems to be concentrated aroundOpenPower, using accelerators such asFPGAsandGPUsto battle the end ofMoore's law.
[6]ContentsHistory[edit]In December 1999, IBM announced a US$100 million research initiative for a five-year effort to build a massivelyparallel computer, to be applied to the study of biomolecular phenomena such asprotein folding.
[7]The project had two main goals: to advance our understanding of the mechanisms behind protein folding via large-scale simulation, and to explore novel ideas in massively parallel machine architecture and software. Major areas of investigation included: how to use this novel platform to effectively meet its scientific goals, how to make such massively parallel machines more usable, and how to achieve performance targets at a reasonable cost, through novel machine architectures. The initial design for Blue Gene was based on an early version of theCyclops64architecture, designed byMonty Denneau. The initial research and development work was pursued atIBM T.J. Watson Research Centerand led byWilliam R. Pulleyblank.
[8]At IBM, Alan Gara started working on an extension of theQCDOCarchitecture into a more general-purpose supercomputer: The 4D nearest-neighbor interconnection network was replaced by a network supporting routing of messages from any node to any other; and a parallel I/O subsystem was added. DOE started funding the development of this system and it became known as Blue Gene/L (L for Light); development of the original Blue Gene system continued under the name Blue Gene/C (C for Cyclops) and, later, Cyclops64.
In November 2004 a 16-racksystem, with each rack holding 1,024 compute nodes, achieved first place in theTOP500list, with aLinpackperformance of 70.72 TFLOPS.
[1]It thereby overtook NEC'sEarth Simulator, which had held the title of the fastest computer in the world since 2002. From 2004 through 2007 the Blue Gene/L installation at LLNL[9]gradually expanded to 104 racks, achieving 478 TFLOPS Linpack and 596 TFLOPS peak. The LLNL BlueGene/L installation held the first position in the TOP500 list for 3.5 years, until in June 2008 it was overtaken by IBM's Cell-basedRoadrunnersystem atLos Alamos National Laboratory, which was the first system to surpass the 1 PetaFLOPS mark. The system was built in Rochester, MN IBM plant.
While the LLNL installation was the largest Blue Gene/L installation, many smaller installations followed. In November 2006, there were 27 computers on theTOP500list using the Blue Gene/L architecture. All these computers were listed as having an architecture ofeServer Blue Gene Solution. For example, three racks of Blue Gene/L were housed at theSan Diego Supercomputer Center.
While theTOP500measures performance on a single benchmark application, Linpack, Blue Gene/L also set records for performance on a wider set of applications. Blue Gene/L was the first supercomputer ever to run over 100TFLOPSsustained on a real-world application, namely a three-dimensional molecular dynamics code (ddcMD), simulating solidification (nucleation and growth processes) of molten metal under high pressure and temperature conditions. This achievement won the 2005Gordon Bell Prize.
In June 2006,NNSAand IBM announced that Blue Gene/L achieved 207.3 TFLOPS on a quantum chemical application (Qbox).
[10]At Supercomputing 2006,[11]Blue Gene/L was awarded the winning prize in all HPC Challenge Classes of awards.
[12]In 2007, a team from theIBM Almaden Research Centerand theUniversity of Nevadaran anartificial neural networkalmost half as complex as the brain of a mouse for the equivalent of a second (the network was run at 1/10 of normal speed for 10 seconds).
[13]The name[edit]The name Blue Gene comes from what it was originally designed to do, help biologists understand the processes ofprotein foldingandgene development.
[14]"Blue" is a traditional moniker that IBM uses for many of its products andthe company itself. The original Blue Gene design was renamed "Blue Gene/C" and eventuallyCyclops64. The "L" in Blue Gene/L comes from "Light" as that design's original name was "Blue Light". The "P" version was designed to be apetascaledesign. "Q" is just the letter after "P". There is no Blue Gene/R.
[15]Major features[edit]The Blue Gene/L supercomputer was unique in the following aspects:[16]Architecture[edit]The Blue Gene/L architecture was an evolution of the QCDSP andQCDOCarchitectures. Each Blue Gene/L Compute or I/O node was a singleASICwith associatedDRAMmemory chips. The ASIC integrated two 700 MHzPowerPC 440embedded processors, each with a double-pipeline-double-precisionFloating-Point Unit(FPU), acachesub-system with built-in DRAM controller and the logic to support multiple communication sub-systems. The dual FPUs gave each Blue Gene/L node a theoretical peak performance of 5.6GFLOPS (gigaFLOPS). The two CPUs were notcache coherentwith one another.
Compute nodes were packaged two per compute card, with 16 compute cards plus up to 2 I/O nodes per node board. There were 32 node boards per cabinet/rack.
[17]By the integration of all essential sub-systems on a single chip, and the use of low-power logic, each Compute or I/O node dissipated low power (about 17 watts, including DRAMs). This allowed aggressive packaging of up to 1024 compute nodes, plus additional I/O nodes, in a standard19-inch rack, within reasonable limits of electrical power supply and air cooling. The performance metrics, in terms ofFLOPS per watt, FLOPS per m2of floorspace and FLOPS per unit cost, allowed scaling up to very high performance. With so many nodes, component failures were inevitable. The system was able to electrically isolate faulty components, down to a granularity of half a rack (512 compute nodes), to allow the machine to continue to run.
Each Blue Gene/L node was attached to three parallel communications networks: a3Dtoroidal networkfor peer-to-peer communication between compute nodes, a collective network for collective communication (broadcasts and reduce operations), and a global interrupt network forfast barriers. The I/O nodes, which run theLinuxoperating system, provided communication to storage and external hosts via anEthernetnetwork. The I/O nodes handled filesystem operations on behalf of the compute nodes. Finally, a separate and privateEthernetnetwork provided access to any node for configuration,bootingand diagnostics. To allow multiple programs to run concurrently, a Blue Gene/L system could be partitioned into electronically isolated sets of nodes. The number of nodes in a partition had to be a positiveintegerpower of 2, with at least 25= 32 nodes. To run a program on Blue Gene/L, a partition of the computer was first to be reserved. The program was then loaded and run on all the nodes within the partition, and no other program could access nodes within the partition while it was in use. Upon completion, the partition nodes were released for future programs to use.
Blue Gene/L compute nodes used a minimaloperating systemsupporting a single user program. Only a subset ofPOSIXcalls was supported, and only one process could run at a time on node in co-processor modeâ€”or one process per CPU in virtual mode. Programmers needed to implementgreen threadsin order to simulate local concurrency. Application development was usually performed inC,C++, orFortranusingMPIfor communication. However, some scripting languages such asRuby[18]andPython[19]have been ported to the compute nodes.
IBM has published BlueMatter, the application developed to exercise Blue Gene/L, as open source here.
[20]This serves to document how the torus and collective interfaces were used by applications, and may serve as a base for others to exercise the current generation of supercomputers.
Blue Gene/P[edit]In June 2007, IBM unveiledBlue Gene/P, the second generation of the Blue Gene series of supercomputers and designed through a collaboration that included IBM, LLNL, andArgonne National Laboratory'sLeadership Computing Facility.
[21]Design[edit]The design of Blue Gene/P is a technology evolution from Blue Gene/L. Each Blue Gene/P Compute chip contains fourPowerPC 450processor cores, running at 850 MHz. The cores arecache coherentand the chip can operate as a 4-waysymmetric multiprocessor(SMP). The memory subsystem on the chip consists of small private L2 caches, a central shared 8 MB L3 cache, and dualDDR2memory controllers. The chip also integrates the logic for node-to-node communication, using the same network topologies as Blue Gene/L, but at more than twice the bandwidth. A compute card contains a Blue Gene/P chip with 2 or 4 GB DRAM, comprising a "compute node". A single compute node has a peak performance of 13.6 GFLOPS. 32 Compute cards are plugged into an air-cooled node board. Arackcontains 32 node boards (thus 1024 nodes, 4096 processor cores).
[22]By using many small, low-power, densely packaged chips, Blue Gene/P exceeded thepower efficiencyof other supercomputers of its generation, and at 371MFLOPS/WBlue Gene/P installations ranked at or near the top of theGreen500lists in 2007-2008.
[2]Installations[edit]The following is an incomplete list of Blue Gene/P installations. Per November 2009, theTOP500list contained 15 Blue Gene/P installations of 2-racks (2048 nodes, 8192 processor cores, 23.86TFLOPSLinpack) and larger.
[1]Applications[edit]Blue Gene/Q[edit]The third supercomputer design in the Blue Gene series,Blue Gene/Qhas a peak performance of 20Petaflops,[38]reachingLINPACK benchmarksperformanceof 17Petaflops. Blue Gene/Q continues to expand and enhance the Blue Gene/L and /P architectures.
Design[edit]The Blue Gene/Q Compute chip is an 18-core chip. The64-bitA2processor cores are 4-waysimultaneously multithreaded, and run at 1.6 GHz. Each processor core has aSIMDquad-vectordouble-precisionfloating-pointunit (IBM QPX). 16 Processor cores are used for computing, and a 17th core for operating system assist functions such asinterrupts,asynchronous I/O,MPIpacing andRAS. The 18th core is used as aredundantspare, used to increase manufacturing yield. The spared-out core is shut down in functional operation. The processor cores are linked by a crossbar switch to a 32 MBeDRAML2 cache, operating at half core speed. The L2 cache is multi-versioned, supportingtransactional memoryandspeculative execution, and has hardware support foratomic operations.
[39]L2 cache misses are handled by two built-inDDR3memory controllers running at 1.33 GHz. The chip also integrates logic for chip-to-chip communications in a 5D torus configuration, with 2GB/s chip-to-chip links. The Blue Gene/Q chip is manufactured on IBM's copper SOI process at 45 nm. It delivers a peak performance of 204.8 GFLOPS at 1.6 GHz, drawing about 55 watts. The chip measures 19Ã—19 mm (359.5 mmÂ²) and comprises 1.47 billion transistors. The chip is mounted on a compute card along with 16 GBDDR3DRAM(i.e., 1 GB for each user processor core).
[40]A Q32[41]compute drawer contains 32 compute cards, each water cooled.
[42]A "midplane" (crate) contains 16 Q32 compute drawers for a total of 512 compute nodes, electrically interconnected in a 5D torus configuration (4x4x4x4x2). Beyond the midplane level, all connections are optical. Racks have two midplanes, thus 32 compute drawers, for a total of 1024 compute nodes, 16,384 user cores and 16 TB RAM.
[42]Separate I/O drawers, placed at the top of a rack or in a separate rack, are air cooled and contain 8 compute cards and 8 PCIe expansion slots forInfiniBandor10 Gigabit Ethernetnetworking.
[42]Performance[edit]At the time of the Blue Gene/Q system announcement in November 2011, an initial 4-rack Blue Gene/Q system (4096 nodes, 65536 user processor cores) achieved #17 in theTOP500list[1]with 677.1 TeraFLOPS Linpack, outperforming the original 2007 104-rack BlueGene/L installation described above. The same 4-rack system achieved the top position in theGraph500list[3]with over 250 GTEPS (gigatraversed edges per second). Blue Gene/Q systems also topped theGreen500list of most energy efficient supercomputers with up to 2.1GFLOPS/W.
[2]In June 2012, Blue Gene/Q installations took the top positions in all three lists:TOP500,[1]Graph500[3]andGreen500.
[2]Installations[edit]The following is an incomplete list of Blue Gene/Q installations. Per June 2012, the TOP500 list contained 20 Blue Gene/Q installations of 1/2-rack (512 nodes, 8192 processor cores, 86.35 TFLOPS Linpack) and larger.
[1]At a (size-independent) power efficiency of about 2.1 GFLOPS/W, all these systems also populated the top of the June 2012Green 500list.
[2]Applications[edit]Record-breaking science applications have been run on the BG/Q, the first to cross 10petaflopsof sustained performance. The cosmology simulation framework HACC achieved almost 14 petaflops with a 3.6 trillion particle benchmark run,[63]while the Cardioid code,[64][65]which models the electrophysiology of the human heart, achieved nearly 12 petaflops with a near real-time simulation, both onSequoia. A fully compressible flow solver has also achieved 14.4 PFLOP/s (originally 11 PFLOP/s) on Sequoia, 72% of the machine's nominal peak performance.
[66]See also[edit]References[edit]External links[edit]Navigation menuSearch
