old id = 1118
"Talks and Posters"
2009
http://www.cs.nyu.edu/~yann/talks

Talks by Members of CBLLWe very strongly encourage interested readers to use the DjVu versions: they display instantly, load much faster, and have no compatibility problems.
2011-05-05: LIP6 ML Seminar, Universite P. et M. CurieSLIDES (PDF)A similar seminar was given at the Xerox Research Center Europe in Grenoble the following day.
Title: Learning Feature Hierarchies for VisionAbstract: Intelligent perceptual tasks such as vision and audition require the construction of good internal representations. Theoretical and empirical evidence suggest that the perceptual world is best represented by a multi-stage hierarchy in which features in successive stages are increasingly global, invariant, and abstract. An important challenge for Machine Learning is to devise "deep learning" methods for multi-stage architecture than can automatically learn good feature hierarchies from labeled and unlabeled data.
A class of such methods that combine unsupervised sparse coding, and supervised refinement will be described. We demonstrate the use of these deep learning methods to train convolutional networks (ConvNets). ConvNets are biologically-inspired architectures consisting of multiple stages of filter banks, interspersed with non-linear operations, and spatial pooling operations, analogous to the simple cells and complex cells in the mammalian visual cortex.
A number of applications will be shown through videos and live demos, including a category-level object recognition system that can be trained on the fly, a pedestrian detector, and system that recognizes human activities in videos, and a trainable vision system for off-road mobile robot navigation.
A new kind of "dataflow" computer architecture, dubbed NeuFlow, was designed to run these algorithms (and other vision and recognition algorithms) in real time on small, embeddable platforms. an FPGA implementation of NeuFlow running various vision applications will be shown. An ASIC is being designed in collaboration with e-lab at Yale, which will be capable of 700 Giga-operations per second for less than 3 Watts.
2011-03-02: Rutgers/Yahoo ML SeminarSLIDES (PDF)VIDEOTitle: Learning Feature Hierarchies for Vision2011-02-23: Princeton Plasma Physics LabTitle:Building Artificial Vision Systems with Machine Learning.
This talk is for a general technical audience.
SLIDES (PDF)VIDEODirect link to .mov video fileAbstract: Animals and humans autonomously learn to perceive and navigate the world. What "learning algorithm" does the cortex use to organize itself? Could computers and robots learn to perceive the way animals do, by just observing the world and moving around it? This constitutes a major challenge for machine learning and computer vision.
The visual cortex uses a multi-stage hierarchy of representations, from pixels, to edges, motifs, parts, objects, and scenes. A new branch of machine learning research, known as "deep learning" is producing new algorithms that can learn such multi-stage hierarchies of representations from raw inputs. I will describe a biologically-inspired, trainable vision architecture called convolutional network. It consists of multiple stages of filter banks, non-linear operations, and spatial pooling operations, analogous to the simple cells and complex cells in the mammalian visual cortex. Convolutional nets are first trained with unlabeled samples using a learning method based on sparse coding, and subsequently fine-tuned using labelled samples with a gradient-based supervised learning algorithm.
A number of applications will be shown through videos and live demos, including a category-level object recognition system that can be trained on the fly, a pedestrian detector, and system that recognizes human activities in videos, and a trainable vision system for off-road mobile robot navigation. A very fast implementation of these systems on specialized hardware will be shown. It is based on a new programmable and reconfigurable "dataflow" architecture dubbed NeuFlow.
2010-07-12: Five Lectures at the PCMI Summer SchoolFive lectures on machine learning and object recognition at the Park City Mathematics Institute Graduate Summer School, organized by the Institute of Advanced Studies.
Slides:Introduction [PDF]Energy-Based Learning [PDF]Multi-Stage Learning [PDF]Convolutional Networks [PDF]Unsupervised Deep Learning [PDF]2010-06-30: Keynote talk at ICISP, Trois Rivieres, QuebecLearning Hierarchies of Visual FeaturesKeynote talk given at theInternational Conference on Image and Signal Processingin Trois Rivieres, Quebec.
Slides:[Slides in PDF(24.4MB)] [Slides in DjVu(12.3MB)] [Slides in ODP (Open Office / Open Document Format)(25.1MB)]2010-05-05: IS&T Colloquium at NASA Goddard Space Flight CenterCan Robots Learn to See?NASA IS&T Colloquiumdelivered at theNASA Goddard Space Flight Centerin Maryland (with video).
NASA has a link tothe presentation and a video of the talk.
Slides:[Slides in PDF(29.3MB)] [Slides in DjVu(15.2MB)] [Slides in ODP(27.7MB)]Directdirect link to the video webcast.
2010-04-19: NSF Workshop on Hybrid Neuro-Computer Vision Systems at Columbia UniversityLearning Deep Hierarchies of Visual FeaturesTalk given at Columbia University for theNSF Workshop on Hybrid Neuro-Computer Vision Systems at Columbia University. The audience was a mixture of neuroscientists, computer vision researchers and hardware experts.
Slides:[Slides in PDF(19.7MB)] [Slides in DjVu(10.4MB)]Video from Columbia University siteswfobject.registerObject("player","9.0.98","expressInstall.swf");2010-01-14: Lectures at the CIfAR/Microsoft Winter School, Bangalore, IndiaSeries of lectures at theMicrosoft/CIfAR Winter School on Machine Learning and Computer Vision.
Slides:Part 1: [PDF][ODP]Part 2: [PDF][ODP]Part 3: [PDF][ODP]2009-07-20: Tutorial at the Vision-Learning Pattern Recognition Summer School, BeijingTwo lectures given at the2009 Sino-USA Vision-Learning Pattern Recognition Summer Schoolthat took place at Peking University, Beijing.
[PDF(21.5MB)] [DjVu(7.6MB)] [ODP(15.2MB)] Deep Learning[PDF(8.5MB)] [DjVu(4.2MB)] [ODP(15.0MB)] Other Methods and Applications of Deep Learning[PDF(16.5MB)] [DjVu(8.5MB)] [ODP(12.0MB)] Learning Invariant Feature Hierarchies[PDF(3.2MB)] [DjVu(0.9MB)] [ODP(41KB)] Future Challenges2009-04-10: Interview at NYASPodcast of an interview with Yann LeCun about research at CBLL in vision, learning, robotics and neuroscience. This is an 18 minute interview conducted by the New York Academy of Science:[MP3 from NYAS]2009-03-02: Distinguished Lecture at NEC Labs, Princeton, NJLearning Hierarchies of Invariant Visual Features(a slightly updated version of the UIUC talk).
Slides:[Slides in PDF(27.3MB)] [Slides in DjVu(11.3MB)] [Slides in ODP (Open Office / Open Document Format)(29.5MB)]2009-03-02: Distinguished Lecture at University of Illinois, Urbana ChampaignLearning Hierarchies of Invariant Visual FeaturesSlides:[Slides in PDF(26.2MB)] [Slides in DjVu(10.8MB)] [Slides in ODP (Open Office / Open Document Format)(29.4MB)]VideoIntelligent tasks, such as visual perception, auditory perception, and language understanding require the construction of good internal representations of the world. Internal representations (or "features") must be invariant (or robust) to irrelevant variations of the input, but must preserve the information relevant to the task. An important goal of our research is to devise methods that can automatically learn good internal representations from labeled and unlabeled data. Results from theoretical analysis, and experimental evidence from visual neuroscience, suggest that the visual world is best represented by a multi-stage hierarchy, in which features in successive stages are increasingly global, invariant, and abstract. The main question is how can one train such deep architectures from unlabeled data and limited amounts of labeled data.
Several methods have recently been proposed to train deep architectures in an unsupervised fashion. Each layer of the deep architecture is composed of a feed-forward encoder which computes a feature vector from the input, and a feed-back decoder which reconstructs the input from the features. The training shapes an energy landscape with low valleys around the training samples and high plateaus everywhere else. A number of such layers can be stacked and trained sequentially. A particular class of methods for deep energy-based unsupervised learning will be described that imposes sparsity constraints on the features. When applied to natural image patches, the method produces hierarchies of filters similar to those found in the mammalian visual cortex. A simple modification of the sparsity criterion produces locally-invariant features with similar characteristics as hand-designed features, such as SIFT.
An application to category-level object recognition with invariance to pose and illumination will be described. By stacking multiple stages of sparse features, and refining the whole system with supervised training, state-the-art accuracy can be achieved on standard datasets with very few labeled samples. Another application to vision-based navigation for off-road mobile robots will be shown. After a phase of off-line unsupervised learning, the system autonomously learns to discriminate obstacles from traversable areas at long range using labels produced with stereo vision for nearby areas.
This is joint work with Y-Lan Boureau, Karol Gregor, Raia Hadsell, Koray Kavakcuoglu, and Marc'Aurelio Ranzato.
2008-09-05: Machine Learning Summer SchoolFour lectures at the 2008 Machine Learning Summer School, Ile de RÃ©, France, September 5, 2009.
[PDF(16.2MB)] [DjVu(7.3MB)] [ODP(2.8MB)] Energy-Based Models[PDF(14.9MB)] [DjVu(5.9MB)] [ODP(4.4MB)] Supervised Learning[PDF(14.2MB)] [DjVu(6.2MB)] [ODP(4.6MB)] Manifold Learning[PDF(20.9MB)] [DjVu(9.1MB)] [ODP(19.7MB)]: Deep Learning2008-04-09: Talk at Google and Stanford on Deep LearningTalk onVisual Perception with Deep Learninggiven by Yann at Google on April 9, 2008. A similar talk was given at the Stanford AI Lab on April 10.
Slides:[Slides in PDF(22.7MB)] [Slides in DjVu(7.6MB)] [Slides in ODP (Open Office / Open Document Format)(19.4MB)]67 minute Video on YouTubeLink to the YouTube page for this videoAbstract:A long-term goal of Machine Learning research is to solve highy complex "intelligent" tasks, such as visual perception auditory perception, and language understanding. To reach that goal, the ML community must solve two problems: the Deep Learning Problem, and the Partition Function Problem.
There is considerable theoretical and empirical evidence that complex tasks, such as invariant object recognition in vision, require "deep" architectures, composed of multiple layers of trainable non-linear modules. The Deep Learning Problem is related to the difficulty of training such deep architectures.
Several methods have recently been proposed to train (or pre-train) deep architectures in an unsupervised fashion. Each layer of the deep architecture is composed of an encoder which computes a feature vector from the input, and a decoder which reconstructs the input from the features. A large number of such layers can be stacked and trained sequentially, thereby learning a deep hierarchy of features with increasing levels of abstraction. The training of each layer can be seen as shaping an energy landscape with low valleys around the training samples and high plateaus everywhere else. Forming these high plateaus constitute the so-called Partition Function problem.
A particular class of methods for deep energy-based unsupervised learning will be described that solves the Partition Function problem by imposing sparsity constraints on the features. The method can learn multiple levels of sparse and overcomplete representations of data. When applied to natural image patches, the method produces hierarchies of filters similar to those found in the mammalian visual cortex.
An application to category-level object recognition with invariance to pose and illumination will be described (with a live demo). Another application to vision-based navigation for off-road mobile robots will be described (with videos). The system autonomously learns to discriminate obstacles from traversable areas at long range.
This is joint work with Y-Lan Boureau, Sumit Chopra, Raia Hadsell, Fu-Jie Huang, Koray Kavakcuoglu, and Marc'Aurelio Ranzato.
2007-12-08: Interview with Yann LeCun at Video LecturesA 15 minute Interview with Yann LeCun on machine learning research, lecturing styles, where NIPS is going, the philosophy of science, and various other topics.
2007-12-07: Who is Afraid of Convex Optimization?Video and slides of a talk given at the2007 NIPS workshop on Efficient Learning, in Vancouver, Canada, December 7, 2007.
I'll probably make a few friends with that one.
[Slides in DjVu(3.6MB)] [Slides in PDF(8.6MB)]Click on the image at right to view the video of the talk (with lots of questions from the audience).
Who is Afraid of Non-Convex Loss Functions?2007-12-06: Learning a Deep Hierarchy of Sparse Invariant FeaturesSlides and Video of a talk given at theNIPS satellite session on deep learning. in Vancouver, Canada, December 6, 2007.
[Slides in DjVu(5.8MB)] [Slides in PDF(8.9MB)] [Slides in ODP(4.0MB)]Video of the talk:Part 1(85.0MB),Part 2(84.3MB)(Part 2 also contains Martin Szummer's talk)Other talks at that satellite session are available at themeeting's main web site.
2007-09-24: Energy-Based Models in Document Recognition and Computer VisionSlides of a keynote talk given at the2007 International Conference on Document Analysis and Recognition (ICDAR), in Curitiba, Brazil, September 24, 2007.
[Slides in DjVu(11.7MB)] [Slides in PDF(26.7MB)]PAPER:Yann LeCun, Sumit Chopra, Marc'Aurelio Ranzato and Fu-Jie Huang:Energy-Based Models in Document Recognition and Computer Vision,Proc. International Conference on Document Analysis and Recognition (ICDAR),2007,[key=lecun-icdar-keynote-07].
Abstract:Over the last few years, the Machine Learning and Natural Language Processing communities have devoted a considerable of work to learning models whose outputs are "structured", such as sequences of characters and words in a human language. The methods of choice include Conditional Random Fields, Hidden Markov SVMs, and Maximum Margin Markov Networks. These models can be seen as un-normalized versions of discriminative Hidden Markov Models. It may come to a surprise to the ICDAR community that this class of models was originally developed in the handwriting recognition community in the mid 90's to train handwritten recognition systems at word-level discriminatively. The various approaches can be described in a unified manner through to concept of "Energy-Based Model" (EBM). EBMs capture depencies between variables by associating a scalar energy to each configuration of the variables. Given a set of observed variables (e.g an image), an EBM inference consists in finding configurations of unobserved variables (e.g. a recognized word or sentence) that minimize the energy. Training an EBM consists in designing a loss function whose minimization will shape the energy surface so that correct variable configurations have lower energies than incorrect configurations. The main advantage of the EBM approach is to circumvent one of the main difficulties associated with training probabilistic models: keeping them properly normalized, a potentially intractable problem with complex models. Energy-Based learning has been applied with considerable success to such problems as handwriting recognition, natural language processing, biological sequence analysis, computer vision (object detection and recognition), image segmentation, image restoration, unsupervised feature learning, and dimensionality reduction. Several specific applications will be described (and, for some, illustrated with real-time demonstrations) including: a check reading system, a real-time system for simultaneously detecting human faces in images and estimating their pose; an unsupervised method for learning invariant feature hierarchies; and a real-time system for detecting and recognizing generic object categories in images, such as airplanes, cars, animal, and people.
2007-09-14: Tutorial at IPAM on Trainable MetricsSlides and audio podcast of a tutorial given at the2007 IPAM Workshop on the Mathematics of Knowledge and Search Engines, September 14, 2007.
1. Learning Similarity Metrics [DjVu(6.1MB)] [PDF(14.2MB)]2. Supervised and Unsupervised Methods for Learning Invariant Feature Hierarchies [DjVu(13.1MB)] [PDF(29.4MB)]Audio Podcast: [MP3 from NYU(58MB)][MP3 from IPAM(58MB)]2007-07-13: Supervised and Unsupervised Methods for Learning Invariant Feature HierarchiesSlides of a tutorial given at the2007 International Computer Vision Summer School, July 13, 2007.
[Slides in DjVu(12.9MB)] [Slides in PDF(30.0MB)] [Open Office .ODP(17.7MB)]2006-12-08: Learning Similarity Metrics with InvarianceSlides andvideoof an invited talk given at the 2006 NIPS workshop"Learning to Compare Samples", December 8, 2006.
[Slides in DjVu(2.7MB)] [Slides in PDF(13.7MB)]Video of the talkat VideoLectures.net.
2006-12-04: NIPS Tutorial: Energy-Based Models, Structured Learning Beyond LikelihoodsSlides and Video of a2-hour Tutorial at the NIPS conferencein Vancouver, Dec 04, 2006.
[Slides in DjVu(6.5MB)] [Slides in PDF(23.6MB)] [OpenOffice .ODP(4.9MB)] [PowerPoint .PPT(9.5MB)]Thevideo of the tutorial is availablefrom the NIPS website in .mov (QuickTime) format at several resolutions:320x240.
640x480.
900x600.
2006-09-07: Keynote talk at the IEEE Machine Learning for Signal Processing Workshop, Maynooth, IrelandSlides of a keynote speech at the IEEE Machine Learning for Signal Processing Workshop, Maynooth, Ireland.
[Slides in DjVu(8.4MB)] [Slides in PDF(25.5MB)]2006-08-16: Tutorial on Energy-Based Models, CIAR Summer School, TorontoSlides of a 3-hour tutorial given byYann LeCunat the2006 CIAR Summer School: Neural Computation & Adaptive Perception, at University of Toronto. This is new version of the tutorial based on the paperA Tutorial on Energy-Based Learning. This is considerably reworked from 2005 IPAM version.
A Tutorial on Energy-Based Learning: [Slides in DjVu(5.2MB)] [Slides in PDF(18.2MB)]Deep Learning for Generic Object Recognition: [Slides in DjVu(3.8MB)] [Slides in PDF(11.6MB)]2006-06-09: Keynote Speech at the Canadian Robotics and Vision Conference, QuebecSupervised and Unsupervised Learning with Energy-Based Models: [Slides in DjVu(6.9MB)] [Slides in PDF(20.6MB)]This is a good overview of research at CBLL: energy-based learning, object recognition, face detection, unsupervised feature learning, and robot vision and navigation.
2005-07-19: Tutorial on Energy-Based Models, Invariant Recognition, Trainable Metrics, and Graph Transformer Network, IPAM Summer School, UCLASlides andVideosof a 4-hour tutorial given byYann LeCunat the2005 IPAM Graduate Summer School: Intelligent Extraction of Information from Graphs and High Dimensional Data, at IPAM/UCLA.
Streaming videosof all the talks are available from the IPAM web site in RealVideo format.
The tutorial includes 4 talks:Energy-Based Models part 1, Introduction:[Streaming Video of the Talk] [Slides in DjVu(2.2MB)] [Slides in PDF(4.0MB)]Energy-Based Models part 2, Architectures and Loss functions:[Streaming Video of the Talk] [Slides in DjVu(3.7MB)] [Slides in PDF(5.3MB)]Architectures for Invariant Image Recognition, Convolutional Networks:[Streaming Video of the Talk] [Slides in DjVu(4.7MB)] [Slides in PDF(11.9MB)]Trainable Dissimilarity Metrics, Segmentation, Sequence Labeling, Graph Transformer Networks:[Streaming Video of the Talk] [Slides in DjVu(2.1MB)] [Slides in PDF(5.7MB)]2005-05-23: Tutorial on Energy-Based Models, Invariant Recognition, and Graph Transformer Network, TTI Summer School, ChicagoSlides andvideosof a 4-hour tutorial given byYann LeCunat theLearning Theory Summer Schoolorganized by theToyota Technological Institutein Chicago. The tutorial includes 3 talks. The 3 videos areavailable from VideoLectures.net.
Energy-Based Models: [DjVu(5.5MB)] [PDF(8.2MB)]Invariant Recognition, Convolutional Networks: [DjVu(5.2MB)] [PDF(13.2MB)]Graph Transformer Networks: [DjVu(1.1MB)] [PDF(2.1MB)]VideoLectures.net also has videos of two "lunch-time debates" (or panel discussions) in which Yann was a participant, together with Rob Schapire, David McAllester, Yasemin Altun, Mikhail Belkin, Yoram Singer, and John Langford.
Lunch-time debate on 2005-05-24Lunch-time debate on 2005-05-252005-03-23: Invariant Recognition of Generic Object Categories with Energy-Based Models, MSRI Workshop on Object RecognitionA 1-hour talk on invariant recognition of generic object categories, and on energy-based models given at the Workshop on on Object Recognition at the Mathematical Science Research Institute in Berkeley.
View aVideoof Yann's talk at MSRI:MSRI Server pageStreaming Quicktime Video from MSRIDownload QuickTime MPEG4 File from MSRI(508MB)Download QuickTime MPEG4 File from NYU(508MB)Slides: [DjVu(4.8MB)] [PDF(9.8MB)]2001-10-22: DjVu a compression technique and software platform for distributing scanned documents, digital documents, and high-resolution images on the Web, UIUC Distinguished LectureA 1-hour talk (with a video) of a Distinguished Lecture given on October 22, 2001 at the University of Illinois, Urbana-Champaign.
video in AVI/MPEG4 format (200MB)Slides in DjVu format (0.6MB)Slides in PDF format (23.6MB)
