Recurrent Neural Networks. Remembering what’s important | by Mahendran Venkatachalam | Towards Data Science
2019
https://towardsdatascience.com/recurrent-neural-networks-d4642c9bc7ce

Sign up Sign In Write Sign up Sign In Member-only story Recurrent Neural Networks Remembering what’s important Mahendran Venkatachalam · Follow Published in Towards Data Science · 8 min read · Mar 1, 2019 -- 2 Share Recurrent Neural Networks (RNNs) add an interesting twist to basic neural networks. A vanilla neural network takes in a fixed size vector as input which limits its usage in situations that involve a ‘series’ type input with no predetermined size.
RNNs are designed to take a series of input with no predetermined limit on size. One could ask what’s the big deal, I can call a regular NN repeatedly too? Sure can, but the ‘series’ part of the input means something. A single input item from the series is related to others and likely has an influence on its neighbors. Otherwise it's just “many” inputs, not a “series” input (duh!).
So we need something that captures this relationship across inputs meaningfully.
Recurrent Neural Networks Recurrent Neural Network remembers the past and it’s decisions are influenced by what it has learnt from the past. Note: Basic feed forward networks “remember” things too, but they remember things they learnt during training. For example, an image classifier learns what a “1” looks like during training and then uses that knowledge to classify things in production.
While RNNs learn similarly while training, in addition, they remember things learnt from prior input(s) while generating output(s). It’s part of the network. RNNs can take one or more input vectors and produce one or more output vectors and the output(s) are influenced not just by weights applied on inputs like a regular NN, but also by a “hidden” state vector representing the context based on prior input(s)/output(s). So, the same input could produce a different output depending on previous inputs in the series.
-- -- 2 Follow Written by Mahendran Venkatachalam 298 Followers · Writer for Towards Data Science https://gotensor.com/ Follow More from Mahendran Venkatachalam and Towards Data Science Mahendran Venkatachalam in Towards Data Science Attention in Neural Networks Some variations of attention architectures · 12 min read · Jul 7, 2019 -- 3 Adrian H. Raudaschl in Towards Data Science Forget RAG, the Future is RAG-Fusion The Next Frontier of Search: Retrieval Augmented Generation meets Reciprocal Rank Fusion and Generated Queries · 10 min read · Oct 6 -- 24 Damian Gil in Towards Data Science Mastering Customer Segmentation with LLM Unlock advanced customer segmentation techniques using LLMs, and improve your clustering models with advanced techniques 24 min read · Sep 26 -- 26 Mahendran Venkatachalam in Towards Data Science About Adversarial Examples Adversarial examples are an interesting topic in the world of deep neural networks. This post will try to address some basic questions on… · 7 min read · Jan 2, 2019 -- Recommended from Medium Issac kondreddy Chapter 3: Recurrent Neural Networks — Unlocking the Secrets of Sequential Data Introduction 3 min read · May 8 -- Jonte Dancker in Towards Data Science A Brief Introduction to Recurrent Neural Networks An introduction to RNN, LSTM, and GRU and their implementation 12 min read · Dec 26, 2022 -- 5 Lists Predictive Modeling w/ Python · Practical Guides to Machine Learning · Natural Language Processing · The New Chatbots: ChatGPT, Bard, and Beyond · Muhammad Ali Haider Recurrent Neural Networks (RNN) What are Recurrent Neural Networks (RNN)? 4 min read · Jun 14 -- Everton Gomede, PhD Next Word Prediction: Enhancing Language Understanding and Communication Introduction 4 min read · Aug 24 -- Saba Hesaraki Recurrent Neural Network (RNN) A Recurrent Neural Network (RNN) is a type of neural network architecture designed for processing sequences of data, where the order of… 3 min read · 4 days ago -- Priyanka Kumari LSTM & RNN Models for NLP 1. LSTM Architecture 4 min read · Jun 23 -- Help Status About Careers Blog Privacy Terms Text to speech Teams
