OpenAI Said Its Code Was Risky. Two Grads Re-Created It Anyway | WIRED
2019
https://www.wired.com/story/dangerous-ai-open-source

Open Navigation Menu To revist this article, visit My Profile, then View saved stories.
Close Alert Backchannel Business Culture Gear Ideas Science Security Merch To revist this article, visit My Profile, then View saved stories.
Close Alert Search Backchannel Business Culture Gear Ideas Science Security Merch Podcasts Video Artificial Intelligence Climate Games Newsletters Magazine Events Wired Insider Jobs Coupons Tom Simonite Business OpenAI Said Its Code Was Risky. Two Grads Re-Created It Anyway Play/Pause Button Pause Casey Chin; Getty Images Save this story Save Save this story Save In February, an artificial intelligence lab cofounded by Elon Musk informed the world that its latest breakthrough was too risky to release to the public. OpenAI claimed it had made language software so fluent at generating text that it might be adapted to crank out fake news or spam.
On Thursday, two recent master's graduates in computer science released what they say is a re-creation of OpenAI‚Äôs withheld software onto the internet for anyone to download and use.
Aaron Gokaslan, 23, and Vanya Cohen, 24, say they aren‚Äôt out to cause havoc and don‚Äôt believe such software poses much risk to society yet. The pair say their release was intended to show that you don‚Äôt have to be an elite lab rich in dollars and PhDs to create this kind of software: They used an estimated $50,000 worth of free cloud computing from Google, which hands out credits to academic institutions. And they argue that setting their creation free can help others explore and prepare for future advances‚Äîgood or bad.
‚ÄúThis allows everyone to have an important conversation about security, and researchers to help secure against future potential abuses,‚Äù says Cohen, who notes language software also has many positive uses. ‚ÄúI‚Äôve gotten scores of messages, and most of them have been like, ‚ÄòWay to go.‚Äô‚Äù The duo‚Äôs experiment, like OpenAI‚Äôs, involved giving machine learning software text from millions of webpages gathered by harvesting links shared on Reddit. After the software internalizes patterns of language from the text, it can then be adapted to tasks such as translation, powering chatbots, or generating new text in response to a prompt.
The text Gokaslan and Cohen‚Äôs software generates can be impressively fluid. When WIRED gave it the prompt ‚ÄúThe problem with America is‚Äù it added ‚Äúthat, because everything is a narrative, we're all imprisoned in our own set of lies.‚Äù A few sentences later it praised Donald Trump for being able to ‚Äúgive voice to those who had been left voiceless.‚Äù That text showed similarities to what WIRED saw when playing with the (ultimately withheld) model OpenAI developed earlier this year , called GPT-2. That one riffed about connections between Hilary Clinton and George Soros. Both versions of the software show the signs of training on content linked from Reddit, where political debates can be fiery.
But neither project can generate perfect prose: Machine learning software picks up the statistical patterns of language, not a true understanding of the world. Text from both the original and wannabe software often makes nonsensical leaps. Neither can be directed to include particular facts or points of view.
Gear Everything Apple Announced at Today‚Äôs Hardware Event Brenda Stolyar Business Sam Bankman-Fried Built a Crypto Paradise in the Bahamas‚ÄîNow He's a Bad Memory Joel Khalili Science Everyone Was Wrong About Why Cats Purr Jorge Garay Security They Cracked the Code to a Locked USB Drive Worth $235 Million in Bitcoin. Then It Got Weird Andy Greenberg Those shortcomings have caused some AI researchers to greet OpenAI‚Äôs claims of an imminent threat to society with derision.
 Humans can‚Äî and do ‚Äîwrite more potent misleading text.
Tuesday, OpenAI released a report saying it was aware of more than five other groups that had replicated its work at full scale, but that none had released the software. The report also said that a smaller version of GPT-2 OpenAI had released was roughly as good as the full withheld one at creating fake news articles. (You can try that smaller version online.
) Gokaslan and Cohen took the report‚Äôs data to mean that their own software wouldn‚Äôt be significantly more dangerous than what OpenAI had already released, if it was dangerous at all. They wanted to show the world that similar projects are now within reach of anyone with some programming skills and motivation. "If you gave a high school student guidance, they could probably do it,‚Äù Gokaslan says.
Miles Brundage, who works on policy at OpenAI, declines to say how dangerous the software the pair released might be. No one has had time to properly test it, he says, although figures released by Gokaslan and Cohen suggest it is slightly less powerful than the full GPT-2. Brundage adds OpenAI would like to eventually release that full version, but is waiting to feel ‚Äúcomfortable‚Äù there won‚Äôt be negative consequences.
Brundage acknowledges that Gokaslan and Cohen have shown how widening access to powerful computers and AI skills is increasing the number of people who can do such work. He still thinks anyone working on something similar should proceed with caution and talk through their release plans with OpenAI. ‚ÄúI encourage people to reach out to us,‚Äù he says.
Another AI safety lesson from the episode is to always read your email. Gokaslan and Cohen tried to inform OpenAI about their work by contacting the lead author on the lab‚Äôs technical paper about GPT-2. They say they never heard back, causing them to miss out on whatever OpenAI advises other researchers about the risks of software like its own.
A spokesperson for OpenAI said the researcher Gokaslan and Cohen tried to contact ‚Äúgets a lot of email,‚Äù and that the lab's policy team has been monitoring a dedicated email address for discussions about GPT-2 previously publicized in blog posts.
Gokaslan and Cohen did make contact with OpenAI Thursday, after a tweet announcing their release began circulating among AI researchers. They say they‚Äôre looking forward to discussing their work and its implications. They‚Äôre also working on a research paper describing their project‚Äîand they plan to write it themselves.
Everything you need to know about cyberwar The psychedelic, glow-in-the-dark art of Alex Aliume 3 years of misery inside Google , the happiest place in tech Why a promising cancer therapy isn't used in the US The best coolers for every kind of outdoor adventure üëÅ Facial recognition is suddenly everywhere.
 Should you worry? Plus, read the latest news on artificial intelligence üèÉüèΩ‚Äç‚ôÄÔ∏è Want the best tools to get healthy? Check out our Gear team‚Äôs picks for the best fitness trackers , running gear (including shoes and socks ), and best headphones.
Senior Editor X Topics artificial intelligence OpenAI Will Knight Will Knight Khari Johnson Will Knight Will Bedingfield Paresh Dave Paresh Dave Facebook X Pinterest YouTube Instagram Tiktok More From WIRED Subscribe Newsletters Mattresses Reviews FAQ Wired Staff Coupons Editorial Standards Archive Contact Advertise Contact Us Customer Care Jobs Press Center RSS Accessibility Help Cond√© Nast Store Do Not Sell My Personal Info ¬© 2023 Cond√© Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights.
WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Cond√© Nast.
Ad Choices Select international site United States LargeChevron UK Italia Jap√≥n
