Human Compatible - Wikipedia
2019
https://en.wikipedia.org/wiki/Human_Compatible

Main menu Main page Contents Current events Random article About Wikipedia Contact us Donate Help Learn to edit Community portal Recent changes Upload file Search Create account Log in Personal tools Create account Log in learn more Contributions Talk Contents (Top) 1 Summary Toggle Summary subsection 1.1 Russell's three principles 2 Reception 3 See also 4 References 5 External links Toggle the table of contents Human Compatible Add languages Add links Article Talk English Read Edit View history Tools Read Edit View history What links here Related changes Upload file Special pages Permanent link Page information Cite this page Get shortened URL Wikidata item Download as PDF Printable version Hardcover edition Author Stuart J. Russell Country United States Language English Subject AI control problem Genre Non-fiction Publisher Viking Publication date October 8, 2019 Pages 352 ISBN.
mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit} 978-0-525-55861-3 OCLC 1083694322 Human Compatible: Artificial Intelligence and the Problem of Control is a 2019 non-fiction book by computer scientist Stuart J. Russell.
 It asserts that the risk to humanity from advanced artificial intelligence (AI) is a serious concern despite the uncertainty surrounding future progress in AI. It also proposes an approach to the AI control problem.
Summary [ edit ] Russell begins by asserting that the standard model of AI research, in which the primary definition of success is getting better and better at achieving rigid human-specified goals, is dangerously misguided. Such goals may not reflect what human designers intend, such as by failing to take into account any human values not included in the goals. If an AI developed according to the standard model were to become superintelligent , it would likely not fully reflect human values and could be catastrophic to humanity. Russell asserts that precisely because the timeline for developing human-level or superintelligent AI is highly uncertain, safety research should be begun as soon as possible, as it is also highly uncertain how long it would take to complete such research.
Russell argues that continuing progress in AI capability is inevitable because of economic pressures. Such pressures can already be seen in the development of existing AI technologies such as self-driving cars and personal assistant software.
 Moreover, human-level AI could be worth many trillions of dollars. Russell then examines the current debate surrounding AI risk. He offers refutations to a number of common arguments dismissing AI risk and attributes much of their persistence to tribalism—AI researchers may see AI risk concerns as an "attack" on their field. Russell reiterates that there are legitimate reasons to take AI risk concerns seriously and that economic pressures make continued innovation in AI inevitable.
Russell then proposes an approach to developing provably beneficial machines that focus on deference to humans. Unlike in the standard model of AI, where the objective is rigid and certain, this approach would have the AI's true objective remain uncertain, with the AI only approaching certainty about it as it gains more information about humans and the world. This uncertainty would, ideally, prevent catastrophic misunderstandings of human preferences and encourage cooperation and communication with humans. Russell concludes by calling for tighter governance of AI research and development as well as cultural introspection about the appropriate amount of autonomy to retain in an AI-dominated world.
Russell's three principles [ edit ] Russell lists three principles to guide the development of beneficial machines. He emphasizes that these principles are not meant to be explicitly coded into the machines; rather, they are intended for human developers. The principles are as follows: [1] : 173 1. The machine's only objective is to maximize the realization of human preferences.
2. The machine is initially uncertain about what those preferences are.
3. The ultimate source of information about human preferences is human behavior.
The "preferences" Russell refers to "are all-encompassing; they cover everything you might care about, arbitrarily far into the future." [1] : 173 Similarly, "behavior" includes any choice between options, [1] : 177 and the uncertainty is such that some probability, which may be quite small, must be assigned to every logically possible human preference.
[1] : 201 Russell explores inverse reinforcement learning , in which a machine infers a reward function from observed behavior, as a possible basis for a mechanism for learning human preferences.
[1] : 191–193 Reception [ edit ] Several reviewers agreed with the book's arguments. Ian Sample in The Guardian called it "convincing" and "the most important book on AI this year".
[2] Richard Waters of the Financial Times praised the book's "bracing intellectual rigour".
[3] Kirkus Reviews endorsed it as "a strong case for planning for the day when machines can outsmart us".
[4] The same reviewers characterized the book as "wry and witty", [2] or "accessible" [4] due to its "laconic style and dry humour".
[3] Matthew Hutson of the Wall Street Journal said "Mr. Russell's exciting book goes deep while sparkling with dry witticisms".
[5] A Library Journal reviewer called it "The right guide at the right time".
[6] James McConnachie of The Times wrote "This is not quite the popular book that AI urgently needs. Its technical parts are too difficult, and its philosophical ones too easy. But it is fascinating and significant." [7] By contrast, Human Compatible was criticized in its Nature review by David Leslie, an Ethics Fellow at the Alan Turing Institute ; and similarly in a New York Times opinion essay by Melanie Mitchell.
 One point of contention was whether superintelligence is possible. Leslie states Russell "fails to convince that we will ever see the arrival of a 'second intelligent species'", [8] and Mitchell doubts a machine could ever "surpass the generality and flexibility of human intelligence" without losing "the speed, precision, and programmability of a computer".
[9] A second disagreement was whether intelligent machines would naturally tend to adopt so-called "common sense" moral values. In Russell's thought experiment about a geoengineering robot that "asphyxiates humanity to deacidify the oceans", Leslie "struggles to identify any intelligence". Similarly, Mitchell believes an intelligent robot would naturally tend to be "tempered by the common sense, values and social judgment without which general intelligence cannot exist".
[10] [11] The book was longlisted for the 2019 Financial Times /McKinsey Award.
[12] See also [ edit ] Artificial Intelligence: A Modern Approach Center for Human-Compatible Artificial Intelligence The Precipice: Existential Risk and the Future of Humanity Slaughterbots Superintelligence: Paths, Dangers, Strategies References [ edit ] ^ a b c d e Russell, Stuart (October 8, 2019).
Human Compatible: Artificial Intelligence and the Problem of Control.
 United States: Viking.
ISBN 978-0-525-55861-3.
OCLC 1083694322.
^ a b Sample, Ian (October 24, 2019).
"Human Compatible by Stuart Russell review – AI and our future".
The Guardian.
^ a b Waters, Richard (18 October 2019).
"Human Compatible — can we keep control over a superintelligence?".
www.ft.com.
 Retrieved 23 February 2020.
^ a b "HUMAN COMPATIBLE | Kirkus Reviews".
Kirkus Reviews.
 2019.
 Retrieved 23 February 2020.
^ Hutson, Matthew (November 19, 2019).
" 'Human Compatible' and 'Artificial Intelligence' Review: Learn Like a Machine".
The Wall Street Journal.
^ Hahn, Jim (2019).
"Human Compatible: Artificial Intelligence and the Problem of Control".
Library Journal.
 Retrieved 23 February 2020.
^ McConnachie, James (October 6, 2019).
"Human Compatible by Stuart Russell review — an AI expert's chilling warning".
The Times.
^ Leslie, David (2019-10-02).
"Raging robots, hapless humans: the AI dystopia".
Nature.
574 (7776): 32–33.
Bibcode : 2019Natur.574...32L.
doi : 10.1038/d41586-019-02939-0.
^ Mitchell, Melanie (2019-10-31).
"Opinion | We Shouldn't be Scared by 'Superintelligent A.I.' ".
The New York Times.
ISSN 0362-4331.
 Retrieved 2023-07-18.
^ Leslie, David (2 October 2019). "Raging robots, hapless humans: the AI dystopia".
Nature.
574 (7776): 32–33.
Bibcode : 2019Natur.574...32L.
doi : 10.1038/d41586-019-02939-0.
^ Mitchell, Melanie (October 31, 2019).
"We Shouldn't be Scared by 'Superintelligent A.I.' ".
The New York Times.
^ Hill, Andrew (11 August 2019).
"Business Book of the Year Award 2019 — the longlist".
www.ft.com.
 Retrieved 23 February 2020.
External links [ edit ] Interview with Stuart J. Russell.
mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:"[ "}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:" ]"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em} v t e Existential risk from artificial intelligence Concepts AGI AI alignment AI capability control AI safety AI takeover Consequentialism Ethics of artificial intelligence Existential risk from artificial general intelligence Friendly artificial intelligence Instrumental convergence Intelligence explosion Longtermism Machine ethics Suffering risks Superintelligence Technological singularity Organizations Alignment Research Center Center for AI Safety Center for Applied Rationality Center for Human-Compatible Artificial Intelligence Centre for the Study of Existential Risk EleutherAI Future of Humanity Institute Future of Life Institute Google DeepMind Humanity+ Institute for Ethics and Emerging Technologies Leverhulme Centre for the Future of Intelligence Machine Intelligence Research Institute OpenAI People Scott Alexander Sam Altman Yoshua Bengio Nick Bostrom Paul Christiano Eric Drexler Sam Harris Stephen Hawking Dan Hendrycks Geoffrey Hinton Bill Joy Shane Legg Elon Musk Steve Omohundro Huw Price Martin Rees Stuart J. Russell Jaan Tallinn Max Tegmark Frank Wilczek Roman Yampolskiy Eliezer Yudkowsky Other Statement on AI risk of extinction Human Compatible Open letter on artificial intelligence (2015) Our Final Invention The Precipice Superintelligence: Paths, Dangers, Strategies Do You Trust This Computer? Artificial Intelligence Act Category American non-fiction books 2019 non-fiction books English-language books Existential risk from artificial general intelligence Futurology books Technology books Viking Press books Non-fiction books about Artificial intelligence Articles with short description Short description matches Wikidata This page was last edited on 27 October 2023, at 07:37 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License 4.0 ; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy.
 Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.
, a non-profit organization.
Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Toggle limited content width
