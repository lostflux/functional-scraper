The Pentagon Inches Toward Letting AI Control Weapons | WIRED
2021
https://www.wired.com/story/pentagon-inches-toward-letting-ai-control-weapons

Open Navigation Menu To revist this article, visit My Profile, then View saved stories.
Close Alert Backchannel Business Culture Gear Ideas Science Security Merch To revist this article, visit My Profile, then View saved stories.
Close Alert Search Backchannel Business Culture Gear Ideas Science Security Merch Podcasts Video Artificial Intelligence Climate Games Newsletters Magazine Events Wired Insider Jobs Coupons Will Knight Business The Pentagon Inches Toward Letting AI Control Weapons Photograph: Getty Images Save this story Save Save this story Save Application Human-computer interaction Ethics End User Government Sector Defense Technology Machine learning Machine vision Robotics Last August, several dozen military drones and tanklike robots took to the skies and roads 40 miles south of Seattle. Their mission: Find terrorists suspected of hiding among several buildings.
So many robots were involved in the operation that no human operator could keep a close eye on all of them. So they were given instructions to find‚Äîand eliminate‚Äîenemy combatants when necessary.
The mission was just an exercise, organized by the Defense Advanced Research Projects Agency , a blue-sky research division of the Pentagon; the robots were armed with nothing more lethal than radio transmitters designed to simulate interactions with both friendly and enemy robots.
The drill was one of several conducted last summer to test how artificial intelligence could help expand the use of automation in military systems, including in scenarios that are too complex and fast-moving for humans to make every critical decision. The demonstrations also reflect a subtle shift in the Pentagon‚Äôs thinking about autonomous weapons, as it becomes clearer that machines can outperform humans at parsing complex situations or operating at high speed.
General John Murray of the US Army Futures Command told an audience at the US Military Academy last month that swarms of robots will force military planners, policymakers, and society to think about whether a person should make every decision about using lethal force in new autonomous systems. Murray asked: ‚ÄúIs it within a human's ability to pick out which ones have to be engaged‚Äù and then make 100 individual decisions? ‚ÄúIs it even necessary to have a human in the loop?‚Äù he added.
Other comments from military commanders suggest interest in giving autonomous weapons systems more agency. At a conference on AI in the Air Force last week, Michael Kanaan, director of operations for the Air Force Artificial Intelligence Accelerator at MIT and a leading voice on AI within the US military, said thinking is evolving. He says AI should perform more identifying and distinguishing potential targets while humans make high-level decisions. ‚ÄúI think that's where we're going,‚Äù Kanaan says.
‚ÄúIs it even necessary to have a human in the loop?‚Äù General John Murray, US Army Futures Command At the same event, Lieutenant General Clinton Hinote , deputy chief of staff for strategy, integration, and requirements at the Pentagon, says that whether a person can be removed from the loop of a lethal autonomous system is ‚Äúone of the most interesting debates that is coming, [and] has not been settled yet.‚Äù A report this month from the National Security Commission on Artificial Intelligence (NSCAI), an advisory group created by Congress, recommended, among other things, that the US resist calls for an international ban on the development of autonomous weapons.
Timothy Chung , the Darpa program manager in charge of the swarming project, says last summer‚Äôs exercises were designed to explore when a human drone operator should, and should not, make decisions for the autonomous systems. For example, when faced with attacks on several fronts, human control can sometimes get in the way of a mission, because people are unable to react quickly enough. ‚ÄúActually, the systems can do better from not having someone intervene,‚Äù Chung says.
Gear Everything Apple Announced at Today‚Äôs Hardware Event Brenda Stolyar Business Sam Bankman-Fried Built a Crypto Paradise in the Bahamas‚ÄîNow He's a Bad Memory Joel Khalili Science Everyone Was Wrong About Why Cats Purr Jorge Garay Security They Cracked the Code to a Locked USB Drive Worth $235 Million in Bitcoin. Then It Got Weird Andy Greenberg The drones and the wheeled robots, each about the size of a large backpack, were given an overall objective, then tapped AI algorithms to devise a plan to achieve it. Some of them surrounded buildings while others carried out surveillance sweeps. A few were destroyed by simulated explosives; some identified beacons representing enemy combatants and chose to attack.
The US and other nations have used autonomy in weapons systems for decades. Some missiles can, for instance, autonomously identify and attack enemies within a given area. But rapid advances in AI algorithms will change how the military uses such systems. Off-the-shelf AI code capable of controlling robots and identifying landmarks and targets, often with high reliability, will make it possible to deploy more systems in a wider range of situations.
But as the drone demonstrations highlight, more widespread use of AI will sometimes make it more difficult to keep a human in the loop. This might prove problematic, because AI technology can harbor biases or behave unpredictably.
 A vision algorithm trained to recognize a particular uniform might mistakenly target someone wearing similar clothing. Chung says the swarm project presumes that AI algorithms will improve to a point where they can identify enemies with enough reliability to be trusted.
Use of AI in weapons systems has become controversial in recent years. Google faced employee protest and public outcry in 2018 after supplying AI technology to the Air Force through a project known as Maven.
To some degree, the project is part of a long history of autonomy in weapons systems, with some missiles already capable of carrying out limited missions independent of human control. But it also shows how recent advances in AI will make autonomy more attractive and inevitable in certain situations. What's more, it highlights the trust that will be placed in technology that can still behave unpredictably.
Paul Scharre , an expert at the Center for New American Security and author of Army of None: Autonomous Weapons and the Future of War , says it is time to have a more sophisticated discussion about autonomous weapons technology. ‚ÄúThe discussion surrounding ‚Äòhumans in the loop‚Äô ought to be more sophisticated than simply a binary ‚Äòare they or aren't they?‚Äô‚Äù Scharre says. ‚ÄúIf a human makes a decision to engage a swarm of enemy drones, does the human need to individually select each target?‚Äù The Defense Department issued a policy on autonomous weapons in November 2012, stating that autonomous weapons systems need to have human oversight‚Äîbut this need not mean soldiers making every decision.
Gear Everything Apple Announced at Today‚Äôs Hardware Event Brenda Stolyar Business Sam Bankman-Fried Built a Crypto Paradise in the Bahamas‚ÄîNow He's a Bad Memory Joel Khalili Science Everyone Was Wrong About Why Cats Purr Jorge Garay Security They Cracked the Code to a Locked USB Drive Worth $235 Million in Bitcoin. Then It Got Weird Andy Greenberg Those who believe that militaries could use AI to cross a Rubicon when it comes to human responsibility for lethal force see things differently.
‚ÄúLethal autonomous weapons cheap enough that every terrorist can afford them are not in America's national security interest,‚Äù says Max Tegmark , a professor at MIT and cofounder of the Future of Life Institute , a nonprofit that opposes autonomous weapons.
Tegmark says AI weapons should be ‚Äústigmatized and banned like biological weapons.‚Äù The NSCAI report's opposition to a global ban is a strategic mistake, he says: ‚ÄúI think we'll one day regret it even more than we regret having armed the Taliban.‚Äù üì© The latest on tech, science, and more: Get our newsletters ! Here's how to survive a killer asteroid Even Calibri‚Äôs creator is glad Microsoft is moving on A wildlife photographer judges our New Pok√©mon Snap shots Opportunities‚Äîand obstacles‚Äîfor women in cybersecurity Will future electric vehicles be powered by deep-sea metals ? üëÅÔ∏è Explore AI like never before with our new database üéÆ WIRED Games: Get the latest tips, reviews, and more üéß Things not sounding right? Check out our favorite wireless headphones , soundbars , and Bluetooth speakers Senior Writer X Topics Pentagon drones artificial intelligence robots Defense darpa magazine-29.07-29.08 Peter Guest Khari Johnson Matt Laslo Paresh Dave Susan D'Agostino Paresh Dave Peter Guest Khari Johnson Facebook X Pinterest YouTube Instagram Tiktok More From WIRED Subscribe Newsletters Mattresses Reviews FAQ Wired Staff Coupons Editorial Standards Archive Contact Advertise Contact Us Customer Care Jobs Press Center RSS Accessibility Help Cond√© Nast Store Do Not Sell My Personal Info ¬© 2023 Cond√© Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights.
WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Cond√© Nast.
Ad Choices Select international site United States LargeChevron UK Italia Jap√≥n
