Microsoft's Massive New Language AI Is Triple the Size of Open…
2005
https://blubrry.com/singularityhub/81924527/microsofts-massive-new-language-ai-is-triple-the-size-of-openais-gpt-3

Podcast Hosting Hosting Plans Easy-to-use tools and plans for your podcast PowerPress Plugin No. 1 WordPress podcast publishing plugin Podcast Website Free, easy-to-use WordPress site Professional Podcast Hosting Tools for networks, businesses, & advanced creators Private Podcasting For designated ears only, available via app and desktop Sign Up for Podcast Hosting Start your free 30 day trial Podcast Statistics Podcast Statistics Analyze your audience: Geo, Apps & Devices, etc.
Audience Surveys Demographics directly from your listeners Sign Up Get free podcast statistics Podcast Growth Dynamic Ad Insertion Pre-mid-post roll ad campaign control Programmatic Advertising Earn money immediately for your show Pro-Production Pros to assist your podcast launch and regular publishing Podcast Directory Promote your podcast in the world’s largest directory Getting Started Getting Started Our getting started guides How-to-Podcast You’ve got lots to learn & we have lots of podcast knowledge to share Documentation Support for all of our tools, making podcasting easy from start to finish Connect Podcast Support Contact us 7 days a week, read documentation, or call us! Developers Resources to integrate with Blubrry Podcast Insider & Blog Subscribe to our show and learn from our weekly blog posts Community (FB page) Become part of the Blubrry online community About Blubrry We’ve been around since 2005 Socials Plans & Pricing Sign Up Manage Account Billing Sign out Podcaster Dashboard Podcast Hosting Podcast Statistics Podcast Growth Getting Started Connect Plans & Pricing Log In Sign Up Dashboard Hosting Plans Easy-to-use tools and plans for your podcast Professional Podcast Hosting Tools for networks, businesses, & advanced creators PowerPress Plugin No. 1 WordPress podcast publishing plugin Private Podcasting For designated ears only, available via app and desktop Podcast Website Free, easy-to-use WordPress site Sign Up for Podcast Hosting Start your free 30 day trial Podcast Statistics Analyze your audience: Geo, Apps & Devices, etc.
Audience Surveys Demographics directly from your listeners Sign Up Get free podcast statistics Dynamic Ad Insertion Pre-mid-post roll ad campaign control Podcast Directory Promote your podcast in the world’s largest directory Programmatic Advertising Earn money immediately for your show Pro-Production Pros to assist your podcast launch and regular publishing Getting Started Our getting started guides How-to-Podcast You’ve got lots to learn & we have lots of podcast knowledge to share Documentation Support for all of our tools, making podcasting easy from start to finish Podcast Support Contact us 7 days a week, read documentation, or call us! Developers Resources to integrate with Blubrry Podcast Insider & Blog Subscribe to our show and learn from our weekly blog posts Community (FB page) Become part of the Blubrry online community About Blubrry We’ve been around since 2005 Singularity Hub Daily Microsoft's Massive New Language AI Is Triple the Size of OpenAI’s GPT-3 Just under a year and a half ago OpenAI announced completion of GPT-3, its natural language processing algorithm that was, at the time, the largest and most complex model of its type. This week, Microsoft and Nvidia introduced a new model they’re calling “the world’s largest and most powerful generative language model.” The Megatron-Turing Natural Language Generation model (MT-NLG) is more than triple the size of GPT-3 at 530 billion parameters.
GPT-3’s 175 billion parameters was already a lot; its predecessor, GPT-2, had a mere 1.5 billion parameters, and Microsoft’s Turing Natural Language Generation model, released in February 2020, had 17 billion.
A parameter is an attribute a machine learning model defines based on its training data, and tuning more of them requires upping the amount of data the model is trained on. It’s essentially learning to predict how likely it is that a given word will be preceded or followed by another word, and how much that likelihood changes based on other words in the sentence.
As you can imagine, getting to 530 billion parameters required quite a lot of input data and just as much computing power. The algorithm was trained using an Nvidia supercomputer made up of 560 servers, each holding eight 80-gigabyte GPUs. That’s 4,480 GPUs total, and an estimated cost of over $85 million.
For training data, Megatron-Turing’s creators used The Pile, a dataset put together by open-source language model research group Eleuther AI. Comprised of everything from PubMed to Wikipedia to Github, the dataset totals 825GB, broken down into 22 smaller datasets. Microsoft and Nvidia curated the dataset, selecting subsets they found to be “of the highest relative quality.” They added data from Common Crawl, a non-profit that scans the open web every month and downloads content from billions of HTML pages then makes it available in a special format for large-scale data mining. GPT-3 was also trained using Common Crawl data.
Microsoft’s blog post on Megatron-Turing says the algorithm is skilled at tasks like completion prediction, reading comprehension, commonsense reasoning, natural language inferences, and word sense disambiguation. But stay tuned—there will likely be more skills added to that list once the model starts being widely utilized.
GPT-3 turned out to have capabilities beyond what its creators anticipated, like writing code, doing math, translating between languages, and autocompleting images (oh, and writing a short film with a twist ending). This led some to speculate that GPT-3 might be the gateway to artificial general intelligence. But the algorithm’s variety of talents, while unexpected, still fell within the language domain (including programming languages), so that’s a bit of a stretch.
However, given the tricks GPT-3 had up its sleeve based on its 175 billion parameters, it’s intriguing to wonder what the Megatron-Turing model may surprise us with at 530 billion. The algorithm likely won’t be commercially available for some time, so it’ll be a while before we find out.
The new model’s creators, though, are highly optimistic. “We look forward to how MT-NLG will shape tomorrow’s products and motivate the community to push the boundaries of natural language processing even further,” they wrote in the blog post. “The journey is long and far from complete, but we are excited by what is possible and what lies ahead.” Image Credit: Kranich17 from Pixabay Terms Privacy Contact Support Podcast Statistics PowerPress Plugin Podcast Manual Developers About Careers Partners Affiliate Program
