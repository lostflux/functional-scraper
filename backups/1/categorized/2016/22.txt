Deep neural reasoning | Nature
2016
https://www.nature.com/articles/nature19477

Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript.
Advertisement View all journals Search Log in Explore content About the journal Publish with us Sign up for alerts RSS feed nature news & views article Download PDF Published: 12 October 2016 Artificial intelligence Deep neural reasoning Herbert Jaeger 1 Nature volume 538 , pages 467–468 ( 2016 ) Cite this article 23k Accesses 27 Citations 38 Altmetric Metrics details Subjects Computer science Neuroscience The human brain can solve highly abstract reasoning problems using a neural network that is entirely physical. The underlying mechanisms are only partially understood, but an artificial network provides valuable insight.
See Article p.471 You have full access to this article via your institution.
Download PDF Download PDF A classic example of logical reasoning is the syllogism, “All men are mortal. Socrates is a man. Therefore, Socrates is mortal.” According to both ancient and modern views 1 , reasoning amounts to a rule-based mental manipulation of symbols — in this example, the words 'All', 'men', and so on. But human brains are made of neurons that operate by exchanging jittery electrical pulses, rather than word-like symbols. This difference encapsulates a notorious scientific and philosophical enigma, sometimes referred to as the neural–symbolic integration problem 2 , which remains unsolved. On page 471 , Graves et al.
3 use the machine-learning methods of 'deep learning' to impart some crucial symbolic-reasoning mechanisms to an artificial neural system. Their system can solve complex tasks by learning symbolic-reasoning rules from examples, an achievement that has potential implications for the neural–symbolic integration problem.
A key requirement for reasoning is a working memory. In digital computers, this role is served by the random-access memory (RAM). When a computer reasons — when it executes a program — information is bundled together in working memory in ever-changing combinations. Comparing human reasoning to the running of computer programs is not a far-fetched metaphor. In fact, a venerable historical alley leads from Aristotle's definition of syllogisms to the modern model of a programmable computer (the Turing machine). Alan Turing himself used 'mind' language in his groundbreaking work 4 : “The behaviour of the computer at any moment is determined by the symbols which he is observing and his 'state of mind' at that moment.” Although there are clear parallels between human reasoning and the running of computer programs, we lack an understanding of how either of them could be implemented in biological or artificial neural networks. Graves and colleagues take a substantial step forward in this quest by presenting a neuro-computational system that shows striking similarities to a digital computer.
The authors' system consists of several modules, all of which are entirely non-symbolic and operate by exchanging streams of purely analog activation patterns — just like those recorded from biological brains. There are two main modules: a 'memory' comprised of a large grid of memory cells, each of which can have a particular numerical value that is akin to a voltage; and a 'controller', which is an artificial neural network. The controller can access selected locations on the memory grid, read what it finds there, combine that with input data and write numerical values back to selected memory locations. The two modules interact in many respects like the RAM and central processing unit of a digital computer.
Graves and colleagues demonstrate the capabilities of their system by putting it through several tasks that require rational reasoning, such as planning a multi-stage journey using public transport. Such tasks are fairly easy to solve using the symbolic computer programs of artificial intelligence, but have so far been rather out of reach of artificial neural networks.
The authors' neural system cannot and need not be programmed — instead, it is trained.
A digital computer solves a given task by executing a program that has been written for that purpose. By contrast, the authors' neural system cannot and need not be programmed — instead, it is trained. During training, the system is presented with a large number of solved examples of the task at hand. With each new presentation, the system slightly adapts its internal neural wiring so that its response moves gradually closer to the given task's solution.
The analog, smoothly adaptable nature of the authors' neural system is the key to its ability to be trained. Mathematically speaking, the system is a 'differentiable function', which has led to the authors calling it a differentiable neural computer (DNC). A digital computer is not differentiable and could not be trained in any similar fashion.
A DNC is a mathematical object that boasts tens of thousands of adjustable parameters. Training such a monster raises a plethora of mathematical, numerical and run-time issues. Only in the past few years has machine-learning research overcome these obstacles, through a compendium of techniques that have become branded as deep learning 5.
 The authors' training of a DNC is a splendid demonstration of the power of deep learning.
Graves et al.
 steer clear of grand claims about their work's implications for the neural–symbolic integration problem and, with due caution, suggest possible mappings of DNC structures to those of biological brains. This is wise, because the debates fought out in this arena are fierce and without winners. Instead, the authors establish an undeniable technical anchor point that will help to ground the debates — they have shown that certain non-trivial, central aspects of symbolic reasoning can be learnt by artificial neural systems.
With regard to practical exploits, deep-learning methods have so far excelled in tasks that require limited or no working memory, such as image recognition 6 and sentence-wise language translation 7.
 Whether or not DNCs will bring about practical advances in big-data technologies remains to be seen. The authors' demonstrations are not particularly complex as demands on rational reasoning go, and could be solved by the algorithms of symbolic artificial intelligence of the 1970s. However, those programs were handcrafted by humans and do not learn from examples.
For the time being, the DNC by itself cannot compete with state-of-the-art methods in digital computing when it comes to logical data mining 8.
 But a flexible, extensible DNC-style working memory might allow deep learning to expand into big-data applications that have a rational reasoning component, such as generating video commentaries or semantic text analysis. A precursor to the DNC, the neural Turing machine 9 , certainly sent thrills through the deep-learning community.
Footnote 1 Notes See all news & views References Newell, A.
Cogn. Sci.
4 , 135–183 (1980).
Article Google Scholar Hammer, B. & Hitzler, P. (eds) Perspectives of Neural–Symbolic Integration http://doi.org/fsrb8m (Springer, 2007).
Book Google Scholar Graves, A. et al.
Nature 538 , 471–476 (2016).
Article ADS Google Scholar Turing, A. M.
J. Math.
58 , 345–363 (1936).
Article Google Scholar LeCun, Y., Bengio, Y. & Hinton, G.
Nature 521 , 436–444 (2015).
Article ADS CAS Google Scholar Szegedy, C. et al.
Proc. IEEE Conf. Computer Vision Pattern Recognition http://dx.doi.org/10.1109/CVPR.2015.7298594 (2015).
Bahdanau, D., Cho, K. & Bengio, Y.
Int. Conf. Learning Representations Preprint at http://arxiv.org/abs/1409.0473 (2014).
De Raedt, L. & Kimmig, A.
Machine Learn.
100 , 5–47 (2015).
Article MathSciNet Google Scholar Graves, A., Wayne, G. & Danihelka, I. Preprint at http://arxiv.org/abs/1410.5401 (2014).
Download references Author information Authors and Affiliations Herbert Jaeger is at Jacobs University Bremen, 28759 Bremen, Germany., Herbert Jaeger Authors Herbert Jaeger View author publications You can also search for this author in PubMed Google Scholar Corresponding author Correspondence to Herbert Jaeger.
Related audio Developing a neural network that can learn… and remember Related links Related links Related links in Nature Research Computer science: Nanoscale connections for brain-like circuits Artificial intelligence: Learning to see and act Rights and permissions Reprints and Permissions About this article Cite this article Jaeger, H. Deep neural reasoning.
Nature 538 , 467–468 (2016). https://doi.org/10.1038/nature19477 Download citation Published : 12 October 2016 Issue Date : 27 October 2016 DOI : https://doi.org/10.1038/nature19477 Share this article Anyone you share the following link with will be able to read this content: Sorry, a shareable link is not currently available for this article.
Provided by the Springer Nature SharedIt content-sharing initiative This article is cited by Deep Residual Networks’ Abstract Reasoning Performance on Raven’s Progressive Matrices Shuyu Wang SN Computer Science (2021) You have full access to this article via your institution.
Download PDF Download PDF Associated content Hybrid computing using a neural network with dynamic external memory Alex Graves Greg Wayne Demis Hassabis Nature Article Advertisement Explore content Research articles News Opinion Research Analysis Careers Books & Culture Podcasts Videos Current issue Browse issues Collections Subjects Follow us on Facebook Follow us on Twitter Sign up for alerts RSS feed About the journal Journal Staff About the Editors Journal Information Our publishing models Editorial Values Statement Journal Metrics Awards Contact Editorial policies History of Nature Send a news tip Publish with us For Authors For Referees Language editing services Submit manuscript Search Quick links Explore articles by subject Find a job Guide to authors Editorial policies Nature ( Nature ) ISSN 1476-4687 (online) ISSN 0028-0836 (print) nature.com sitemap About Nature Portfolio About us Press releases Press office Contact us Discover content Journals A-Z Articles by subject Nano Protocol Exchange Nature Index Publishing policies Nature portfolio policies Open access Author & Researcher services Reprints & permissions Research data Language editing Scientific editing Nature Masterclasses Live Expert Trainer-led workshops Research Solutions Libraries & institutions Librarian service & tools Librarian portal Open research Recommend to library Advertising & partnerships Advertising Partnerships & Services Media kits Branded content Career development Nature Careers Nature Conferences Nature events Regional websites Nature Africa Nature China Nature India Nature Italy Nature Japan Nature Korea Nature Middle East Privacy Policy Use of cookies Your privacy choices/Manage cookies Legal notice Accessibility statement Terms & Conditions Your US state privacy rights © 2023 Springer Nature Limited Close Sign up for the Nature Briefing newsletter — what matters in science, free to your inbox daily.
Close Get the most important science stories of the day, free in your inbox.
