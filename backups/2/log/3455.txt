Open Navigation Menu To revist this article, visit My Profile, then View saved stories.
Close Alert Backchannel Business Culture Gear Ideas Science Security Merch To revist this article, visit My Profile, then View saved stories.
Close Alert Search Backchannel Business Culture Gear Ideas Science Security Merch Podcasts Video Artificial Intelligence Climate Games Newsletters Magazine Events Wired Insider Jobs Coupons Cade Metz Business Google's Data Center Engineer Shares Secrets of 'Warehouse' Computing Picasa Save this story Save Save this story Save Luiz André Barroso doesn't see Google's data centers as data centers. He sees them as computers the size of warehouses.
Barroso is a distinguished engineer at Google and a former researcher at the once and future computer giants Compaq and Digital Equipment Corp. He helped pioneer multicore microprocessors -- chips that are actually many chips -- and together with Urs Hölzle , the man who oversaw the development of Google's worldwide network of top-secret computing facilities, he wrote the definitive book on modern data center design. It's called The Datacenter as a Computer , and it explains why today's massive internet applications don't run on an ordinary collection of servers. The entire data center, including its many servers, must be built to work as a whole.
"These new large data centers are quite different from traditional hosting facilities of earlier times," Barroso and Hölzle wrote. "Large portions of the hardware and software resources in these facilities must work in concert to efficiently deliver good levels of internet service performance, something that can only be achieved by a holistic approach to their design and deployment. In other words, we must treat the data center itself as one massive warehouse-scale computer." The trick is to split your massive application into tiny pieces and spread them evenly across the array of servers. Each server is just a piece of Barroso's "computer." If you do this right, you don't even need powerful servers. In fact, Barroso says, it's better to use modest machines with modest processors, spreading your application as thin as you can. Modest machines are cheaper and potentially more energy-efficient, and if you spread the load thin enough -- i.e. you use more servers -- you're better prepared when any one machine breaks down.
In the seven or eight years since Google first put this idea into action, it has inspired a revolution among the giants of the net, with Amazon, Microsoft, Yahoo!, and Facebook all moving in a similar direction. And now, free-thinking server manufacturers are taking the idea to extremes , building machines that seek to tackle large problems using hundreds of chips originally designed for cellphones and tablets. A company called Calxeda offers servers built with ARM chips not unlike the one in your iPhone. A second startup called SeaMicro is doing much the same with Intel's mobile chip, the Atom. And HP is exploring similar servers with a research effort dubbed Project Moonshot.
Culture Taylor Swift and Beyoncé Are Resurrecting the American Movie Theater Angela Watercutter Gear The Best Home Depot Black Friday Deals Matt Jancer Gear Apple’s Pledge to Support RCS Messaging Could Finally Kill SMS Boone Ashworth Business Sweden’s Tesla Blockade Is Spreading Morgan Meaker Luiz Barroso applauds these efforts. This is just the sort of thing he espouses in Datacenter as a computer.
 But he also warns that there are limits to how thin you can spread your application. In response to the hype surrounding these "cellphone servers," he urged Hölzle to pen a follow-up to their book -- a paper that would show why Google-like parallel computing may not fly, if taken too far.
The paper -- written by Hölzle and edited by Barroso -- points out that as you spread your application thinner and thinner, the spreading gets harder and harder. At a certain point, he says, it may not be worth it.
Rise of the Google Warehouse Luiz Barroso joined Google in 2001, when the company still leased space in ordinary data centers, much like the rest of the world. He started as a software engineer, but Hölzle -- Google's first vice president of engineering -- soon put him in charge of the effort to rebuild the company's infrastructure, including not only the software but the hardware. "I was the closest thing we had to a hardware person," he remembers.
Running the company's "platforms team," he helped Google build not only its own data centers, but its own servers and other hardware equipment. Over the years, reports have indicated that Google even builds its own network switches and routers.
 Barroso declines to provide specifics -- Google typically says very little about its data centers, seeing them as a competitive advantage as rivals -- but his point is that Google builds equipment that fits into its vision of the warehouse computer.
Google's servers aren't the most powerful on the earth. On the contrary, the whole idea is to make them less powerful. Modest machines save money. "One powerful machine ends up costing more than two not-so-powerful machines that have the same performance," Barroso says. And if you run your application across a wide array of low-cost servers, it doesn't mean as much when one goes down. "The easiest thing for a software engineer is to have just one big computer with one CPU that's so fast that you don't need other CPUs. But that computer will fail," he says. "Having a larger number of small computational units gives you an easier way of tackling the fault-tolerant issue." Culture Taylor Swift and Beyoncé Are Resurrecting the American Movie Theater Angela Watercutter Gear The Best Home Depot Black Friday Deals Matt Jancer Gear Apple’s Pledge to Support RCS Messaging Could Finally Kill SMS Boone Ashworth Business Sweden’s Tesla Blockade Is Spreading Morgan Meaker In building servers with hundreds of low-power chips, companies such as Calxeda and SeaMicro are merely extending this idea, splitting tasks up into even smaller pieces. SeaMicro CEO Andrew Feldman cites The Datacenter as a Computer as an influence. Meanwhile, academics such as Dave Andersen and Steve Swanson have shown that such systems can run large applications while consuming considerably less power.
 Dave Anderson calls his research system the Fast Array of Wimpy Nodes, and at least among the research community, the wimpy name has stuck. But the nodes are wimpy only on their own. If you put enough of them together, they're quite powerful.
Or at least they're powerful when paired with the right software.
Google Takes It Easy With Google's warehouse computer, the software is as much a part of the whole as the hardware. Indeed, that massive array of servers can't perform to its potential unless the software is built to use it.
The problem is that building software for a parallel system is more difficult than building it for a single all-powerful machine. And the difficulty only increases as you break your application into tinier and tinier pieces and spread it across wimpier and wimpier systems. "There's easy parallelism, but then there's harder parallelism," Barroso says. "There are some parts of a program that are trivial to chunk into pieces, that don't necessarily have to interact with each other... but eventually you've exhausted this, and you have to go down to other pieces of the code that are hard to parallelize." There comes a point, Barroso says, when it's just not worth it to keep going. You run into Amdahl’s law , which says that if you parallelize only part of a system, your performance will improve only so much. Hölzle's paper was called "Brawny cores still beat wimpy cores, most of the time," and Amdahl's law was at the heart of it.
"Amdahl's law is a way of mathematically expressing that unless things are perfectly parallelizable, there will always been a pretty harsh upper-bound on how much faster you can make a computer by just adding more parallel processing," Barroso tells us. "It's a very cruel law. You can't revoke it. If ten percent of your problem is not parallelizable, no matter what you do -- even if you add more computers or processors to a system -- you're never going to make it more than ten times faster." Culture Taylor Swift and Beyoncé Are Resurrecting the American Movie Theater Angela Watercutter Gear The Best Home Depot Black Friday Deals Matt Jancer Gear Apple’s Pledge to Support RCS Messaging Could Finally Kill SMS Boone Ashworth Business Sweden’s Tesla Blockade Is Spreading Morgan Meaker Dave Andersen, a professor of computer science at Carnegie Mellon University, acknowledges these limitations, calling Hölzle's paper "reasonably well balanced." And Barroso, who edited the paper, agrees. "The point is that efficiency is fantastic and wimpy cores are fine, but if you go down to the wimpiest range, your gains really have to be enormous if you want to consider all the aggravation -- and the hit to their productivity -- that your software engineers face." Barroso declines to discuss specific "wimpy node" systems. But like Andersen, he points out that there will be cases where a server based on, say, Intel Atom chips will perform quite well -- and prove more energy-efficient than systems using much faster chips. With the first paper he wrote after joining Google a decade ago, he says, he and a few other Google engineers made one of the first arguments for wimpy cores, though they didn't call them that. All these years later, he still very much believes in the idea -- until it reaches that limit.
The word is that Google is now rebuilding its infrastructure from scratch. So we ask Barroso if Google might try to push the limits of Amdahl's and tackle that "hard parallelization" he speaks of. "I would hope not," he says. "We really like the easy stuff." Senior Writer X Topics Amazon ARM big data Calxeda data Enterprise Facebook Google Intel SeaMicro Servers Paresh Dave Gregory Barber Paresh Dave Amanda Hoover Steven Levy Paresh Dave Aarian Marshall Caitlin Harrington Facebook X Pinterest YouTube Instagram Tiktok More From WIRED Subscribe Newsletters Mattresses Reviews FAQ Wired Staff Coupons Black Friday Editorial Standards Archive Contact Advertise Contact Us Customer Care Jobs Press Center RSS Accessibility Help Condé Nast Store Do Not Sell My Personal Info © 2023 Condé Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights.
WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast.
Ad Choices Select international site United States LargeChevron UK Italia Japón Czech Republic & Slovakia
