Open Navigation Menu To revist this article, visit My Profile, then View saved stories.
Close Alert Backchannel Business Culture Gear Ideas Science Security Merch To revist this article, visit My Profile, then View saved stories.
Close Alert Search Backchannel Business Culture Gear Ideas Science Security Merch Podcasts Video Artificial Intelligence Climate Games Newsletters Magazine Events Wired Insider Jobs Coupons Will Knight Business A New Chip Cluster Will Make Massive AI Models Possible The Cerebras chip is part of a Cambrian explosion in new chip designs specialized for AI.
Photograph: Cerebras Save this story Save Save this story Save Application Text analysis Text generation End User Big company Research Sector Manufacturing IT Semiconductors Research Technology Chips Machine learning Neural Network When it comes to the neural networks that power today‚Äôs artificial intelligence , sometimes the bigger they are, the smarter they are too. Recent leaps in machine understanding of language , for example, have hinged on building some of the most enormous AI models ever and stuffing them with huge gobs of text. A new cluster of computer chips could now help these networks grow to almost unimaginable size‚Äîand show whether going ever larger may unlock further AI advances, not only in language understanding , but perhaps also in areas like robotics and computer vision.
Cerebras Systems , a startup that has already built the world‚Äôs largest computer chip , has now developed technology that lets a cluster of those chips run AI models that are more than a hundred times bigger than the most gargantuan ones around today.
Cerebras says it can now run a neural network with 120 trillion connections, mathematical simulations of the interplay between biological neurons and synapses. The largest AI models in existence today have about a trillion connections, and they cost many millions of dollars to build and train. But Cerebras says its hardware will run calculations in about a 50th of the time of existing hardware. Its chip cluster, along with power and cooling requirements, presumably still won‚Äôt come cheap, but Cerberas at least claims its tech will be substantially more efficient.
Courtesy of Cerebras Culture Taylor Swift and Beyonc√© Are Resurrecting the American Movie Theater Angela Watercutter Business Sweden‚Äôs Tesla Blockade Is Spreading Morgan Meaker Security Running Signal Will Soon Cost $50 Million a Year Andy Greenberg Gear The PlayStation Portal Turns Your PS5 Into a Handheld, Sorta Eric Ravenscraft ‚ÄúWe built it with synthetic parameters,‚Äù says Andrew Feldman, founder and CEO of Cerebras, who will present details of the tech at a chip conference this week. ‚ÄúSo we know we can, but we haven't trained a model, because we're infrastructure builders, and, well, there is no model yet‚Äù of that size, he adds.
Today, most AI programs are trained using GPUs, a type of chip originally designed for generating computer graphics but also well suited for the parallel processing that neural networks require. Large AI models are essentially divided up across dozens or hundreds of GPUs, connected using high-speed wiring.
GPUs still make sense for AI, but as models get larger and companies look for an edge, more specialized designs may find their niches. Recent advances and commercial interest have sparked a Cambrian explosion in new chip designs specialized for AI. The Cerebras chip is an intriguing part of that evolution. While normal semiconductor designers split a wafer into pieces to make individual chips, Cerebras packs in much more computational power by using the entire thing, having its many computational units, or cores, talk to each other more efficiently. A GPU typically has a few hundred cores, but Cerebras‚Äôs latest chip, called the Wafer Scale Engine Two (WSE-2), has 850,000 of them.
The design can run a big neural network more efficiently than banks of GPUs wired together. But manufacturing and running the chip is a challenge, requiring new methods for etching silicon features, a design that includes redundancies to account for manufacturing flaws, and a novel water system to keep the giant chip chilled.
To build a cluster of WSE-2 chips capable of running AI models of record size, Cerebras had to solve another engineering challenge: how to get data in and out of the chip efficiently. Regular chips have their own memory on board, but Cerebras developed an off-chip memory box called MemoryX. The company also created software that allows a neural network to be partially stored in that off-chip memory, with only the computations shuttled over to the silicon chip. And it built a hardware and software system called SwarmX that wires everything together.
Photograph: Cerebras Culture Taylor Swift and Beyonc√© Are Resurrecting the American Movie Theater Angela Watercutter Business Sweden‚Äôs Tesla Blockade Is Spreading Morgan Meaker Security Running Signal Will Soon Cost $50 Million a Year Andy Greenberg Gear The PlayStation Portal Turns Your PS5 Into a Handheld, Sorta Eric Ravenscraft ‚ÄúThey can improve the scalability of training to huge dimensions, beyond what anybody is doing today,‚Äù says Mike Demler , a senior analyst with the Linley Group and a senior editor of The Microprocessor Report.
Demler says it isn‚Äôt yet clear how much of a market there will be for the cluster, especially since some potential customers are already designing their own, more specialized chips in-house. He adds that the real performance of the chip, in terms of speed, efficiency, and cost, are as yet unclear. Cerebras hasn‚Äôt published any benchmark results so far.
‚ÄúThere‚Äôs a lot of impressive engineering in the new MemoryX and SwarmX technology,‚Äù Demler says. ‚ÄúBut just like the processor, this is highly specialized stuff; it only makes sense for training the very largest models.‚Äù Cerebras‚Äô chips have so far been adopted by labs that need supercomputing power. Early customers include Argonne National Labs, Lawrence Livermore National Lab, pharma companies including GlaxoSmithKline and AstraZeneca, and what Feldman describes as ‚Äúmilitary intelligence‚Äù organizations.
This shows that the Cerebras chip can be used for more than just powering neural networks; the computations these labs run involve similarly massive parallel mathematical operations. ‚ÄúAnd they‚Äôre always thirsty for more compute power,‚Äù says Demler, who adds that the chip could conceivably become important for the future of supercomputing.
David Kanter, an analyst with Real World Technologies and executive director of MLCommons , an organization that measures the performance of different AI algorithms and hardware, says he sees a future market for much bigger AI models. ‚ÄúI generally tend to believe in data-centric ML [machine learning], so we want larger data sets that enable building larger models with more parameters,‚Äù Kanter says.
According to Feldman, Cerebras plans to expand by targeting a nascent market for massive natural-language-processing AI algorithms. He says the company has talked to engineers at OpenAI , a firm in San Francisco that has pioneered the use of massive neural networks for language learning as well as robotics and game-playing.
The latest of OpenAI‚Äôs algorithms, called GPT-3, can handle language in surprisingly cogent ways, ginning up news articles on a given topic or summarizing content coherently, or even writing computer code , although it is also prone to fits of misunderstanding, misinformation, and occasional misogyny.
 The neural network behind GPT-3 has around 160 billion parameters.
‚ÄúFrom talking to OpenAI, GPT-4 will be about 100 trillion parameters,‚Äù Feldman says. ‚ÄúThat won‚Äôt be ready for several years.‚Äù Culture Taylor Swift and Beyonc√© Are Resurrecting the American Movie Theater Angela Watercutter Business Sweden‚Äôs Tesla Blockade Is Spreading Morgan Meaker Security Running Signal Will Soon Cost $50 Million a Year Andy Greenberg Gear The PlayStation Portal Turns Your PS5 Into a Handheld, Sorta Eric Ravenscraft OpenAI has made GPT-3 accessible to developers and startups via an API, but the company faces increasing competition from startups developing similar language tools. One of the founders of OpenAI, Sam Altman , is an investor in Cerebras. ‚ÄúI certainly think we can make much more progress on current hardware,‚Äù Altman says. ‚ÄúBut it would be great if Cerebras‚Äô hardware were even more capable.‚Äù Building a model the size of GPT-3 produced some surprising results. Asked whether a version of GPT that‚Äôs 100 times larger would necessarily be smarter‚Äî perhaps demonstrating fewer errors or a greater understanding of common sense‚ÄîAltman says it‚Äôs hard to be sure, but he‚Äôs ‚Äúoptimistic.‚Äù Such advances may be at least a few years away. Nearer term, Cerebras is hoping that enough companies will see a need for hardware designed to supersize all sorts of AI models.
üì© The latest on tech, science, and more: Get our newsletters ! A people's history of Black Twitter The push for ad agencies to ditch big oil clients Virtual reality lets you travel anywhere‚Äînew or old I think an AI is flirting with me.
 Is it OK if I flirt back? Why the first Mars drilling attempt came up empty üëÅÔ∏è Explore AI like never before with our new database üéÆ WIRED Games: Get the latest tips, reviews, and more üíª Upgrade your work game with our Gear team‚Äôs favorite laptops , keyboards , typing alternatives , and noise-canceling headphones Senior Writer X Topics artificial intelligence chips Semiconductors Will Knight Susan D'Agostino Will Knight Will Knight Niamh Rowe Christopher Beam Steven Levy Caitlin Harrington Facebook X Pinterest YouTube Instagram Tiktok More From WIRED Subscribe Newsletters Mattresses Reviews FAQ Wired Staff Coupons Black Friday Editorial Standards Archive Contact Advertise Contact Us Customer Care Jobs Press Center RSS Accessibility Help Cond√© Nast Store Do Not Sell My Personal Info ¬© 2023 Cond√© Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights.
WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Cond√© Nast.
Ad Choices Select international site United States LargeChevron UK Italia Jap√≥n Czech Republic & Slovakia
