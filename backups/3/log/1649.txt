Artificial Intelligence View All AI, ML and Deep Learning Auto ML Data Labelling Synthetic Data Conversational AI NLP Text-to-Speech Security View All Data Security and Privacy Network Security and Privacy Software Security Computer Hardware Security Cloud and Data Storage Security Data Infrastructure View All Data Science Data Management Data Storage and Cloud Big Data and Analytics Data Networks Automation View All Industrial Automation Business Process Automation Development Automation Robotic Process Automation Test Automation Enterprise Analytics View All Business Intelligence Disaster Recovery Business Continuity Statistical Analysis Predictive Analysis More Data Decision Makers Virtual Communication Team Collaboration UCaaS Virtual Reality Collaboration Virtual Employee Experience Programming & Development Product Development Application Development Test Management Development Languages Facebook AI’s RoBERTa improves Google’s BERT pretraining methods Share on Facebook Share on X Share on LinkedIn Are you ready to bring more awareness to your brand? Consider becoming a sponsor for The AI Impact Tour. Learn more about the opportunities here.
Facebook AI and University of Washington researchers devised ways to enhance Google’s BERT language model and achieve performance on par or exceeding state-of-the-art results in GLUE, SQuAD, and RACE benchmark data sets. Researchers detailed how RoBERTa works in a paper published last week on arXiv.
Named RoBERTa for “Robustly Optimized BERT approach,” the model adopts many of the techniques used by Bidirectional Encoder Representations from Transformers (BERT), a novel natural language model open-sourced by Google last fall.
Part of what’s different about RoBERTa is that it relies on pretraining with larger batches of data and changes to the masking pattern of training data. While in pretraining, the original BERT uses masked language modeling and next-sentence prediction, but RoBERTa drops the next-sentence prediction approach.
Overall, RoBERTa achieves state-of-the-art results in 4 of 9 GLUE benchmark tasks and boasts an overall GLUE task performance on par with XLNet.
VB Event The AI Impact Tour Connect with the enterprise AI community at VentureBeat’s AI Impact Tour coming to a city near you! “We find that BERT was significantly undertrained and can match or exceed the performance of every model published after it,” the report reads. “Our training improvements show that masked language model pretraining, under the right design choices, is competitive with all other recently published methods.” To make RoBERTa, researchers used 1,024 Nvidia V100 GPUs for roughly one day.
The original BERT is trained with the 16GB BookCorpus data set and English Wikipedia, but RoBERTa utilizes CommonCrawl (CC)-News, a 76GB data set with 63 million English news articles obtained between September 2016 and February 2019.
“Finally, we pretrain RoBERTa for significantly longer, increasing the number of pretraining steps from 100K to 300K, and then further to 500K. We again observe significant gains in downstream task performance, and the 300K and 500K step models outperform XLNet across most tasks,” the report reads.
The introduction of RoBERTa continues what’s been an active year for the massive language understanding AI systems OpenAI’s GPT-2 , Google Brain’s XLNet , and Microsoft’s MT-DNN , each of which surpassed BERT in benchmark performance results.
The cost of training such models can be extremely expensive and carry a sizable carbon footprint.
Earlier this month at Transform 2019, Facebook AI VP Jérôme Pesenti said that compute demands for cutting-edge or robust systems are a challenge even for companies like Google and Facebook.
VentureBeat's mission is to be a digital town square for technical decision-makers to gain knowledge about transformative enterprise technology and transact.
Discover our Briefings.
The AI Impact Tour Join us for an evening full of networking and insights at VentureBeat's AI Impact Tour, coming to San Francisco, New York, and Los Angeles! VentureBeat Homepage Follow us on Facebook Follow us on X Follow us on LinkedIn Follow us on RSS Press Releases Contact Us Advertise Share a News Tip Contribute to DataDecisionMakers Careers Privacy Policy Terms of Service Do Not Sell My Personal Information © 2023 VentureBeat.
 All rights reserved.
