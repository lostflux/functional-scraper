Open Navigation Menu To revist this article, visit My Profile, then View saved stories.
Close Alert Backchannel Business Culture Gear Ideas Science Security Merch To revist this article, visit My Profile, then View saved stories.
Close Alert Search Backchannel Business Culture Gear Ideas Science Security Merch Podcasts Video Artificial Intelligence Climate Games Newsletters Magazine Events Wired Insider Jobs Coupons Vittoria Elliott Business Big Tech Ditched Trust and Safety. Now Startups Are Selling It Back As a Service PHOTO-ILLUSTRATION: ANJALI NAIR; GETTY IMAGES Save this story Save Save this story Save Massive layoffs across the tech sector have hit trust and safety teams hard over the past year. But with wars raging in Ukraine and the Middle East and more than 50 elections taking place in the next 12 months, experts worry that a nascent industry of startups created to keep people safe online wonâ€™t be able to cope.
The cuts made headlines a year ago, when X (then Twitter) fired 3,700 people â€”including hundreds in trust and safety roles. Since then, Meta, Alphabet, and Amazon have made similar cuts. The layoffs at X inspired other platforms to do the same, argues Sabhanaz Rashid Diya, founding director at tech policy think tank the Tech Global Institute and a former member of Metaâ€™s policy team. â€œIn many ways, Twitter got away with it,â€ she says. â€œThatâ€™s given the other companies the confidence to say, â€˜You know what? Itâ€™s OK. You can survive and not face a terrible consequence.â€™â€ Still, the cost of these cuts is arguably already evident in the way major platforms have scrambled to respond to the war between Israel and Hamas. And the shift away from in-house trust and safety teams has created an opening for consultancies and startups to offer something new: trust and safety as a service.
These companies, many of them founded and staffed by people with Big Tech pedigrees, let platforms â€œbuy rather than buildâ€ trust and safety services, says Talha Baig, a former Meta engineer whose startup, Sero AI, recently received backing from accelerator Y Combinator. â€œThere is a lot more labor out on the marketplace, and thereâ€™s also a lot more customers willing to buy that labor.â€ But experts warn that outsourcing trust and safety also means outsourcing responsibilities to teams with no power to change the way platforms actually work.
Sahar Massachi, a former member of Metaâ€™s civic integrity team and cofounder and executive director of the Integrity Institute think tank, worries that by outsourcing key functions, platforms may be undermining their ability to improve products. Trust and safety issues can sometimes be more about product design than active moderationâ€”should a user be able to reshare content? How much weight should different metrics be given within a recommendation algorithm? â€œThe vendors could be great, but they wonâ€™t be able to have insight into that because of the ways that companies work,â€ Massachi says.
Culture Taylor Swift and BeyoncÃ© Are Resurrecting the American Movie Theater Angela Watercutter Gear The Best Home Depot Black Friday Deals Matt Jancer Gear Appleâ€™s Pledge to Support RCS Messaging Could Finally Kill SMS Boone Ashworth Business Swedenâ€™s Tesla Blockade Is Spreading Morgan Meaker The same is true of the AI systems that companies use to help flag potentially dangerous or abusive content. Platforms often use huge troves of data to build internal tools that help them streamline that process, says Louis-Victor de Franssu, cofounder of trust and safety platform Tremau. But many of these companies have to rely on commercially available models to build their systemsâ€”which could introduce new problems.
â€œThere are companies that say they sell AI, but in reality what they do is they bundle together different models,â€ says Franssu. This means a company might be combining a bunch of different machine learning modelsâ€”say, one that detects the age of a user and another that detects nudity to flag potential child sexual abuse materialâ€”into a service they offer clients.
And while this can make services cheaper, it also means that any issue in a model an outsourcer uses will be replicated across its clients, says Gabe Nicholas, a research fellow at the Center for Democracy and Technology. â€œFrom a free speech perspective, that means if thereâ€™s an error on one platform, you canâ€™t bring your speech somewhere elseâ€“if thereâ€™s an error, that error will proliferate everywhere.â€ This problem can be compounded if several outsourcers are using the same foundational models.
By outsourcing critical functions to third parties, platforms could also make it harder for people to understand where moderation decisions are being made, or for civil societyâ€”the think tanks and nonprofits that closely watch major platformsâ€”to know where to place accountability for failures.
â€œ[Many watching] talk as if these big platforms are the ones making the decisions. Thatâ€™s where so many people in academia, civil society, and the government point their criticism to,â€ says Nicholas,. â€œThe idea that we may be pointing this to the wrong place is a scary thought.â€ Historically, large firms like Telus International, Teleperformance, and Accenture would be contracted to manage a key part of outsourced trust and safety work: content moderation. This often looked like call centers , with large numbers of low-paid staffers manually parsing through posts to decide whether they violate a platformâ€™s policies against things like hate speech, spam, and nudity. New trust and safety startups are leaning more toward automation and artificial intelligence, often specializing in certain types of content or topic areasâ€”like terrorism or child sexual abuseâ€”or focusing on a particular medium, like text versus video. Others are building tools that allow a client to run various trust and safety processes through a single interface.
Culture Taylor Swift and BeyoncÃ© Are Resurrecting the American Movie Theater Angela Watercutter Gear The Best Home Depot Black Friday Deals Matt Jancer Gear Appleâ€™s Pledge to Support RCS Messaging Could Finally Kill SMS Boone Ashworth Business Swedenâ€™s Tesla Blockade Is Spreading Morgan Meaker Big Tech companies have tended to see trust and safety as a cost center, says Baigâ€”something they have to do to keep regulators and civil society groups at bay, but without much monetary value. But that soon may change. The European Unionâ€™s Digital Services Act and the UKâ€™s Online Safety Act , for instance, have created new obligations for tech companies big and small to monitor what happens on their platforms, and these pieces of legislation allow governments to levy huge fines.
â€œCompanies donâ€™t change the way in which they moderate content on their platform to gain 5, 10, 30 percent efficiency,â€ says Tremauâ€™s Franssu. â€œWhat will motivate them is if theyâ€™re scared of getting fined, especially fines as big as 6 percent of global annual revenue, or criminal liability, as we may see in the UK.â€ New regulations in the UK and Europe will also come to bear on smaller platforms, particularly around the kinds of content and services children can access. Startups may prefer to buy trust and safety as a service, rather than building their own teams and systems, says Sara Ittelson, a partner at the venture fund Accel, which has invested in the trust and safety tool Cinder. â€œIt used to be that companies thought that trust and safety issues were only surfacing for platforms of a particular size,â€ she says. â€œBut in reality, youâ€™re going to get them pretty early on.â€ And people, she argues, ultimately donâ€™t want to use platforms they donâ€™t feel safe on or that are full of junk content.
The explosion of interest in generative AI has only increased the pressure on companies to address trust and safety issues earlier in their life cycles. Generative AI tools can now be used to manufacture and share child sexual abuse material and nonconsensual pornography , both of which would violate most platformsâ€™ guidelines. â€œThereâ€™s much greater awareness as to how these tools can be exploited,â€ Ittelson says. This has raised questions for companies about how they are going to make sure their platforms arenâ€™t overrun with generative AI content, or how theyâ€™re going to ensure their tools arenâ€™t abused.
â€œGen AI is making it 10 times worse,â€ says Dror Nahumi, a partner at Norwest Venture Partners, which has invested in trust and safety startup ActiveFence. â€œIf Iâ€™m a bad actor and I was creating an article a week, now I could create 10 different posts in the same week without making any extra effort.â€ Culture Taylor Swift and BeyoncÃ© Are Resurrecting the American Movie Theater Angela Watercutter Gear The Best Home Depot Black Friday Deals Matt Jancer Gear Appleâ€™s Pledge to Support RCS Messaging Could Finally Kill SMS Boone Ashworth Business Swedenâ€™s Tesla Blockade Is Spreading Morgan Meaker While investors that spoke to WIRED were hesitant to guess at the potential size of the trust and safety as a service industry, ActiveFence, which was founded in 2017 and is amongst the oldest players in field, raised $100 million in 2021 and was valued at about half a billion dollars in 2021. And its cofounder, Noam Schwartz, says that valuation has risen.
While itâ€™s still nascent, the industry is clearly growing. â€œThis is exactly the way the cybersecurity industry was 20 years ago,â€ says Schwartz. A 2020 report from the venture capital firm Paladin Capital found that the industry had already raised over $1 billion in funding, and a 2023 report from the UK Department for Science Innovation, and Technology estimated that â€œSafety Tech,â€ which includes everything from content moderation to scam detection, was on track to hit Â£1 billion ($1.22 billion) in revenue by the mid-2020â€™s.
Though Nahumi says the Big Tech layoffs may indicate that there is, momentarily, less appetite to spend on trust and safety in general, â€œin the long term, we see that as a good thing for the companies in the space because it means that [tech companies] will have to rely more and more on services from companies that specialize in the space, and not something built in-house.â€ You Might Also Like â€¦ ğŸ“© Get the long view on tech with Steven Levy's Plaintext newsletter Watch this guy work, and youâ€™ll finally understand the TikTok era How Telegram became a terrifying weapon in the Israel-Hamas War Inside Elon Muskâ€™s first election crisis â€”a day after he â€œfreedâ€ the bird The ultra-efficient farm of the future is in the sky The best pickleball paddles for beginners and pros ğŸŒ² Our Gear team has branched out with a new guide to the best sleeping pads and fresh picks for the best coolers and binoculars Platforms and power reporter Topics content moderation Social Media algorithms Peter Guest Peter Guest David Gilbert Khari Johnson Peter Guest Matt Burgess Amanda Hoover Khari Johnson Facebook X Pinterest YouTube Instagram Tiktok More From WIRED Subscribe Newsletters Mattresses Reviews FAQ Wired Staff Coupons Black Friday Editorial Standards Archive Contact Advertise Contact Us Customer Care Jobs Press Center RSS Accessibility Help CondÃ© Nast Store Do Not Sell My Personal Info Â© 2023 CondÃ© Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights.
WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of CondÃ© Nast.
Ad Choices Select international site United States LargeChevron UK Italia JapÃ³n Czech Republic & Slovakia
