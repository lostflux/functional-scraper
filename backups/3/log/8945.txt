Open Navigation Menu To revist this article, visit My Profile, then View saved stories.
Close Alert Backchannel Business Culture Gear Ideas Science Security Merch To revist this article, visit My Profile, then View saved stories.
Close Alert Search Backchannel Business Culture Gear Ideas Science Security Merch Podcasts Video Artificial Intelligence Climate Games Newsletters Magazine Events Wired Insider Jobs Coupons Will Knight Business Researchers Want Guardrails to Help Prevent Bias in AI Photograph: Daniel Grizelj/Getty Images Save this story Save Save this story Save Artificial intelligence has given us algorithms capable of recognizing faces , diagnosing disease , and of course, crushing computer games.
 But even the smartest algorithms can sometimes behave in unexpected and unwanted ways‚Äîfor example, picking up gender bias from the text or images they are fed.
A new framework for building AI programs suggests a way to prevent aberrant behavior in machine learning by specifying guardrails in the code from the outset. It aims to be particularly useful for nonexperts deploying AI, an increasingly common issue as the technology moves out of research labs and into the real world.
The approach is one of several proposed in recent years for curbing the worst tendencies of AI programs. Such safeguards could prove vital as AI is used in more critical situations, and as people become suspicious of AI systems that perpetuate bias or cause accidents.
Last week Apple was rocked by claims that the algorithm behind its credit card offers much lower credit limits to women than men of the same financial means. It was unable to prove that the algorithm had not inadvertently picked up some form of bias from training data. Just the idea that the Apple Card might be biased was enough to turn customers against it.
Similar backlashes could derail adoption of AI in areas like health care, education, and government. ‚ÄúPeople are looking at how AI systems are being deployed and they're seeing they are not always being fair or safe,‚Äù says Emma Brunskill , an assistant professor at Stanford and one of the researchers behind the new approach. ‚ÄúWe're worried right now that people may lose faith in some forms of AI, and therefore the potential benefits of AI might not be realized.‚Äù Examples abound of AI systems behaving badly. Last year, Amazon was forced to ditch a hiring algorithm that was found to be gender biased; Google was left red-faced after the autocomplete algorithm for its search bar was found to produce racial and sexual slurs. In September, a canonical image database was shown to generate all sorts of inappropriate labels for images of people.
Machine-learning experts often design their algorithms to guard against certain unintended consequences. But that‚Äôs not as easy for nonexperts who might use a machine-learning algorithm off the shelf. It‚Äôs further complicated by the fact that there are many ways to define ‚Äúfairness‚Äù mathematically or algorithmically.
The new approach proposes building an algorithm so that, when it is deployed, there are boundaries on the results it can produce. ‚ÄúWe need to make sure that it's easy to use a machine-learning algorithm responsibly, to avoid unsafe or unfair behavior,‚Äù says Philip Thomas , an assistant professor at the University of Massachusetts Amherst who also worked on the project.
Culture Taylor Swift and Beyonc√© Are Resurrecting the American Movie Theater Angela Watercutter Gear The Best Home Depot Black Friday Deals Matt Jancer Gear Apple‚Äôs Pledge to Support RCS Messaging Could Finally Kill SMS Boone Ashworth Business Sweden‚Äôs Tesla Blockade Is Spreading Morgan Meaker The researchers demonstrate the method on several machine-learning techniques and a couple of hypothetical problems in a paper published in the journal Science Thursday.
First, they show how it could be used in a simple algorithm that predicts college students' GPAs from entrance exam results‚Äîa common practice that can result in gender bias, because women tend to do better in school than their entrance exam scores would suggest. In the new algorithm, a user can limit how much the algorithm may overestimate and underestimate student GPAs for male and female students on average.
In another example, the team developed an algorithm for balancing the performance and safety of an automated insulin pump. Such pumps decide how much insulin to deliver at mealtimes, and machine learning can help determine the right dose for a patient. The algorithm they designed can be told by a doctor to only consider dosages within a particular range, and to have a low probability of suggesting dangerously low or high blood sugar levels.
‚ÄúWe need to make sure that it's easy to use a machine-learning algorithm responsibly, to avoid unsafe or unfair behavior.‚Äù Philip Thomas, University of Massachusetts The researchers call their algorithms ‚ÄúSeldonian‚Äù in reference to Hari Seldon , a character in Isaac Asimov stories that feature his famous ‚Äú three laws of robotics ,‚Äù which begin with the rule: ‚ÄúA robot may not injure a human being or, through inaction, allow a human being to come to harm.‚Äù The new approach is unlikely to solve the problem of algorithms misbehaving. Partly that‚Äôs because there‚Äôs no guarantee organizations deploying AI will adopt such approaches when they can come at the cost of optimal performance.
The work also highlights the fact that defining ‚Äúfairness‚Äù in a machine-learning algorithm is not a simple task. In the GPA example, for instance, the researchers provide five different ways to define gender fairness.
‚ÄúOne of the major challenges in making algorithms fair lies in deciding what fairness actually means,‚Äù says Chris Russell , a fellow at the Alan Turing Institute in the UK. ‚ÄúTrying to understand what fairness means, and when a particular approach is the right one to use is a major area of ongoing research." If even experts cannot agree on what is fair, Russell says it might be a mistake to put the burden on less proficient users. ‚ÄúAt the moment, there are more than 30 different definitions of fairness in the literature,‚Äù he notes. ‚ÄúThis makes it almost impossible for a nonexpert to know if they are doing the right thing.‚Äù Meet the immigrants who took on Amazon Alien hunters need the far side of the moon to stay quiet The future of banking is ‚Ä¶ you're broke How to shut up your gadgets at night so you can sleep The super-optimized dirt that helps keep racehorses safe üëÅ A safer way to protect your data ; plus, the latest news on AI üíª Upgrade your work game with our Gear team‚Äôs favorite laptops , keyboards , typing alternatives , and noise-canceling headphones Senior Writer X Topics artificial intelligence algorithms bias Khari Johnson Niamh Rowe Will Knight Will Knight Steven Levy Will Knight Khari Johnson Will Knight Facebook X Pinterest YouTube Instagram Tiktok More From WIRED Subscribe Newsletters Mattresses Reviews FAQ Wired Staff Coupons Black Friday Editorial Standards Archive Contact Advertise Contact Us Customer Care Jobs Press Center RSS Accessibility Help Cond√© Nast Store Do Not Sell My Personal Info ¬© 2023 Cond√© Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights.
WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Cond√© Nast.
Ad Choices Select international site United States LargeChevron UK Italia Jap√≥n Czech Republic & Slovakia
