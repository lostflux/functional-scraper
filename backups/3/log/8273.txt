Open Navigation Menu To revist this article, visit My Profile, then View saved stories.
Close Alert Backchannel Business Culture Gear Ideas Science Security Merch To revist this article, visit My Profile, then View saved stories.
Close Alert Search Backchannel Business Culture Gear Ideas Science Security Merch Podcasts Video Artificial Intelligence Climate Games Newsletters Magazine Events Wired Insider Jobs Coupons Will Knight Business Six Months Ago Elon Musk Called for a Pause on AI. Instead Development Sped Up Photograph: Win McNamee/Getty Images Save this story Save Save this story Save Six months ago this week, many prominent AI researchers, engineers, and entrepreneurs signed an open letter calling for a six-month pause on development of AI systems more capable than OpenAI‚Äôs latest GPT-4 language generator. It argued that AI is advancing so quickly and unpredictably that it could eliminate countless jobs, flood us with disinformation, and‚Äî as a wave of panicky headlines reported ‚Äîdestroy humanity. Whoops! As you may have noticed, the letter did not result in a pause in AI development, or even a slow down to a more measured pace. Companies have instead accelerated their efforts to build more advanced AI.
Elon Musk, one of the most prominent signatories, didn‚Äôt wait long to ignore his own call for a slowdown. In July he announced xAI , a new company he said would seek to go beyond existing AI and compete with OpenAI, Google, and Microsoft. And many Google employees who also signed the open letter have stuck with their company as it prepares to release an AI model called Gemini , which boasts broader capabilities than OpenAI‚Äôs GPT-4.
WIRED reached out to more than a dozen signatories of the letter to ask what effect they think it had and whether their alarm about AI has deepened or faded in the past six months. None who responded seemed to have expected AI research to really grind to a halt.
‚ÄúI never thought that companies were voluntarily going to pause,‚Äù says Max Tegmark, an astrophysicist at MIT who leads the Future of Life Institute, the organization behind the letter‚Äîan admission that some might argue makes the whole project look cynical. Tegmark says his main goal was not to pause AI but to legitimize conversation about the dangers of the technology, up to and including the fact that it might turn on humanity. The result ‚Äúexceeded my expectations,‚Äù he says.
The responses to my follow-up also show the huge diversity of concerns experts have about AI‚Äîand that many signers aren‚Äôt actually obsessed with existential risk.
Lars Kotthoff , an associate professor at the University of Wyoming, says he wouldn‚Äôt sign the same letter today because many who called for a pause are still working to advance AI. ‚ÄúI‚Äôm open to signing letters that go in a similar direction, but not exactly like this one,‚Äù Kotthoff says. He adds that what concerns him most today is the prospect of a ‚Äúsocietal backlash against AI developments, which might precipitate another AI winter‚Äù by quashing research funding and making people spurn AI products and tools.
Having signed the letter, what have I done for the last year or so? I‚Äôve been doing AI research.
Stephen Mander, Lancaster University Other signers told me they would gladly sign again, but their big worries seem to involve near-term problems, such as disinformation and job losses , rather than Terminator scenarios.
‚ÄúIn the age of the internet and Trump, I can more easily see how AI can lead to destruction of human civilization by distorting information and corrupting knowledge,‚Äù says Richard Kiehl , a professor working on microelectronics at Arizona State University.
Culture Taylor Swift and Beyonc√© Are Resurrecting the American Movie Theater Angela Watercutter Gear The Best Home Depot Black Friday Deals Matt Jancer Gear Apple‚Äôs Pledge to Support RCS Messaging Could Finally Kill SMS Boone Ashworth Business Sweden‚Äôs Tesla Blockade Is Spreading Morgan Meaker ‚ÄúAre we going to get Skynet that‚Äôs going to hack into all these military servers and launch nukes all over the planet? I really don‚Äôt think so,‚Äù says Stephen Mander , a PhD student working on AI at Lancaster University in the UK. He does see widespread job displacement looming, however, and calls it an ‚Äúexistential risk‚Äù to social stability. But he also worries that the letter may have spurred more people to experiment with AI and acknowledges that he didn‚Äôt act on the letter‚Äôs call to slow down. ‚ÄúHaving signed the letter, what have I done for the last year or so? I‚Äôve been doing AI research,‚Äù he says.
Despite the letter‚Äôs failure to trigger a widespread pause, it did help propel the idea that AI could snuff out humanity into a mainstream topic of discussion. It was followed by a public statement signed by the leaders of OpenAI and Google‚Äôs DeepMind AI division that compared the existential risk posed by AI to that of nuclear weapons and pandemics. Next month, the British government will host an international ‚ÄúAI safety‚Äù conference , where leaders from numerous countries will discuss possible harms AI could cause, including existential threats.
Perhaps AI doomers hijacked the narrative with the pause letter, but the unease around the recent, rapid progress in AI is real enough‚Äîand understandable. A few weeks before the letter was written, OpenAI had released GPT-4, a large language model that gave ChatGPT new power to answer questions and caught AI researchers by surprise.
 As the potential of GPT-4 and other language models has become more apparent, surveys suggest that the public is becoming more worried than excited about AI technology. The obvious ways these tools could be misused is spurring regulators around the world into action.
The letter‚Äôs demand for a six-month moratorium on AI development may have created the impression that its signatories expected bad things to happen soon. But for many of them, a key theme seems to be uncertainty‚Äîaround how capable AI actually is, how rapidly things may change, and how the technology is being developed.
‚ÄúM‚Äã‚Äãany AI skeptics want to hear a concrete doom scenario, but to me, the fact that it is difficult to imagine detailed, concrete scenarios is kind of the point‚Äîit shows how hard it is for even world-class AI experts to predict the future of AI and how it will impact a complex world‚Äù says Scott Niekum , a professor at the University of Massachusetts Amherst who works on AI risk and signed the letter. ‚ÄúAnd when you combine that prediction difficulty with lagging progress in safety, interpretability, and regulation, I think that should raise some alarms.‚Äù Uncertainty is hardly proof that humanity is in danger. But the fact that so many people working in AI still seem unsettled may be reason enough for the companies developing AI to take a more thoughtful‚Äîor slower‚Äîapproach.
‚ÄúMany people who would be in a great position to take advantage of further progress would now instead prefer to see a pause,‚Äù says signee Vincent Conitzer , a professor who works on AI at CMU. ‚ÄúIf nothing else, that should be a signal that something very unusual is up.‚Äù You Might Also Like ‚Ä¶ üì® Make the most of chatbots with our AI Unlocked newsletter Taylor Swift, Star Wars, Stranger Things , and Deadpool have one man in common Generative AI is playing a surprising role in Israel-Hamas disinformation The new era of social media looks as bad for privacy as the last one Johnny Cash‚Äôs Taylor Swift cover predicts the boring future of AI music Your internet browser does not belong to you üîå Charge right into summer with the best travel adapters , power banks , and USB hubs Senior Writer X Topics Fast Forward artificial intelligence ethics machine learning Elon Musk ChatGPT Alphabet Will Knight Matt Burgess Will Knight Reece Rogers Vittoria Elliott Will Knight Peter Guest Will Knight Facebook X Pinterest YouTube Instagram Tiktok More From WIRED Subscribe Newsletters Mattresses Reviews FAQ Wired Staff Coupons Black Friday Editorial Standards Archive Contact Advertise Contact Us Customer Care Jobs Press Center RSS Accessibility Help Cond√© Nast Store Do Not Sell My Personal Info ¬© 2023 Cond√© Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights.
WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Cond√© Nast.
Ad Choices Select international site United States LargeChevron UK Italia Jap√≥n Czech Republic & Slovakia
