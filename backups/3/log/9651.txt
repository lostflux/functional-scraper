Open Navigation Menu To revist this article, visit My Profile, then View saved stories.
Close Alert Backchannel Business Culture Gear Ideas Science Security Merch To revist this article, visit My Profile, then View saved stories.
Close Alert Search Backchannel Business Culture Gear Ideas Science Security Merch Podcasts Video Artificial Intelligence Climate Games Newsletters Magazine Events Wired Insider Jobs Coupons John Pavlus Science Computers Are Learning to See in Higher Dimensions The new deep learning techniques, which have shown promise in identifying lung tumors in CT scans more accurately than before, could someday lead to better medical diagnostics.
Illustration: Olena Shmahalo/Quanta Magazine Save this story Save Save this story Save End User Research Sector Automotive Health care Defense Source Data Sensors Images Technology Machine learning Machine vision Neural Network Computers can now drive cars , beat world champions at board games like chess and Go , and even write prose.
 The revolution in artificial intelligence stems in large part from the power of one particular kind of artificial neural network, whose design is inspired by the connected layers of neurons in the mammalian visual cortex. These â€œconvolutional neural networksâ€ (CNNs) have proved surprisingly adept at learning patterns in two-dimensional dataâ€”especially in computer vision tasks like recognizing handwritten words and objects in digital images.
Original story reprinted with permission from Quanta Magazine , an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developÂ­ments and trends in matheÂ­matics and the physical and life sciences.
But when applied to data sets without a built-in planar geometryâ€”say, models of irregular shapes used in 3D computer animation, or the point clouds generated by self-driving cars to map their surroundingsâ€”this powerful machine learning architecture doesnâ€™t work well. Around 2016, a new discipline called geometric deep learning emerged with the goal of lifting CNNs out of flatland.
Now researchers have delivered with a new theoretical framework for building neural networks that can learn patterns on any kind of geometric surface. These â€œ gauge-equivariant convolutional neural networks ,â€ or gauge CNNs, developed at the University of Amsterdam and Qualcomm AI Research by Taco Cohen, Maurice Weiler, Berkay Kicanaoglu, and Max Welling, can detect patterns not only in 2D arrays of pixels but also on spheres and asymmetrically curved objects. â€œThis framework is a fairly definitive answer to this problem of deep learning on curved surfaces,â€ Welling said.
Already, gauge CNNs have greatly outperformed their predecessors in learning patterns in simulated global climate data, which is naturally mapped onto a sphere. The algorithms may also prove useful for improving the vision of drones and autonomous vehicles that see objects in 3D, and for detecting patterns in data gathered from the irregularly curved surfaces of hearts, brains, or other organs.
Taco Cohen, a machine learning researcher at Qualcomm and the University of Amsterdam, is one of the lead architects of gauge-equivariant convolutional neural networks.
Photograph: Ork de Rooij The researchersâ€™ solution to getting deep learning to work beyond flatland also has deep connections to physics. Physical theories that describe the world, like Albert Einsteinâ€™s general theory of relativity and the Standard Model of particle physics, exhibit a property called â€œgauge equivariance.â€ This means that quantities in the world and their relationships donâ€™t depend on arbitrary frames of reference (or â€œgaugesâ€); they remain consistent whether an observer is moving or standing still, and no matter how far apart the numbers are on a ruler. Measurements made in those different gauges must be convertible into each other in a way that preserves the underlying relationships between things.
For example, imagine measuring the length of a football field in yards, then measuring it again in meters. The numbers will change, but in a predictable way. Similarly, two photographers taking a picture of an object from two different vantage points will produce different images, but those images can be related to each other. Gauge equivariance ensures that physicistsâ€™ models of reality stay consistent, regardless of their perspective or units of measurement. And gauge CNNs make the same assumption about data.
â€œThe same idea [from physics] that thereâ€™s no special orientationâ€”they wanted to get that into neural networks,â€ said Kyle Cranmer, a physicist at New York University who applies machine learning to particle physics data. â€œAnd they figured out how to do it.â€ Michael Bronstein, a computer scientist at Imperial College London, coined the term â€œgeometric deep learningâ€ in 2015 to describe nascent efforts to get off flatland and design neural networks that could learn patterns in nonplanar data. The termâ€”and the research effortâ€”soon caught on.
Bronstein and his collaborators knew that going beyond the Euclidean plane would require them to reimagine one of the basic computational procedures that made neural networks so effective at 2D image recognition in the first place. This procedure, called â€œconvolution,â€ lets a layer of the neural network perform a mathematical operation on small patches of the input data and then pass the results to the next layer in the network.
Culture Taylor Swift and BeyoncÃ© Are Resurrecting the American Movie Theater Angela Watercutter Gear The Best Home Depot Black Friday Deals Matt Jancer Gear Appleâ€™s Pledge to Support RCS Messaging Could Finally Kill SMS Boone Ashworth Business Swedenâ€™s Tesla Blockade Is Spreading Morgan Meaker â€œYou can think of convolution, roughly speaking, as a sliding window,â€ Bronstein explained. A convolutional neural network slides many of these â€œwindowsâ€ over the data like filters, with each one designed to detect a certain kind of pattern in the data. In the case of a cat photo, a trained CNN may use filters that detect low-level features in the raw input pixels, such as edges. These features are passed up to other layers in the network, which perform additional convolutions and extract higher-level features, like eyes, tails or triangular ears. A CNN trained to recognize cats will ultimately use the results of these layered convolutions to assign a labelâ€”say, â€œcatâ€ or â€œnot catâ€â€”to the whole image.
Illustration: Lucy Reading-Ikkanda/Quanta Magazine But that approach only works on a plane. â€œAs the surface on which you want to do your analysis becomes curved, then youâ€™re basically in trouble,â€ said Welling.
Performing a convolution on a curved surface â€” known in geometry as a manifold â€” is much like holding a small square of translucent graph paper over a globe and attempting to accurately trace the coastline of Greenland. You canâ€™t press the square onto Greenland without crinkling the paper, which means your drawing will be distorted when you lay it flat again. But holding the square of paper tangent to the globe at one point and tracing Greenlandâ€™s edge while peering through the paper (a technique known as Mercator projection) will produce distortions too. Alternatively, you could just place your graph paper on a flat world map instead of a globe, but then youâ€™d just be replicating those distortionsâ€”like the fact that the entire top edge of the map actually represents only a single point on the globe (the North Pole). And if the manifold isnâ€™t a neat sphere like a globe, but something more complex or irregular like the 3D shape of a bottle, or a folded protein, doing convolution on it becomes even more difficult.
Bronstein and his collaborators found one solution to the problem of convolution over non-Euclidean manifolds in 2015 , by reimagining the sliding window as something shaped more like a circular spiderweb than a piece of graph paper, so that you could press it against the globe (or any curved surface) without crinkling, stretching or tearing it.
Culture Taylor Swift and BeyoncÃ© Are Resurrecting the American Movie Theater Angela Watercutter Gear The Best Home Depot Black Friday Deals Matt Jancer Gear Appleâ€™s Pledge to Support RCS Messaging Could Finally Kill SMS Boone Ashworth Business Swedenâ€™s Tesla Blockade Is Spreading Morgan Meaker Changing the properties of the sliding filter in this way made the CNN much better at â€œunderstandingâ€ certain geometric relationships. For example, the network could automatically recognize that a 3D shape bent into two different posesâ€”like a human figure standing up and a human figure lifting one legâ€”were instances of the same object, rather than two completely different objects. The change also made the neural network dramatically more efficient at learning. Standard CNNs â€œused millions of examples of shapes [and needed] training for weeks,â€ Bronstein said. â€œWe used something like 100 shapes in different poses and trained for maybe half an hour.â€ At the same time, Taco Cohen and his colleagues in Amsterdam were beginning to approach the same problem from the opposite direction. In 2015, Cohen, a graduate student at the time, wasnâ€™t studying how to lift deep learning out of flatland. Rather, he was interested in what he thought was a practical engineering problem: data efficiency, or how to train neural networks with fewer examples than the thousands or millions that they often required. â€œDeep learning methods are, letâ€™s say, very slow learners,â€ Cohen said. This poses few problems if youâ€™re training a CNN to recognize, say, cats (given the bottomless supply of cat images on the internet). But if you want the network to detect something more important, like cancerous nodules in images of lung tissue, then finding sufficient training data â€” which needs to be medically accurate, appropriately labeled, and free of privacy issues â€” isnâ€™t so easy. The fewer examples needed to train the network, the better.
Cohen knew that one way to increase the data efficiency of a neural network would be to equip it with certain assumptions about the data in advance â€” like, for instance, that a lung tumor is still a lung tumor, even if itâ€™s rotated or reflected within an image. Usually, a convolutional network has to learn this information from scratch by training on many examples of the same pattern in different orientations. In 2016, Cohen and Welling co-authored a paper defining how to encode some of these assumptions into a neural network as geometric symmetries. This approach worked so well that by 2018, Cohen and co-author Marysia Winkels had generalized it even further, demonstrating promising results on recognizing lung cancer in CT scans: Their neural network could identify visual evidence of the disease using just one-tenth of the data used to train other networks.
Culture Taylor Swift and BeyoncÃ© Are Resurrecting the American Movie Theater Angela Watercutter Gear The Best Home Depot Black Friday Deals Matt Jancer Gear Appleâ€™s Pledge to Support RCS Messaging Could Finally Kill SMS Boone Ashworth Business Swedenâ€™s Tesla Blockade Is Spreading Morgan Meaker The Amsterdam researchers kept on generalizing. Thatâ€™s how they found their way to gauge equivariance.
Physics and machine learning have a basic similarity. As Cohen put it, â€œBoth fields are concerned with making observations and then building models to predict future observations.â€ Crucially, he noted, both fields seek models not of individual things â€” itâ€™s no good having one description of hydrogen atoms and another of upside-down hydrogen atoms â€” but of general categories of things. â€œPhysics, of course, has been quite successful at that.â€ Equivariance (or â€œcovariance,â€ the term that physicists prefer) is an assumption that physicists since Einstein have relied on to generalize their models. â€œIt just means that if youâ€™re describing some physics right, then it should be independent of what kind of â€˜rulersâ€™ you use, or more generally what kind of observers you are,â€ explained Miranda Cheng, a theoretical physicist at the University of Amsterdam who wrote a paper with Cohen and others exploring the connections between physics and gauge CNNs. Or as Einstein himself put it in 1916: â€œThe general laws of nature are to be expressed by equations which hold good for all systems of coordinates.â€ Miranda Cheng, a physicist at the University of Amsterdam.
Photographer: Ilvy Njiokiktjien/Quanta Magazine Convolutional networks became one of the most successful methods in deep learning by exploiting a simple example of this principle called â€œtranslation equivariance.â€ A window filter that detects a certain feature in an image â€” say, vertical edges â€” will slide (or â€œtranslateâ€) over the plane of pixels and encode the locations of all such vertical edges; it then creates a â€œfeature mapâ€ marking these locations and passes it up to the next layer in the network. Creating feature maps is possible because of translation equivariance: The neural network â€œassumesâ€ that the same feature can appear anywhere in the 2D plane and is able to recognize a vertical edge as a vertical edge whether itâ€™s in the upper right corner or the lower left.
â€œThe point about equivariant neural networks is [to] take these obvious symmetries and put them into the network architecture so that itâ€™s kind of free lunch,â€ Weiler said.
By 2018, Weiler, Cohen and their doctoral supervisor Max Welling had extended this â€œfree lunchâ€ to include other kinds of equivariance. Their â€œgroup-equivariantâ€ CNNs could detect rotated or reflected features in flat images without having to train on specific examples of the features in those orientations; spherical CNNs could create feature maps from data on the surface of a sphere without distorting them as flat projections.
Culture Taylor Swift and BeyoncÃ© Are Resurrecting the American Movie Theater Angela Watercutter Gear The Best Home Depot Black Friday Deals Matt Jancer Gear Appleâ€™s Pledge to Support RCS Messaging Could Finally Kill SMS Boone Ashworth Business Swedenâ€™s Tesla Blockade Is Spreading Morgan Meaker These approaches still werenâ€™t general enough to handle data on manifolds with a bumpy, irregular structure â€” which describes the geometry of almost everything, from potatoes to proteins, to human bodies, to the curvature of space-time. These kinds of manifolds have no â€œglobalâ€ symmetry for a neural network to make equivariant assumptions about: Every location on them is different.
Illustration: Lucy Reading-Ikkanda/Quanta Magazine The challenge is that sliding a flat filter over the surface can change the orientation of the filter, depending on the particular path it takes. Imagine a filter designed to detect a simple pattern: a dark blob on the left and a light blob on the right. Slide it up, down, left or right on a flat grid, and it will always stay right-side up. But even on the surface of a sphere, this changes. If you move the filter 180 degrees around the sphereâ€™s equator, the filterâ€™s orientation stays the same: dark blob on the left, light blob on the right. However, if you slide it to the same spot by moving over the sphereâ€™s north pole, the filter is now upside down â€” dark blob on the right, light blob on the left. The filter wonâ€™t detect the same pattern in the data or encode the same feature map. Move the filter around a more complicated manifold, and it could end up pointing in any number of inconsistent directions.
Culture Taylor Swift and BeyoncÃ© Are Resurrecting the American Movie Theater Angela Watercutter Gear The Best Home Depot Black Friday Deals Matt Jancer Gear Appleâ€™s Pledge to Support RCS Messaging Could Finally Kill SMS Boone Ashworth Business Swedenâ€™s Tesla Blockade Is Spreading Morgan Meaker Luckily, physicists since Einstein have dealt with the same problem and found a solution: gauge equivariance.
The key, explained Welling, is to forget about keeping track of how the filterâ€™s orientation changes as it moves along different paths. Instead, you can choose just one filter orientation (or gauge), and then define a consistent way of converting every other orientation into it.
The catch is that while any arbitrary gauge can be used in an initial orientation, the conversion of other gauges into that frame of reference must preserve the underlying pattern â€” just as converting the speed of light from meters per second into miles per hour must preserve the underlying physical quantity. With this gauge-equivariant approach, said Welling, â€œthe actual numbers change, but they change in a completely predictable way.â€ Cohen, Weiler and Welling encoded gauge equivariance â€” the ultimate â€œfree lunchâ€ â€” into their convolutional neural network in 2019. They did this by placing mathematical constraints on what the neural network could â€œseeâ€ in the data via its convolutions; only gauge-equivariant patterns were passed up through the networkâ€™s layers. â€œBasically you can give it any surfaceâ€ â€” from Euclidean planes to arbitrarily curved objects, including exotic manifolds like Klein bottles or four-dimensional space-time â€” â€œand itâ€™s good for doing deep learning on that surface,â€ said Welling.
The theory of gauge-equivariant CNNs is so generalized that it automatically incorporates the built-in assumptions of previous geometric deep learning approaches â€” like rotational equivariance and shifting filters on spheres. Even Michael Bronsteinâ€™s earlier method, which let neural networks recognize a single 3D shape bent into different poses, fits within it. â€œGauge equivariance is a very broad framework. It contains what we did in 2015 as particular settings,â€ Bronstein said.
A gauge CNN would theoretically work on any curved surface of any dimensionality, but Cohen and his co-authors have tested it on global climate data, which necessarily has an underlying 3D spherical structure. They used their gauge-equivariant framework to construct a CNN trained to detect extreme weather patterns, such as tropical cyclones, from climate simulation data.
In 2017 , government and academic researchers used a standard convolutional network to detect cyclones in the data with 74% accuracy; last year, the gauge CNN detected the cyclones with 97.9% accuracy. (It also outperformed a less general geometric deep learning approach designed in 2018 specifically for spheres â€” that system was 94% accurate.) Mayur Mudigonda, a climate scientist at Lawrence Berkeley National Laboratory who uses deep learning, said heâ€™ll continue to pay attention to gauge CNNs. â€œThat aspect of human visual intelligenceâ€ â€” spotting patterns accurately regardless of their orientation â€” â€œis what weâ€™d like to translate into the climate community,â€ he said. Qualcomm, a chip manufacturer which recently hired Cohen and Welling and acquired a startup they built incorporating their early work in equivariant neural networks, is now planning to apply the theory of gauge CNNs to develop improved computer vision applications , like a drone that can â€œseeâ€ in 360 degrees at once. (This fish-eye view of the world can be naturally mapped onto a spherical surface, just like global climate data.) Meanwhile, gauge CNNs are gaining traction among physicists like Cranmer, who plans to put them to work on data from simulations of subatomic particle interactions. â€œWeâ€™re analyzing data related to the strong [nuclear] force, trying to understand whatâ€™s going on inside of a proton,â€ Cranmer said. The data is four-dimensional, he said, â€œso we have a perfect use case for neural networks that have this gauge equivariance.â€ Culture Taylor Swift and BeyoncÃ© Are Resurrecting the American Movie Theater Angela Watercutter Gear The Best Home Depot Black Friday Deals Matt Jancer Gear Appleâ€™s Pledge to Support RCS Messaging Could Finally Kill SMS Boone Ashworth Business Swedenâ€™s Tesla Blockade Is Spreading Morgan Meaker Risi Kondor, a former physicist who now studies equivariant neural networks, said the potential scientific applications of gauge CNNs may be more important than their uses in AI.
â€œIf you are in the business of recognizing cats on YouTube and you discover that youâ€™re not quite as good at recognizing upside-down cats, thatâ€™s not great, but maybe you can live with it,â€ he said. But for physicists, itâ€™s crucial to ensure that a neural network wonâ€™t misidentify a force field or particle trajectory because of its particular orientation. â€œItâ€™s not just a matter of convenience,â€ Kondor saidâ€”â€œitâ€™s essential that the underlying symmetries be respected.â€ But while physicistsâ€™ math helped inspire gauge CNNs, and physicists may find ample use for them, Cohen noted that these neural networks wonâ€™t be discovering any new physics themselves. â€œWeâ€™re now able to design networks that can process very exotic kinds of data, but you have to know what the structure of that data isâ€ in advance, he said. In other words, the reason physicists can use gauge CNNs is because Einstein already proved that space-time can be represented as a four-dimensional curved manifold. Cohenâ€™s neural network wouldnâ€™t be able to â€œseeâ€ that structure on its own. â€œLearning of symmetries is something we donâ€™t do,â€ he said, though he hopes it will be possible in the future.
Cohen canâ€™t help but delight in the interdisciplinary connections that he once intuited and has now demonstrated with mathematical rigor. â€œI have always had this sense that machine learning and physics are doing very similar things,â€ he said. â€œThis is one of the things that I find really marvelous: We just started with this engineering problem, and as we started improving our systems, we gradually unraveled more and more connections.â€ Original story reprinted with permission from Quanta Magazine , an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.
Hollywood bets on a future of quick clips and tiny screens Mind control for the massesâ€” no implant needed Here's what the world will look like in 2030 ... right ? Internet deception is here to stayâ€” what do we do now ? The war vet, the dating site, and the phone call from hell ğŸ‘ Will AI as a field "hit the wall" soon ? Plus, the latest news on artificial intelligence ğŸƒğŸ½â€â™€ï¸ Want the best tools to get healthy? Check out our Gear teamâ€™s picks for the best fitness trackers , running gear (including shoes and socks ), and best headphones Topics Quanta Magazine artificial intelligence physics Matt Reynolds Emily Mullin Robin Andrews Matt Simon Celia Ford Maryn McKenna Garrett M. Graff Erica Kasper Facebook X Pinterest YouTube Instagram Tiktok More From WIRED Subscribe Newsletters Mattresses Reviews FAQ Wired Staff Coupons Black Friday Editorial Standards Archive Contact Advertise Contact Us Customer Care Jobs Press Center RSS Accessibility Help CondÃ© Nast Store Do Not Sell My Personal Info Â© 2023 CondÃ© Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights.
WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of CondÃ© Nast.
Ad Choices Select international site United States LargeChevron UK Italia JapÃ³n Czech Republic & Slovakia
