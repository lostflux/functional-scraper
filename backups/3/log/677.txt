Toggle Navigation News Events TNW Conference 2024 June 20 & 21, 2024 TNW Vision: 2024 All events Spaces Programs Newsletters Partner with us Jobs Contact News news news news Latest Deep tech Sustainability Ecosystems Data and security Fintech and ecommerce Future of work More Startups and technology Investors and funding Government and policy Corporates and innovation Gadgets & apps Early bird Business passes are 90% SOLD OUT 🎟️ Buy now before they are gone → Data and security ‘Unsafe’ AI images proliferate online. Study suggests 3 ways to curb the scourge A significant percentage of AI-generated images are violent or dehumanising, finds new research Over the past year, AI image generators have taken the world by storm. Heck, even our distinguished writers at TNW use them from time to time.
Truth is, tools like Stable Diffusion, Latent Diffusion, or DALL·E can be incredibly useful for producing unique images from simple prompts — like this picture of Elon Musk riding a unicorn.
But it’s not all fun and games. Users of these AI models can just as easily generate hateful, dehumanising, and pornographic images at the click of a button — with little to no repercussions.
“People use these AI tools to draw all kinds of images, which inherently presents a risk,” s aid researcher Yiting Qu from the CISPA Helmholtz Center for Information Security in Germany.
 Things become especially problematic when disturbing or explicit images are shared on mainstream media platforms, she stressed.
While these risks seem quite obvious, there has been little research undertaken so far to quantify the dangers and create safe guardrails for their use. “Currently, there isn’t even a universal definition in the research community of what is and is not an unsafe image,” said Qu.
The <3 of EU tech The latest rumblings from the EU tech scene, a story from our wise ol' founder Boris, and some questionable AI art. It's free, every week, in your inbox. Sign up now! To illuminate the issue, Qu and her team investigated the most popular AI image generators, the prevalence of unsafe images on these platforms, and three ways to prevent their creation and circulation online.
The researchers fed four prominent AI image generators with text prompts from sources known for unsafe content, such as the far-right platform 4chan. Shockingly, 14.56% of images generated were classified as “unsafe,” with Stable Diffusion producing the highest percentage at 18.92%. These included images with sexually explicit, violent, disturbing, hateful, or political content.
Creating safeguards The fact that so many uncertain images were generated in Qu’s study shows that existing filters do not do their job adequately. The researcher developed her own filter, which scores a much higher hit rate in comparison, but suggests a number of other ways to curb the threat.
One way to prevent the spread of inhumane imagery is to program AI image generators to not generate this imagery in the first place, she said. Essentially, if AI models aren’t trained on unsafe images, they can’t replicate them.
Beyond that, Qu recommends blocking unsafe words from the search function, so that users can’t put together prompts that produce harmful images. For those images already circulating, “there must be a way of classifying these and deleting them online,” she said.
With all these measures, the challenge is to find the right balance. “There needs to be a trade-off between freedom and security of content,” said Qu. “But when it comes to preventing these images from experiencing wide circulation on mainstream platforms, I think strict regulation makes sense.” Aside from generating harmful content, the makers of AI text-to-image software have come under fire for a range of issues, such as stealing artists’ work and amplifying dangerous gender and race stereotypes.
While initiatives like the AI Safety Summit, which took place in the UK this month, aim to create guardrails for the technology, critics claim big tech companies hold too much sway over the negotiations. Whether that’s true or not, the reality is that, at present, proper, safe management of AI is patchy at best and downright alarming at its worst.
Story by Siôn Geschwindt Siôn is a reporter at TNW. From startups to tech giants, he covers the length and breadth of the European tech ecosystem. With a background (show all) Siôn is a reporter at TNW. From startups to tech giants, he covers the length and breadth of the European tech ecosystem. With a background in environmental science, Siôn has a bias for solutions delivering environmental and social impact at scale.
Get the TNW newsletter Get the most important tech news in your inbox each week.
Also tagged with Artificial intelligence Social media Story by Siôn Geschwindt Popular articles 1 Musk mulls removing X, formerly Twitter, from EU to dodge disinformation laws 2 TikTok complies with EU demands against Israel-Hamas disinformation 3 German anti-racism agency quits X amid Israel-Hamas disinformation wave 4 EU online piracy on the rise as consumers feel the pinch 5 Ukraine’s fight against disinformation is creating a new startup sector Related Articles deep tech New erotic roleplaying chatbots promise to indulge your sexual fantasies deep tech UN creates AI advisory body to ‘maximise’ benefits for humankind Join TNW All Access Watch videos of our inspiring talks for free → deep tech Google’s AI could soon consume as much electricity as Ireland, study finds deep tech How this Berlin startup deploys AI to make preventative healthcare accessible The heart of tech More TNW Media Events Programs Spaces Newsletters Jobs in tech About TNW Partner with us Jobs Terms & Conditions Cookie Statement Privacy Statement Editorial Policy Masthead Copyright © 2006—2023, The Next Web B.V. Made with <3 in Amsterdam.
