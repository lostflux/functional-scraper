Open Navigation Menu To revist this article, visit My Profile, then View saved stories.
Close Alert Backchannel Business Culture Gear Ideas Science Security Merch To revist this article, visit My Profile, then View saved stories.
Close Alert Search Backchannel Business Culture Gear Ideas Science Security Merch Podcasts Video Artificial Intelligence Climate Games Newsletters Magazine Events Wired Insider Jobs Coupons Will Knight Business Autonomous Weapons Are Here, but the World Isnâ€™t Ready for Them Illustration: Jenny Sharaf; Getty Images Save this story Save Save this story Save End User Government Sector Defense This may be remembered as the year when the world learned that lethal autonomous weapons had moved from a futuristic worry to a battlefield reality.
 Itâ€™s also the year when policymakers failed to agree on what to do about it.
On Friday, 120 countries participating in the United Nationsâ€™ Convention on Certain Conventional Weapons could not agree on whether to limit the development or use of lethal autonomous weapons. Instead, they pledged to continue and â€œintensifyâ€ discussions.
â€œIt's very disappointing, and a real missed opportunity,â€ says Neil Davison, senior scientific and policy adviser at the International Committee of the Red Cross , a humanitarian organization based in Geneva.
The failure to reach agreement came roughly nine months after the UN reported that a lethal autonomous weapon had been used for the first time in armed conflict, in the Libyan civil war.
In recent years, more weapon systems have incorporated elements of autonomy. Some missiles can, for example, fly without specific instructions within a given area; but they still generally rely on a person to launch an attack. And most governments say that, for now at least, they plan to keep a human â€œin the loopâ€ when using such technology.
But advances in artificial intelligence algorithms , sensors, and electronics have made it easier to build more sophisticated autonomous systems, raising the prospect of machines that can decide on their own when to use lethal force.
A growing list of countries, including Brazil, South Africa, New Zealand, and Switzerland, argue that lethal autonomous weapons should be restricted by treaty, as chemical and biological weapons and land mines have been. Germany and France support restrictions on certain kinds of autonomous weapons, including potentially those that target humans. China supports an extremely narrow set of restrictions.
Other nations, including the US, Russia, India, the UK, and Australia, object to a ban on lethal autonomous weapons, arguing that they need to develop the technology to avoid being placed at a strategic disadvantage.
Killer robots have long captured the public imagination, inspiring both beloved sci-fi characters and dystopian visions of the future.
 A recent renaissance in AI, and the creation of new types of computer programs capable of out-thinking humans in certain realms, has prompted some of techâ€™s biggest names to warn about the existential threat posed by smarter machines.
â€œThe technology is developing much faster than the military-political discussion. And weâ€™re heading, by default, to the worst possible outcome.â€ Max Tegmark, MIT professor and cofounder, the Future of Life Institute The issue became more pressing this year, after the UN report, which said a Turkish-made drone known as Kargu-2 was used in Libyaâ€™s civil war in 2020. Forces aligned with the Government of National Accord reportedly launched drones against troops supporting Libyan National Army leader General Khalifa Haftar that targeted and attacked people independently.
â€œLogistics convoys and retreating Haftar-affiliated forces were â€¦ hunted down and remotely engaged by the unmanned combat aerial vehicles,â€ the report states. The systems â€œwere programmed to attack targets without requiring data connectivity between the operator and the munition: in effect, a true â€˜fire, forget and findâ€™ capability.â€ Culture Taylor Swift and BeyoncÃ© Are Resurrecting the American Movie Theater Angela Watercutter Gear The Best Home Depot Black Friday Deals Matt Jancer Gear Appleâ€™s Pledge to Support RCS Messaging Could Finally Kill SMS Boone Ashworth Business Swedenâ€™s Tesla Blockade Is Spreading Morgan Meaker The news reflects the speed at which autonomy technology is improving. â€œThe technology is developing much faster than the military-political discussion,â€ says Max Tegmark , a professor at MIT and cofounder of the Future of Life Institute , an organization dedicated to addressing existential risks facing humanity. â€œAnd we're heading, by default, to the worst possible outcome.â€ Tegmark is among a growing number of technologists concerned about the proliferation of AI weapons. The Future of Life Institute has produced two short films to raise awareness of the risks posed by so-called â€œslaughterbots.â€ The most recent of these, released in November, focuses on the potential for autonomous drones to carry out targeted assassinations.
â€œThere's a rising tide against the proliferation of slaughterbots,â€ Tegmark says. â€œWe are not saying ban all military AI but just â€˜if human, then kill.â€™ So, ban weapons that target humans.â€ One challenge with prohibiting, or policing, use of autonomous weapons is the difficulty of knowing when theyâ€™ve been used. The company behind the Kargu-2 drone, STM, has not confirmed that it can target and fire on people without human control. The companyâ€™s website now refers to a human controller making decisions about use of lethal force. â€œPrecision strike mission is fully performed by the operator, in line with the Man-in-the-Loop principle,â€ it reads. But a cached version of the site from June contains no such caveat. STM did not respond to a request for comment.
â€œWe are entering a gray area where we're not going to really know how autonomous a drone was when it was used in an attack,â€ says Paul Scharre , vice president and director of studies at the Center for New American Security and the author of Army of None: Autonomous Weapons and the Future of War.
 â€œThat raises some really difficult questions about accountability.â€ Another example of this ambiguity appeared in September with reports of Israel using an AI-assisted weapon to assassinate a prominent Iranian nuclear scientist. According to an investigation by The New York Times , a remotely operated machine gun used a form of facial recognition and autonomy, but itâ€™s unclear whether the weapon was capable of operating without human approval.
The uncertainty is â€œexacerbated by the fact that many companies use the word autonomy when theyâ€™re hyping up the capabilities of their technology,â€ Scharre says. Other recent drone attacks suggest that the underlying technologies are advancing quickly.
Culture Taylor Swift and BeyoncÃ© Are Resurrecting the American Movie Theater Angela Watercutter Gear The Best Home Depot Black Friday Deals Matt Jancer Gear Appleâ€™s Pledge to Support RCS Messaging Could Finally Kill SMS Boone Ashworth Business Swedenâ€™s Tesla Blockade Is Spreading Morgan Meaker In the US, the Defense Advanced Research Projects Agency has been conducting experiments involving large numbers of drones and ground vehicles that collaborate in ways that are challenging for human operators to monitor and control. The US Air Force is also investigating ways that AI could assist or replace fighter pilots, holding a series of dogfights between human pilots and AI ones.
Even if there were a treaty restricting autonomous weapons, Scharre says â€œthere is asymmetry between democracies and authoritarian governments in terms of compliance.â€ Adversaries such as Russia and China might agree to limit the development of autonomous weapons but continue working on them without the same accountability.
Some argue that this means AI weapons need to be developed, if only as defensive measures against the speed and complexity with which autonomous systems can operate.
A Pentagon official told a conference at the US Military Academy in April that it may be necessary to consider removing humans from the chain of command in situations where they cannot respond rapidly enough.
The potential for adversaries to gain an edge is clearly a major concern for military planners. In 2034: A Novel of the Next World War , which was excerpted in WIRED, the writer Elliot Ackerman and US Admiral James Stavridis imagine â€œa massive cyberattack against the United Statesâ€”that our opponents will refine cyber stealth and artificial intelligence in a kind of a witch's brew and then use it against us.â€ Despite previous controversies over military use of AI, US tech companies continue to help the Pentagon hone its AI skills. The National Security Commission on AI, a group charged with reviewing the strategic potential of AI that included representatives from Google, Microsoft, Amazon, and Oracle, recommended investing heavily in AI.
Davison, who has been involved with the UN discussions, says technology is outpacing the policy debate. â€œGovernments really need to take concrete steps to adopt new rules,â€ he adds.
He still holds out hope that countries will agree on some restrictions, even if it happens outside of the UN. He says countriesâ€™ actions suggest that they disapprove of autonomous weapons. â€œWhat's quite interesting is that the allegations of the use of autonomous weapons to target people directly tend to be refuted by those involved, whether militaries or governments or manufacturers,â€ he says.
The race to find â€œgreenâ€ helium Your rooftop garden could be a solar-powered farm This new tech cuts through rock without grinding into it The best Discord bots for your server How to guard against smishing attacks ğŸ‘ï¸ Explore AI like never before with our new database ğŸƒğŸ½â€â™€ï¸ Want the best tools to get healthy? Check out our Gear teamâ€™s picks for the best fitness trackers , running gear (including shoes and socks ), and best headphones Senior Writer X Topics artificial intelligence drones Weapons and Ammo Military Year in Review Will Knight Will Knight Peter Guest Will Knight Khari Johnson Paresh Dave Amanda Hoover Khari Johnson Facebook X Pinterest YouTube Instagram Tiktok More From WIRED Subscribe Newsletters Mattresses Reviews FAQ Wired Staff Coupons Black Friday Editorial Standards Archive Contact Advertise Contact Us Customer Care Jobs Press Center RSS Accessibility Help CondÃ© Nast Store Do Not Sell My Personal Info Â© 2023 CondÃ© Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights.
WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of CondÃ© Nast.
Ad Choices Select international site United States LargeChevron UK Italia JapÃ³n Czech Republic & Slovakia
