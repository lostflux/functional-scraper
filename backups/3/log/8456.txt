The Verge homepage The Verge homepage The Verge The Verge logo.
/ Tech / Reviews / Science / Entertainment / More Menu Expand Menu Artificial Intelligence / Tech / Creators Stable Diffusion made copying artists and generating porn harder and users are mad Stable Diffusion made copying artists and generating porn harder and users are mad / Changes to the AI text-to-image model make it harder for users to mimic specific artistsâ€™ styles or generate NSFW output, but offer other functional improvements.
By James Vincent , a senior reporter who has covered AI, robotics, and more for eight years at The Verge.
| Share this story Users of AI image generator Stable Diffusion are angry about an update to the software that â€œnerfsâ€ its ability to generate NSFW output and pictures in the style of specific artists.
Stability AI, the company that funds and disseminates the software, announced Stable Diffusion Version 2 early this morning European time. The update re-engineers key components of the model and improves certain features like upscaling (the ability to increase the resolution of images) and in-painting (context-aware editing). But, the changes also make it harder for Stable Diffusion to generate certain types of images that have attracted both controversy and criticism. These include nude and pornographic output, photorealistic pictures of celebrities, and images that mimic the artwork of specific artists.
â€œThey have nerfed the modelâ€ â€œThey have nerfed the model,â€ commented one user on a Stable Diffusion sub-reddit. â€œItâ€™s kinda an unpleasant surprise,â€ said another on the softwareâ€™s official Discord server.
Users note that asking Version 2 of Stable Diffusion to generate images in the style of Greg Rutkowski â€” a digital artist whose name has become a literal shorthand for producing high-quality images â€” no longer creates artwork that closely resembles his own. (Compare these two images, for example). â€œWhat did you do to gregğŸ˜”,â€ commented one user on Discord.
Changes to Stable Diffusion are notable, as the software is hugely influential and helps set norms in the fast-moving generative AI scene. Unlike rival models like OpenAIâ€™s DALL-E, Stable Diffusion is open source. This allows the community to quickly improve on the tool and for developers to integrate it into their products free of charge. But it also means Stable Diffusion has fewer constraints in how itâ€™s used and, as a consequence, has attracted significant criticism. In particular, many artists, like Rutkowski, are annoyed that Stable Diffusion and other image generating models were trained on their artwork without their consent and can now reproduce their styles. Whether or not this sort of AI-enabled copying is legal is something of an open question.
 Experts say training AI models on copyright-protected data is likely legal, but that certain use-cases could be challenged in court.
Stable Diffusionâ€™s users have speculated that the changes to the model were made by Stability AI to mitigate such potential legal challenges. However, when The Verge asked Stability AIâ€™s founder Emad Mostaque if this was the case in a private chat, Mostaque did not answer. Mostaque did confirm, though that Stability AI has not removed artistsâ€™ images from the training data (as many users have speculated). Instead, the modelâ€™s reduced ability to copy artists is a result of changes made to how the software encodes and retrieves data.
â€œThere has been no specific filtering of artists here,â€ Mostaque told The Verge.
 (He also expanded on the technical underpinning of these changes in a message posted on Discord.
) What has been removed from Stable Diffusionâ€™s training data, though, is nude and pornographic images. AI image generators are already being used to generate NSFW output , including both photorealistic and anime-style pictures. However, these models can also be used to generate NSFW imagery resembling specific individuals (known as non-consensual pornography) and images of child abuse.
Discussing the changes Stable Diffusion Version 2 in the softwareâ€™s official Discord, Mostaque notes this latter use-case is the reason for filtering out NSFW content. â€œcanâ€™t have kids & nsfw in an open model,â€ says Mostaque (as the two sorts of images can be combined to create child sexual abuse material), â€œso get rid of the kids or get rid of the nsfw.â€ One user on Stable Diffusionâ€™s sub-reddit said the removal of NSFW content was â€œcensorship,â€ and â€œagainst the spirit philosophy of Open Source community.â€ Said the user: â€œTo choose to do NSFW content or not, should be in the hands of the end user, no [sic] in a limited/censored model.â€ Others, though, noted that the open source nature of Stable Diffusion mean nude training data can easily be added back into third-party releases and that the new software doesnâ€™t affect earlier versions: â€œDo not freak out about V2.0 lack of artists/NSFW, youâ€™ll be able to generate your favorite celeb naked soon & anyway you already can.â€ Although the changes to Stable Diffusion Version 2 have annoyed some users, many others praised its potential for deeper functionality, as with the softwareâ€™s new ability to produce content that matches the depth of an existing image. Others said the changes did make it harder to quickly produce high-quality images, but that the community would likely add back this functionality in future versions. As one user on Discord summarized the changes : â€œ2.0 is better at interpreting prompts and making coherent photographic images in my experience so far. it will not make any rutkowski titties though.â€ Mostaque himself compared the new model to a pizza base that lets anyone add ingredients (i.e. training data) of their choice. â€œA good model should be usable by everyone and if you want to add stuff add stuff,â€ he said on Discord.
Mostaque also said future versions of Stable Diffusion would use training datasets that would allow artists to opt-in or opt-out â€” a feature that many artists have requested, and that could help mitigate some criticism. â€œWe are trying to be super transparent as we improve the base models and incorporate community feedback,â€ Mostaque told The Verge.
A public demo of Stable Diffusion Version 2 can be accessed here (though due to high demands from users the model may be inaccessible or slow).
Sam Altman fired as CEO of OpenAI Windows is now an app for iPhones, iPads, Macs, and PCs Apple pulls its ads from X after Muskâ€™s antisemitic posts Screens are good, actually Half-Life gets a big update for its 25th anniversary Verge Deals / Sign up for Verge Deals to get deals on products we've tested sent to your inbox daily.
From our sponsor Advertiser Content From More from Artificial Intelligence Universal Music sues AI company Anthropic for distributing song lyrics OpenAI is opening up DALL-E 3 access YouTube might make an official way to create AI Drake fakes The worldâ€™s biggest AI models arenâ€™t very transparent, Stanford study says Advertiser Content From Terms of Use Privacy Notice Cookie Policy Do Not Sell Or Share My Personal Info Licensing FAQ Accessibility Platform Status How We Rate and Review Products Contact Tip Us Community Guidelines About Ethics Statement The Verge is a vox media network Advertise with us Jobs @ Vox Media Â© 2023 Vox Media , LLC. All Rights Reserved
