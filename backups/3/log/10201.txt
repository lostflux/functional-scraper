Site Navigation The Atlantic Popular Latest Newsletters Sections Politics Ideas Fiction Technology Science Photo Business Culture Planet Global Books Podcasts Health Education Projects Features Family Events Washington Week Progress Newsletters Explore The Atlantic Archive Play The Atlantic crossword The Print Edition Latest Issue Past Issues Give a Gift Search The Atlantic Quick Links Dear Therapist Crossword Puzzle Magazine Archive Your Subscription Popular Latest Newsletters Sign In Subscribe A gift that gets them talking.
Give a year of stories to spark conversation, Plus a free tote.
The Next Pandemic Could Start With a Terrorist Attack Nations around the world should come together now to determine how best to protect humans from biowarfare.
In 1770, the German chemist Carl Wilhelm Scheele performed an experiment and noticed that he’d created a noxious gas. He named it “dephlogisticated muriatic acid.” We know it today as chlorine.
Two centuries later, another German chemist, Fritz Haber, invented a process to synthesize and mass-produce ammonia, which revolutionized agriculture by generating the modern fertilizer industry. He won the Nobel Prize in Chemistry in 1918. But that same research, combined with Scheele’s earlier discovery, helped create the chemical-weapons program that Germany used in World War I. This is an example of what’s known as the “dual-use dilemma,” in which scientific and technological research is intended for good, but can also, either intentionally or accidentally, be used for harm.
In both chemistry and physics, the dual-use dilemma has long been a concern, and it has led to international treaties limiting the most worrisome applications of problematic research. Because of the Convention on the Prohibition of the Development, Production, Stockpiling and Use of Chemical Weapons and on Their Destruction (otherwise known as the Chemical Weapons Convention, or CWC), a treaty signed by 130 countries, many dangerous chemicals that are sometimes used in scientific or medical research have to be monitored and inspected.
One example is ricin, which is produced naturally in castor seeds and is lethal to humans in the tiniest amounts. A brief exposure in a mist or a few grains of powder can be fatal, so it is on the CWC list. Triethanolamine, which is used to treat ear infections and impacted earwax, and is an ingredient to thicken face creams and balance the pH of shaving foams, is listed as well because it can also be used to manufacture hydrazoic acid, otherwise known as mustard gas.
Similar international treaties, enforcement protocols, and agencies exist to monitor dual uses in chemistry, physics, and artificial intelligence. But synthetic biology—which seeks to design or redesign organisms on a molecular level for new purposes, making them adaptable to different environments or giving them different abilities—is so new that such treaties don’t yet exist for it, even though discussions about how to prevent harm have been happening for decades within the scientific community.
From the March 2022 issue: It’s your friends who break your heart In 2000, a team of researchers at the State University of New York at Stony Brook kicked off a two-year experiment to determine whether they could synthesize a live virus from scratch using only publicly available genetic information, off-the-shelf chemicals, and mail-order DNA. (The project was financed with $300,000 from the Defense Advanced Research Projects Agency, as part of a program to develop biowarfare countermeasures.) The researchers purchased short stretches of DNA and painstakingly pieced them together, using 19 additional markers to distinguish their synthetic virus from the natural strain they were attempting to reproduce.
They succeeded. On July 12, 2002—just after Americans had celebrated the first Fourth of July following the 9/11 terrorist attacks, when jittery millions were relieved that another horrific event hadn’t happened on that holiday—those scientists announced that they had re-created the poliovirus in their lab using code, material, and equipment that anyone, even al-Qaeda, could get their hands on. They’d made the virus to send a warning that terrorists might be making biological weapons and that bad actors no longer needed a live virus to weaponize a dangerous pathogen such as smallpox or Ebola.
Poliovirus is perhaps the most studied virus of all time, and at the time of the experiment samples of the virus were stored in labs around the world. The goal of this team’s work wasn’t to reintroduce poliovirus into the wild, but to learn how to synthesize viruses. It was the first time anyone had created this type of virus from scratch, and the Department of Defense hailed the team’s research as a massive technical achievement.
Knowing how to synthesize viral DNA helped the United States gain new insights into how viruses mutate, how they become immune to vaccines, and how they could be developed as weapons. And although creating a virus to study how it might be used as a bioweapon may sound legally questionable, the project didn’t violate any existing dual-use treaties, not even a 1972 treaty explicitly banning germ weapons, which outlaws manufacturing disease-producing agents—such as bacteria, viruses, and biological toxins—that could be used to harm people, animals, or plants.
Nonetheless, the scientific community was incensed. Intentionally making a “synthetic human pathogen” was “irresponsible,” J. Craig Venter, a geneticist and synthetic biology’s progenitor, said at the time. But this was no isolated incident. Consider what happened with smallpox.
The World Health Organization declared smallpox eradicated in 1979. This marked a major human achievement, because smallpox is a truly diabolical disease—extremely contagious, and with no known cure. It causes high fever, vomiting, severe stomachache, a red rash, and painful, yellowish, pus-filled domes all over the body, which start inside the throat, then spread to the mouth, cheeks, eyes, and forehead. As the virus tightens its grip, the rash spreads: to the soles of the feet, the palms of the hands, the crease in the buttocks, and all around the victim’s backside. Any movement pressures those lesions until they burst through nerves and skin, leaving behind a trail of thick fluid made of flaky, dead tissue and virus.
Only two known samples of natural smallpox exist: One is housed at the CDC, the other at the State Research Center of Virology and Biotechnology, in Russia. For years, security experts and scientists have debated whether to destroy those samples, because no one wants another global smallpox pandemic. That debate was made moot in 2018, when a research team at the University of Alberta, in Canada, synthesized horsepox, a previously extinct cousin of smallpox, in just six months, with DNA it had ordered online. The protocol for making horsepox would also work for smallpox.
The team published an in-depth explanation of how it synthesized the virus in PLOS One , a peer-reviewed, open-access scientific journal that anyone can read online.
The paper included the methodology the scientists used to resurrect horsepox along with best practices for those who wanted to repeat the experiment in their own lab. To the team’s credit, before publishing its research, its lead investigator followed scientific protocol and alerted the Canadian government. The team also disclosed its competing interests: One of the investigators was also the CEO and chairman of a company called Tonix Pharmaceuticals, a biotech company investigating novel approaches to neurological disorders; the company and the university had filed a U.S.-patent application for “synthetic chimeric poxviruses” a year earlier. No one—not the Canadian government, nor the journal’s editors—sent back a request for them to rescind the paper.
The poliovirus and horsepox experiments dealt with synthesizing viruses using technology designed for well-intentioned purposes. What scientists and security experts fear is different: terrorists not only synthesizing a deadly pathogen, but intentionally mutating it so that it gains strength, resilience, and speed. Scientists conduct such research in high-security containment labs, attempting to anticipate worst-case-scenario pathogens by creating and studying them. Ron Fouchier, a virologist at the Erasmus Medical Center, in Rotterdam, announced in 2011 that he’d successfully augmented the H5N1 bird-flu virus so that it could be transmitted from birds to humans, and then between people, as a new strain of deadly flu.
Before COVID-19, the H5N1 virus was the worst to hit our planet since the 1918 Spanish flu. At the time that Fouchier conducted his experiment, only 565 people were known to have been infected with H5N1, but it had a high mortality rate: 59 percent of those who’d been infected died. Fouchier had taken one of the most dangerous naturally occurring flu viruses we had ever encountered and made it even more lethal. He told fellow scientists that he’d “mutated the hell” out of H5N1 to make it airborne and therefore significantly more contagious. There was no H5N1 vaccine. The existing virus was already resistant to the antivirals approved for treatment. Fouchier’s discovery, which was funded in part by the U.S. government, scared scientists and security experts so much that, in an unprecedented move, the National Science Advisory Board for Biosecurity, within the National Institutes of Health, asked the journals Science and Nature to redact parts of his paper ahead of publication. They feared that some of the details and mutation data could enable a rogue scientist, hostile government, or group of terrorists to make their own hyper-contagious version of H5N1.
From the June 2020 issue: The prophecies of Q We’ve just lived through a global pandemic that no one wants to see replicated. We may have COVID-19 vaccines, but the path to endemicity is bumpy and will entail incalculable death and morbidity. Before we can even hope to eradicate SARS-CoV-2, as we eventually did with smallpox, there will be more mutations and many new strains. Some could affect the body in ways we’ve not yet seen or even imagined. We will continue to live with tremendous uncertainty over how and when the virus will further mutate.
Obviously, one would hope that virus research would be undertaken in a lab where fanatical adherence to safety and rigorous oversight policies were strictly enforced. Just before the WHO declared smallpox eradicated, a photographer named Janet Parker was working at a medical school in Birmingham, England. She developed a fever and body aches, and, a few days later, a red rash. At the time, she thought it was chicken pox. (That vaccine had not yet been developed.) The tiny, pimple-like dots she’d been expecting, however, developed into much bigger lesions, and they were full of a yellowish, milky fluid. As her condition worsened, doctors determined that she’d contracted smallpox, almost certainly from a sloppily managed high-security research lab inside the same building where she worked.
Parker, sadly, is now remembered as the last person known to have died from smallpox. Does the benefit of being able to accurately predict virus mutations outweigh the public risks of gain-of-function research (that is, research that involves intentionally mutating viruses to make them stronger, more transmissible, and more dangerous)? It depends on whom you ask.
Or, rather, which agency you ask. The NIH issued a series of biosafety guidelines for research on H5N1 and other flu viruses in 2013, but the guidelines were narrow and didn’t cover other kinds of viruses. The White House Office of Science and Technology Policy announced a new process to assess the risks and benefits of gain-of-function experiments in 2014. It included influenza along with the MERS and SARS viruses. But that new policy also halted existing studies intended to develop flu vaccines. So the government reversed course in 2017, when the National Science Advisory Board for Biosecurity determined that such research wouldn’t pose a risk to public safety. In 2019, the U.S. government said that it had resumed funding for—wait for it—a new round of gain-of-function experiments intended to make the H5N1 bird flu more transmissible again.
Meanwhile, this back-and-forth doesn’t stop bad actors from gaining access to open-source research papers and mail-order genetic material. When it comes to synthetic biology, security experts are particularly concerned about future dual-use issues. Traditional force protection—the security strategies to keep populations safe—won’t work against an adversary that has adapted gene products or designer molecules to use as bioweapons.
In an August 2020 paper published in the academic journal CTC Sentinel , which focuses on contemporary terrorism threats, Ken Wickiser, a biochemist and the associate dean of research at West Point, wrote: “As molecular engineering techniques of the synthetic biologists become more robust and widespread, the probability of encountering one or more of these threats is approaching certainty … The change to the threat landscape created by these techniques is rivaled only by the development of the atomic bomb.” In December 2017, the Trump administration released new guidelines clearing the way for government-funded gain-of-function projects intended not just to monitor for new potential pathogens, but to encourage the study of intentional gain-of-function mutations.
To other nations, this broadcasts a clear message: The United States is working on viral bioweapons. The last thing we need right now is a biological arms race. It’s worth noting that the companies that make vaccines haven’t publicly called for gain-of-function research or indicated that the research would assist them in ramping up supply chains for future vaccines.
Banning gain-of-function research isn’t tantamount to stopping work on synthetic viruses, vaccines, antivirals, or virus tests altogether. We are surrounded by viruses. They’re important and integral to our ecosystems. They can be harnessed for beneficial functions, which include precision antibiotics for hard-to-kill microbes, cancer treatments, and delivery vehicles for gene therapies. But we should monitor this type of work as closely as we monitor the development of nuclear technologies.
Countries typically come together during a crisis, not before one. It’s easy to agree on danger. It’s far harder to agree on a shared vision and a grand transformation. But countries could be encouraged to collaborate for public good because they have an overwhelming interest in, say, developing their bioeconomies instead of spending resources to create new tools for biowarfare.
One model is the Bretton Woods Agreement, a 1944 pact between the Allied nations of World War II that laid the foundation for a new global monetary system. Among the agreement’s provisions were plans to create two new organizations tasked with monitoring the new system and promoting economic growth: the World Bank and the International Monetary Fund. The Bretton Woods nations agreed to collaborate. If one country’s currency became too weak, the other countries would step in to help; if it was devalued beyond a certain point, the IMF would bail that country out.
They also agreed to avoid trade wars. But the IMF wouldn’t function like a global central bank. Instead it would operate as a sort of free library, from which its members could borrow when needed, while also being required to contribute to a pool of gold and currency to keep the system running. Eventually, the Bretton Woods system included 44 countries that came to consensus on regulating and promoting international trade.
The collaborative approach worked well because all members stood to gain or lose if they violated the compact. The Bretton Woods system was dissolved in the 1970s, but the IMF and the World Bank still provide a strong foundation for international currency exchange.
Instead of monitoring and regulating a global pool of money, the system I propose would govern the global pool of genetic data. Member nations would agree to use an immutable blockchain-based tracking system to record genetic sequences as well as standardized parts, orders, and products.
This kind of global system would require companies to screen synthetic gene orders against various DNA databases housing sequences of regulated pathogens and known toxins, and then authenticate buyers and record transactions in a public database.
From the September 2021 issue: How the bobos broke America The global pool of genetic data includes DNA, which reveals our most sensitive and personal secrets. Insurance companies, the police, and adversaries would be intensely interested in that information. At least 70 countries now maintain national DNA registries, some of which include data that were collected without gaining informed consent.
The current approach to national registries positions DNA as a policing tool while missing the opportunity to pool genetic data for globally scaled research projects that could benefit us all. A tiny country of just 1.3 million people demonstrates a better way forward.
From a fragile perch in Northern Europe, uncomfortably close to a hostile Russia, Estonia has built what has long been considered one of the world’s most advanced digital ecosystems. Its state-issued digital identity allows residents to safely handle online transactions with government authorities, tax and registration offices, and many other public and private services. Citizens have voted electronically since 2005, using their digital ID for authentication. That same digital ID serves as a backbone for Estonia’s health system, which connects citizens and their centrally stored personal health and medical records to doctors and health-care providers.
Estonia’s digital ecosystem also makes it easier to do data-intensive genetic research. The country’s Biobank includes genetic and health information for 20 percent of its adults, who consented to opt in to genetic-research programs. Estonia’s system offers them free genotyping and related education classes, which—bless the Estonian ethos—people actually attend. That digital-ID system also guarantees participants security and anonymity.
In a biotech Bretton Woods system, member countries could build a similar blockchain-based digital-ID system to create an unchangeable ledger of personal genomic data for research programs. Estonia’s model for informed consent is a good model for member nations of this proposed system.
Member nations would then contribute a percentage of their population’s genetic data into a global pool. Such a system would encourage responsible use and development of genetic data and encourage accountability. A standard system for genetic-sequence storage and retrieval would make audits easier and more scalable.
The stakes are unimaginably high because biology is unpredictable and tends to self-sustain, even when we don’t want it to. Already, new life-forms that never existed before in nature are in development. Some have been booted up from computer code to living cells and tissue. Evolution is evolving, and if we don’t get this next phase right, today’s harmless experimentation could result in tomorrow’s planetary-scale catastrophe.
This post is excerpted from Amy Webb’s book The Genesis Machine: Our Quest to Rewrite Life in the Age of Synthetic Biology.
​When you buy a book using a link on this page, we receive a commission. Thank you for supporting The Atlantic.
