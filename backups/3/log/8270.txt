Open Navigation Menu To revist this article, visit My Profile, then View saved stories.
Close Alert Backchannel Business Culture Gear Ideas Science Security Merch To revist this article, visit My Profile, then View saved stories.
Close Alert Search Backchannel Business Culture Gear Ideas Science Security Merch Podcasts Video Artificial Intelligence Climate Games Newsletters Magazine Events Wired Insider Jobs Coupons Will Knight Business Some Glimpse AGI in ChatGPT. Others Call It a Mirage Photograph: Eugene Mymrin/Getty Images Save this story Save Save this story Save S√©bastien Bubeck, a machine learning researcher at Microsoft , woke up one night last September thinking about artificial intelligence ‚Äîand unicorns.
Bubeck had recently gotten early access to GPT-4 , a powerful text generation algorithm from OpenAI and an upgrade to the machine learning model at the heart of the wildly popular chatbot ChatGPT.
 Bubeck was part of a team working to integrate the new AI system into Microsoft‚Äôs Bing search engine. But he and his colleagues kept marveling at how different GPT-4 seemed from anything they‚Äôd seen before.
GPT-4, like its predecessors, had been fed massive amounts of text and code and trained to use the statistical patterns in that corpus to predict the words that should be generated in reply to a piece of text input. But to Bubeck, the system‚Äôs output seemed to do so much more than just make statistically plausible guesses.
That night, Bubeck got up, went to his computer, and asked GPT-4 to draw a unicorn using TikZ , a relatively obscure programming language for generating scientific diagrams. Bubeck was using a version of GPT-4 that only worked with text, not images. But the code the model presented him with, when fed into a TikZ rendering software, produced a crude yet distinctly unicorny image cobbled together from ovals, rectangles, and a triangle. To Bubeck, such a feat surely required some abstract grasp of the elements of such a creature. ‚ÄúSomething new is happening here,‚Äù he says. ‚ÄúMaybe for the first time we have something that we could call intelligence.‚Äù How intelligent AI is becoming‚Äîand how much to trust the increasingly common feeling that a piece of software is intelligent‚Äîhas become a pressing, almost panic-inducing, question.
After OpenAI released ChatGPT , then powered by GPT-3, last November, it stunned the world with its ability to write poetry and prose on a vast array of subjects, solve coding problems, and synthesize knowledge from the web. But awe has been coupled with shock and concern about the potential for academic fraud , misinformation , and mass unemployment ‚Äîand fears that companies like Microsoft are rushing to develop technology that could prove dangerous.
Understanding the potential or risks of AI‚Äôs new abilities means having a clear grasp of what those abilities are‚Äîand are not. But while there‚Äôs broad agreement that ChatGPT and similar systems give computers significant new skills, researchers are only just beginning to study these behaviors and determine what‚Äôs going on behind the prompt.
Culture Taylor Swift and Beyonc√© Are Resurrecting the American Movie Theater Angela Watercutter Gear The Best Home Depot Black Friday Deals Matt Jancer Gear Apple‚Äôs Pledge to Support RCS Messaging Could Finally Kill SMS Boone Ashworth Business Sweden‚Äôs Tesla Blockade Is Spreading Morgan Meaker While OpenAI has promoted GPT-4 by touting its performance on bar and med school exams, scientists who study aspects of human intelligence say its remarkable capabilities differ from our own in crucial ways. The models‚Äô tendency to make things up is well known, but the divergence goes deeper. And with millions of people using the technology every day and companies betting their future on it, this is a mystery of huge importance.
Bubeck and other AI researchers at Microsoft were inspired to wade into the debate by their experiences with GPT-4. A few weeks after the system was plugged into Bing and its new chat feature was launched, the company released a paper claiming that in early experiments, GPT-4 showed ‚Äúsparks of artificial general intelligence.‚Äù The authors presented a scattering of examples in which the system performed tasks that appear to reflect more general intelligence, significantly beyond previous systems such as GPT-3. The examples show that unlike most previous AI programs, GPT-4 is not limited to a specific task but can turn its hand to all sorts of problems‚Äîa necessary quality of general intelligence.
The authors also suggest that these systems demonstrate an ability to reason, plan, learn from experience, and transfer concepts from one modality to another, such as from text to imagery. ‚ÄúGiven the breadth and depth of GPT-4‚Äôs capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system,‚Äù the paper states.
Bubeck‚Äôs paper, written with 14 others, including Microsoft‚Äôs chief scientific officer, was met with pushback from AI researchers and experts on social media. Use of the term AGI, a vague descriptor sometimes used to allude to the idea of super-intelligent or godlike machines, irked some researchers, who saw it as a symptom of the current hype.
The fact that Microsoft has invested more than $10 billion in OpenAI suggested to some researchers that the company‚Äôs AI experts had an incentive to hype GPT-4‚Äôs potential while downplaying its limitations.
 Others griped that the experiments are impossible to replicate because GPT-4 rarely responds in the same way when a prompt is repeated, and because OpenAI has not shared details of its design. Of course, people also asked why GPT-4 still makes ridiculous mistakes if it is really so smart.
Culture Taylor Swift and Beyonc√© Are Resurrecting the American Movie Theater Angela Watercutter Gear The Best Home Depot Black Friday Deals Matt Jancer Gear Apple‚Äôs Pledge to Support RCS Messaging Could Finally Kill SMS Boone Ashworth Business Sweden‚Äôs Tesla Blockade Is Spreading Morgan Meaker Talia Ringer , a professor at the University of Illinois at Urbana-Champaign, says Microsoft‚Äôs paper ‚Äúshows some interesting phenomena and then makes some really over-the-top claims.‚Äù Touting that systems are highly intelligent encourages users to trust them even when they‚Äôre deeply flawed, they say. Ringer also points out that while it may be tempting to borrow ideas from systems developed to measure human intelligence, many have proven unreliable and even rooted in racism.
Bubek admits that his study has its limits, including the reproducibility issue, and that GPT-4 also has big blind spots. He says use of the term AGI was meant to provoke debate. ‚ÄúIntelligence is by definition general,‚Äù he says. ‚ÄúWe wanted to get at the intelligence of the model and how broad it is‚Äîthat it covers many, many domains.‚Äù But for all of the examples cited in Bubeck‚Äôs paper, there are many that show GPT-4 getting things blatantly wrong‚Äîoften on the very tasks Microsoft‚Äôs team used to tout its success. For example, GPT-4‚Äôs ability to suggest a stable way to stack a challenging collection of objects‚Äî a book, four tennis balls, a nail, a wine glass, a wad of gum, and some uncooked spaghetti ‚Äîseems to point to a grasp of the physical properties of the world that is second nature to humans, including infants.
 However, changing the items and the request can result in bizarre failures that suggest GPT-4‚Äôs grasp of physics is not complete or consistent.
Bubeck notes that GPT-4 lacks a working memory and is hopeless at planning ahead. ‚ÄúGPT-4 is not good at this, and maybe large language models in general will never be good at it,‚Äù he says, referring to the large-scale machine learning algorithms at the heart of systems like GPT-4. ‚ÄúIf you want to say that intelligence is planning, then GPT-4 is not intelligent.‚Äù One thing beyond debate is that the workings of GPT-4 and other powerful AI language models do not resemble the biology of brains or the processes of the human mind. The algorithms must be fed an absurd amount of training data‚Äîa significant portion of all the text on the internet‚Äîfar more than a human needs to learn language skills. The ‚Äúexperience‚Äù that imbues GPT-4, and things built with it, with smarts is shoveled in wholesale rather than gained through interaction with the world and didactic dialog. And with no working memory, ChatGPT can maintain the thread of a conversation only by feeding itself the history of the conversation over again at each turn. Yet despite these differences, GPT-4 is clearly a leap forward, and scientists who research intelligence say its abilities need further interrogation.
A team of cognitive scientists, linguists, neuroscientists, and computer scientists from MIT, UCLA, and the University of Texas, Austin, posted a research paper in January that explores how the abilities of large language models differ from those of humans.
The group concluded that while large language models demonstrate impressive linguistic skill‚Äîincluding the ability to coherently generate a complex essay on a given theme‚Äîthat is not the same as understanding language and how to use it in the world. That disconnect may be why language models have begun to imitate the kind of commonsense reasoning needed to stack objects or solve riddles. But the systems still make strange mistakes when it comes to understanding social relationships, how the physical world works, and how people think.
Culture Taylor Swift and Beyonc√© Are Resurrecting the American Movie Theater Angela Watercutter Gear The Best Home Depot Black Friday Deals Matt Jancer Gear Apple‚Äôs Pledge to Support RCS Messaging Could Finally Kill SMS Boone Ashworth Business Sweden‚Äôs Tesla Blockade Is Spreading Morgan Meaker The way these models use language, by predicting the words most likely to come after a given string, is very difference from how humans speak or write to convey concepts or intentions. The statistical approach can cause chatbots to follow and reflect back the language of users‚Äô prompts to the point of absurdity.
When a chatbot tells someone to leave their spouse , for example, it only comes up with the answer that seems most plausible given the conversational thread. ChatGPT and similar bots will use the first person because they are trained on human writing. But they have no consistent sense of self and can change their claimed beliefs or experiences in an instant. OpenAI also uses feedback from humans to guide a model toward producing answers that people judge as more coherent and correct, which may make the model provide answers deemed more satisfying regardless of how accurate they are.
Josh Tenenbaum , a contributor to the January paper and a professor at MIT who studies human cognition and how to explore it using machines, says GPT-4 is remarkable but quite different from human intelligence in a number of ways. For instance, it lacks the kind of motivation that is crucial to the human mind. ‚ÄúIt doesn‚Äôt care if it‚Äôs turned off,‚Äù Tenenbaum says. And he says humans do not simply follow their programming but invent new goals for themselves based on their wants and needs.
Tenenbaum says some key engineering shifts happened between GPT-3 and GPT-4 and ChatGPT that made them more capable. For one, the model was trained on large amounts of computer code. He and others have argued that the human brain may use something akin to a computer program to handle some cognitive tasks, so perhaps GPT-4 learned some useful things from the patterns found in code. He also points to the feedback ChatGPT received from humans as a key factor.
But he says the resulting abilities aren‚Äôt the same as the general intelligence that characterizes human intelligence. ‚ÄúI‚Äôm interested in the cognitive capacities that led humans individually and collectively to where we are now, and that‚Äôs more than just an ability to perform a whole bunch of tasks,‚Äù he says. ‚ÄúWe make the tasks‚Äîand we make the machines that solve them.‚Äù Culture Taylor Swift and Beyonc√© Are Resurrecting the American Movie Theater Angela Watercutter Gear The Best Home Depot Black Friday Deals Matt Jancer Gear Apple‚Äôs Pledge to Support RCS Messaging Could Finally Kill SMS Boone Ashworth Business Sweden‚Äôs Tesla Blockade Is Spreading Morgan Meaker Tenenbaum also says it isn‚Äôt clear that future generations of GPT would gain these sorts of capabilities, unless some different techniques are employed. This might mean drawing from areas of AI research that go beyond machine learning. And he says it‚Äôs important to think carefully about whether we want to engineer systems that way, as doing so could have unforeseen consequences.
Another author of the January paper, Kyle Mahowald , an assistant professor of linguistics at the University of Texas at Austin, says it‚Äôs a mistake to base any judgements on single examples of GPT-4‚Äôs abilities. He says tools from cognitive psychology could be useful for gauging the intelligence of such models. But he adds that the challenge is complicated by the opacity of GPT-4. ‚ÄúIt matters what is in the training data, and we don‚Äôt know. If GPT-4 succeeds on some commonsense reasoning tasks for which it was explicitly trained and fails on others for which it wasn‚Äôt, it‚Äôs hard to draw conclusions based on that.‚Äù Whether GPT-4 can be considered a step toward AGI, then, depends entirely on your perspective. Redefining the term altogether may provide the most satisfying answer. ‚ÄúThese days my viewpoint is that this is AGI, in that it is a kind of intelligence and it is general‚Äîbut we have to be a little bit less, you know, hysterical about what AGI means,‚Äù says Noah Goodman , an associate professor of psychology, computer science, and linguistics at Stanford University.
Unfortunately, GPT-4 and ChatGPT are designed to resist such easy reframing. They are smart but offer little insight into how or why. What‚Äôs more, the way humans use language relies on having a mental model of an intelligent entity on the other side of the conversation to interpret the words and ideas being expressed. We can‚Äôt help but see flickers of intelligence in something that uses language so effortlessly. ‚ÄúIf the pattern of words is meaning-carrying, then humans are designed to interpret them as intentional, and accommodate that,‚Äù Goodman says.
The fact that AI is not like us, and yet seems so intelligent, is still something to marvel at. ‚ÄúWe‚Äôre getting this tremendous amount of raw intelligence without it necessarily coming with an ego-viewpoint, goals, or a sense of coherent self,‚Äù Goodman says. ‚ÄúThat, to me, is just fascinating.‚Äù You Might Also Like ‚Ä¶ üì© Get the long view on tech with Steven Levy's Plaintext newsletter Watch this guy work, and you‚Äôll finally understand the TikTok era How Telegram became a terrifying weapon in the Israel-Hamas War Inside Elon Musk‚Äôs first election crisis ‚Äîa day after he ‚Äúfreed‚Äù the bird The ultra-efficient farm of the future is in the sky The best pickleball paddles for beginners and pros üå≤ Our Gear team has branched out with a new guide to the best sleeping pads and fresh picks for the best coolers and binoculars Senior Writer X Topics artificial intelligence neural networks ChatGPT OpenAI Kari McMahon Steven Levy David Gilbert Jacopo Prisco Will Knight Nelson C.J.
Khari Johnson Andy Greenberg Facebook X Pinterest YouTube Instagram Tiktok More From WIRED Subscribe Newsletters Mattresses Reviews FAQ Wired Staff Coupons Black Friday Editorial Standards Archive Contact Advertise Contact Us Customer Care Jobs Press Center RSS Accessibility Help Cond√© Nast Store Do Not Sell My Personal Info ¬© 2023 Cond√© Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights.
WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Cond√© Nast.
Ad Choices Select international site United States LargeChevron UK Italia Jap√≥n Czech Republic & Slovakia
