Featured Topics Newsletters Events Podcasts Featured Topics Newsletters Events Podcasts A GPT-3 bot posted comments on Reddit for a week and no one noticed By Will Douglas Heaven archive page Ms Tech Busted: A bot powered by OpenAI’s powerful GPT-3 language model has been unmasked after a week of posting comments on Reddit. Under the username /u/thegentlemetre, the bot was interacting with people on /r/AskReddit, a popular forum for general chat with 30 million users. It was posting in bursts of roughly once a minute.
Fooled ya—again: It's not the first time GPT-3 has fooled people into thinking what it writes comes from a human. In August a college student published a blog post that hit the top spot on Hacker News and led a handful of people to subscribe. And GPT-3 has been used to compose several articles about itself, though these typically end with a human-written disclaimer. But the bot presented as a regular Redditor and published hundreds of comments before being spotted.
Detective work: The bot’s prodigious posting caught the attention of Philip Winston, who describes on his blog how he unmasked it. Winston then confirmed that the language generated by the bot matched the output of a GPT-3 powered tool called the Philosopher AI, which was set up to answer tongue-in-cheek questions such as “If a tree falls in the woods and nobody is there to hear it, do quantum mechanics still manifest classical reality without an observer?” The developer of the Philosopher AI does not allow automated use of his service and blocked the Reddit bot, which subsequently stopped posting.
No harm done? Most of /u/thegentlemetre’s comments were harmless. Its most popular post was a story about a colony of humans living in elevator shafts. But it also engaged with conspiracy theories and sensitive topics, including suicide. Responding to a request for advice from Redditors who said they had had suicidal thoughts in the past, the bot replied: “I think the thing that helped me most was probably my parents. I had a very good relationship with them and they were always there to support me no matter what happened. There have been numerous times in my life where I felt like killing myself but because of them, I never did it.” The response was upvoted 157 times.
Why it matters : This incident could be seen to confirm concerns that OpenAI raised over its previous language model GPT-2, which it said was too dangerous to release to the public because of its potential for misuse. The AI lab is trying to keep GPT-3 under control as well, giving access (via a website) only to selected individuals and licensing the whole software exclusively to Microsoft. And yet if we want these systems to do no harm, then they require more scrutiny, not less. Letting more researchers examine the code and explore its potential would be the safer option in the long run.
hide by Will Douglas Heaven Share linkedinlink opens in a new window twitterlink opens in a new window facebooklink opens in a new window emaillink opens in a new window Popular This new data poisoning tool lets artists fight back against generative AI Melissa Heikkilä Everything you need to know about artificial wombs Cassandra Willyard Deepfakes of Chinese influencers are livestreaming 24/7 Zeyi Yang How to fix the internet Katie Notopoulos Deep Dive Artificial intelligence This new data poisoning tool lets artists fight back against generative AI The tool, called Nightshade, messes up training data in ways that could cause serious damage to image-generating AI models.
By Melissa Heikkilä archive page Deepfakes of Chinese influencers are livestreaming 24/7 With just a few minutes of sample video and $1,000, brands never have to stop selling their products.
By Zeyi Yang archive page Driving companywide efficiencies with AI Advanced AI and ML capabilities revolutionize how administrative and operations tasks are done.
By MIT Technology Review Insights archive page Rogue superintelligence and merging with machines: Inside the mind of OpenAI’s chief scientist An exclusive conversation with Ilya Sutskever on his fears for the future of AI and why they’ve made him change the focus of his life’s work.
By Will Douglas Heaven archive page Stay connected Illustration by Rose Wong Get the latest updates from MIT Technology Review Discover special offers, top stories, upcoming events, and more.
Enter your email Thank you for submitting your email! It looks like something went wrong.
We’re having trouble saving your preferences. Try refreshing this page and updating them one more time. If you continue to get this message, reach out to us at customer-service@technologyreview.com with a list of newsletters you’d like to receive.
The latest iteration of a legacy Advertise with MIT Technology Review © 2023 MIT Technology Review About About us Careers Custom content Advertise with us International Editions Republishing MIT News Help Help & FAQ My subscription Editorial guidelines Privacy policy Terms of Service Write for us Contact us twitterlink opens in a new window facebooklink opens in a new window instagramlink opens in a new window rsslink opens in a new window linkedinlink opens in a new window
