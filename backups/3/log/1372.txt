Featured Topics Newsletters Events Podcasts Featured Topics Newsletters Events Podcasts At the White House, the idea of digital fakery is eroding the truth By Will Knight archive page The frightening future of digital fakery has arrived, in the form of a video of CNN reporter Jim Acosta. The footage on the right shows Acosta roughly handling a White House aide during a press conference yesterday—or does it? (The original clip is shown to the left.) Fact or fake: A fight has broken out on social media over whether the right-hand clip was, in fact, doctored by an editor at the right-wing conspiracy site Infowars to make it seem as if Acosta was being more physically aggressive than he actually was. That would be alarming because the White House press secretary, Sarah Sanders, later retweeted the clip as justification for revoking Acosta’s press credentials.
Truth or scare: As a number of keen-eyed Twitter users have pointed out, it looks like the clip was ever-so-slightly sped up at the moment contact is made.
It’s possible that this was an artifact of turning the clip into a jittery animated GIF, says Hany Farid , a world-renowned expert on digital forensics and a professor at Dartmouth. “A combination of a reduction in the quality of the video, a slowing down of the video, and the particular vantage point of the CSPAN video gives the appearance that there was more contact between the reporter and the intern than there probably was,” he adds. Farid has looked at the clip, but he has not analyzed it in detail.
AI trickery: The incident is all the more troubling given that artificial intelligence is making it ever easier to manipulate video footage. Even I was able to create a ridiculous clip of a Ted Cruz doppelganger with relative ease. The power videos hold as “ground truth” will be eroded as these digital tools become more commonplace. And this will also make it easier for those in power to discredit evidence against them as just more “fake news.” Truth out there? But the clip also shows you don’t need really clever AI to mislead people or stir up controversy. Videos that have been carefully staged and edited can be just as effective. As Farid says: “This is a good example of precisely the problem that emerges when video can be easily manipulated—anyone can claim that a video is fake, and that claim is credible. In many ways, this may be the larger threat than the actual fake footage.” hide by Will Knight Share linkedinlink opens in a new window twitterlink opens in a new window facebooklink opens in a new window emaillink opens in a new window Popular This new data poisoning tool lets artists fight back against generative AI Melissa Heikkilä Everything you need to know about artificial wombs Cassandra Willyard Deepfakes of Chinese influencers are livestreaming 24/7 Zeyi Yang How to fix the internet Katie Notopoulos Deep Dive Artificial intelligence This new data poisoning tool lets artists fight back against generative AI The tool, called Nightshade, messes up training data in ways that could cause serious damage to image-generating AI models.
By Melissa Heikkilä archive page Deepfakes of Chinese influencers are livestreaming 24/7 With just a few minutes of sample video and $1,000, brands never have to stop selling their products.
By Zeyi Yang archive page Driving companywide efficiencies with AI Advanced AI and ML capabilities revolutionize how administrative and operations tasks are done.
By MIT Technology Review Insights archive page Rogue superintelligence and merging with machines: Inside the mind of OpenAI’s chief scientist An exclusive conversation with Ilya Sutskever on his fears for the future of AI and why they’ve made him change the focus of his life’s work.
By Will Douglas Heaven archive page Stay connected Illustration by Rose Wong Get the latest updates from MIT Technology Review Discover special offers, top stories, upcoming events, and more.
Enter your email Thank you for submitting your email! It looks like something went wrong.
We’re having trouble saving your preferences. Try refreshing this page and updating them one more time. If you continue to get this message, reach out to us at customer-service@technologyreview.com with a list of newsletters you’d like to receive.
The latest iteration of a legacy Advertise with MIT Technology Review © 2023 MIT Technology Review About About us Careers Custom content Advertise with us International Editions Republishing MIT News Help Help & FAQ My subscription Editorial guidelines Privacy policy Terms of Service Write for us Contact us twitterlink opens in a new window facebooklink opens in a new window instagramlink opens in a new window rsslink opens in a new window linkedinlink opens in a new window
