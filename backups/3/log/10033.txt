Vox homepage Give Give Newsletters Newsletters Site search Search Vox main menu Explainers Crossword Video Podcasts Politics Policy Culture Science Technology Climate Health Money Life Future Perfect Newsletters More Explainers Israel-Hamas war 2024 election Supreme Court Buy less stuff Open enrollment What to watch All explainers Crossword Video Podcasts Politics Policy Culture Science Technology Climate Health Money Life Future Perfect Newsletters We have a request Vox's journalism is free, because we believe that everyone deserves to understand the world they live in. Reader support helps us do that. Can you chip in to help keep Vox free for all? × Filed under: Future Perfect Technology Artificial Intelligence AI is a “tragedy of the commons.” We’ve got solutions for that.
“Arms race” is the wrong mental model for AI. Here’s a better one.
By Sigal Samuel Jul 7, 2023, 11:20am EDT Share this story Share this on Facebook Share this on Twitter Share All sharing options Share All sharing options for: AI is a “tragedy of the commons.” We’ve got solutions for that.
Reddit Pocket Flipboard Email OpenAI CEO Sam Altman speaks at an event in Tokyo in June 2023.
Tomohiro Ohsumi/Getty Images This story is part of a group of stories called Finding the best ways to do good.
Part of You’ve probably heard AI progress described as a classic “arms race.” The basic logic is that if you don’t race forward on making advanced AI, someone else will — probably someone more reckless and less safety-conscious. So, better that you should build a superintelligent machine than let the other guy cross the finish line first! (In American discussions, the other guy is usually China.
) But as I’ve written before , this isn’t an accurate portrayal of the AI situation. There’s no one “finish line,” because AI is not just one thing with one purpose, like the atomic bomb; it’s a more general-purpose technology, like electricity. Plus, if your lab takes the time to iron out some AI safety issues, other labs may take those improvements on board, which would benefit everyone.
And as AI Impacts lead researcher Katja Grace noted in Time , “In the classic arms race, a party could always theoretically get ahead and win. But with AI, the winner may be advanced AI itself [if it’s unaligned with our goals and harms us]. This can make rushing the losing move.” I think it’s more accurate to view the AI situation as a “tragedy of the commons.” That’s what ecologists and economists call a situation where lots of actors have access to a finite valuable resource and overuse it so much that they destroy it for everyone.
A perfect example of a commons: the capacity of Earth’s atmosphere to absorb greenhouse gas emissions without tipping into climate disaster. Any individual company can argue that it’s pointless for them to use less of that capacity — someone else will just use it instead — and yet every actor acting in their rational self-interest ruins the whole planet.
AI is like that. The commons here is society’s capacity to absorb the impacts of AI without tipping into disaster. Any one company can argue that it would be pointless to limit how much or how fast they deploy increasingly advanced AI — if OpenAI doesn’t do it, it’ll just be Google or Baidu, the argument goes — but if every company acts like that, the societal result could be tragedy.
“Tragedy” sounds bad, but framing AI as a tragedy of the commons should actually make you feel optimistic, because researchers have already found solutions to this type of problem. In fact, political scientist Elinor Ostrom won a Nobel Prize in Economics in 2009 for doing exactly that. So let’s dig into her work and see how it can help us think about AI in a more solutions-focused way.
Elinor Ostrom’s solution to the tragedy of the commons In a 1968 essay in Science , the ecologist Garrett Hardin popularized the idea of the “tragedy of the commons.” He argued that humans compete so hard for resources that they ultimately destroy them; the only ways to avoid that are total government control or total privatization. “Ruin is the destination toward which all men rush,” he wrote, “each pursuing his own best interest.” Ostrom didn’t buy it. Studying communities from Switzerland to the Philippines, she found example after example of people coming together to successfully manage a shared resource, like a pasture. Ostrom discovered that communities can and do avert the tragedy of the commons, especially when they embrace eight core design principles: 1) Clearly define the community managing the resource.
2) Ensure that the rules reasonably balance between using the resource and maintaining it.
3) Involve everyone who’s affected by the rules in the process of writing the rules.
4) Establish mechanisms to monitor resource use and behavior.
5) Create an escalating series of sanctions for rule-breakers.
6) Establish a procedure for resolving any conflicts that arise.
7) Make sure the authorities recognize the community’s right to organize and set rules.
8) Encourage the formation of multiple governance structures at different scales to allow for different levels of decision-making.
Applying Ostrom’s design principles to AI So how can we use these principles to figure out what AI governance should look like? Actually, people are already pushing for some of these principles in relation to AI — they just may not realize that they slot into Ostrom’s framework.
Many have argued that AI governance should start with tracking the chips used to train frontier AI models. Writing in Asterisk magazine, Avital Balwit outlined a potential governance regime: “The basic elements involve tracking the location of advanced AI chips, and then requiring anyone using large numbers of them to prove that the models they train meet certain standards for safety and security.” Chips control corresponds to Ostrom’s principle #4: establishing mechanisms to monitor resource use and behavior.
Related The case for slowing down AI Others are noting that AI companies need to face legal liability if they release a system into the world that creates harm. As tech critics Tristan Harris and Aza Raskin have argued , liability is one of the few threats these companies actually pay attention to. This is Ostrom’s principle #5: escalating sanctions for rule-breakers.
And despite the chorus of tech execs claiming they need to rush ahead with AI lest they lose to China, you’ll also find nuanced thinkers arguing that we need international coordination, much like what we ultimately achieved with nuclear nonproliferation. That’s Ostrom’s principle #8.
If people are already applying some of Ostrom’s thinking, perhaps without realizing it, why is it important to explicitly note the connection to Ostrom? Two reasons. One is that we’re not applying all her principles yet.
The other is this: Stories matter. Myths matter. AI companies love the narrative of AI as an arms race — it justifies their rush to market. But it leaves us all in a pessimistic stance. There’s power in telling ourselves a different story: that AI is a potential tragedy of the commons, but that tragedy is only potential, and we have the power to avert it.
Will you support Vox’s explanatory journalism? Most news outlets make their money through advertising or subscriptions. But when it comes to what we’re trying to do at Vox, there are a couple reasons that we can't rely only on ads and subscriptions to keep the lights on.
First, advertising dollars go up and down with the economy. We often only know a few months out what our advertising revenue will be, which makes it hard to plan ahead.
Second, we’re not in the subscriptions business. Vox is here to help everyone understand the complex issues shaping the world — not just the people who can afford to pay for a subscription. We believe that’s an important part of building a more equal society. We can’t do that if we have a paywall.
That’s why we also turn to you, our readers, to help us keep Vox free.
If you also believe that everyone deserves access to trusted high-quality information, will you make a gift to Vox today? One-Time Monthly Annual $5 /month $10 /month $25 /month $50 /month Other $ /month /month We accept credit card, Apple Pay, and Google Pay. You can also contribute via The rise of artificial intelligence, explained How does AI actually work? 4 What is generative AI, and why is it suddenly everywhere? What happens when ChatGPT starts to feed on its own writing? The exciting new AI transforming search — and maybe everything — explained The tricky truth about how generative AI uses your data How is AI changing society? 19 What the stories we tell about robots tell us about ourselves Silicon Valley’s vision for AI? It’s religion, repackaged.
What will love and death mean in the age of machine intelligence? What if AI treats humans the way we treat animals? Can AI learn to love — and can we learn to love it? Black Mirror’s big AI episode has the wrong villain The ad industry is going all-in on AI The looming threat of AI to Hollywood, and why it should matter to you Can AI kill the greenscreen? What gets lost in the AI debate: It can be really fun How unbelievably realistic fake images could take over the internet Robot priests can bless you, advise you, and even perform your funeral AI art freaks me out. So I tried to make some.
How fake AI images can expand your mind AI art looks way too European An AI artist explains his workflow What will stop AI from flooding the internet with fake images? You’re going to see more AI-written articles whether you like it or not How “windfall profits” from AI companies could fund a universal basic income Show More Is AI coming for your job? 7 AI is flooding the workplace, and workers love it If you’re not using ChatGPT for your writing, you’re probably making a mistake Maybe AI can finally kill the cover letter Americans think AI is someone else’s problem Mark Zuckerberg’s not-so-secret plan to join the AI race The hottest new job is “head of AI” and nobody knows what they do Why Meta is giving away its extremely powerful AI model Should we be worried about AI? 10 Four different ways of understanding AI — and its risks AI experts are increasingly afraid of what they’re creating AI leaders (and Elon Musk) urge all labs to press pause on powerful AI The case for slowing down AI Are we racing toward AI catastrophe? The promise and peril of AI, according to 5 experts An unusual way to figure out if humanity is toast How AI could spark the next pandemic AI is supposedly the new nuclear weapons — but how similar are they, really? Don’t let AI fears of the future overshadow present-day causes Who will regulate AI? 8 The $1 billion gamble to ensure AI doesn’t destroy humanity Finally, a realistic roadmap for getting AI companies in check Biden sure seems serious about not letting AI get out of control Can you safely build something that may kill you? Why an Air Force colonel — and many other experts — are so worried about the existential risk of AI Scared tech workers are scrambling to reinvent themselves as AI experts Panic about overhyped AI risk could lead to the wrong kind of regulation The AI rules that US policymakers are considering, explained Most Read The controversy over TikTok and Osama bin Laden’s “Letter to America,” explained Is the green texting bubble about to burst? Formula 1 grew too fast. Now its new fans are tuning out.
The Ballad of Songbirds & Snakes might be the best Hunger Games movie yet Why are so few people getting the latest Covid-19 vaccine? vox-mark Sign up for the newsletter Sentences The day's most important news stories, explained in your inbox.
Thanks for signing up! Check your inbox for a welcome email.
Email (required) Oops. Something went wrong. Please enter a valid email and try again.
Chorus Facebook Twitter YouTube About us Our staff Privacy policy Ethics & Guidelines How we make money Contact us How to pitch Vox Contact Send Us a Tip Vox Media Terms of Use Privacy Notice Cookie Policy Do Not Sell or Share My Personal Info Licensing FAQ Accessibility Platform Status Advertise with us Jobs @ Vox Media
