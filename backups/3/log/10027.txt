Vox homepage Give Give Newsletters Newsletters Site search Search Vox main menu Explainers Crossword Video Podcasts Politics Policy Culture Science Technology Climate Health Money Life Future Perfect Newsletters More Explainers Israel-Hamas war 2024 election Supreme Court Buy less stuff Open enrollment What to watch All explainers Crossword Video Podcasts Politics Policy Culture Science Technology Climate Health Money Life Future Perfect Newsletters We have a request Vox's journalism is free, because we believe that everyone deserves to understand the world they live in. Reader support helps us do that. Can you chip in to help keep Vox free for all? × Filed under: Future Perfect Technology Artificial Intelligence Can you safely build something that may kill you? How OpenAI’s Sam Altman is keeping up the AI safety balancing act.
By Kelsey Piper May 24, 2023, 9:00am EDT Share this story Share this on Facebook Share this on Twitter Share All sharing options Share All sharing options for: Can you safely build something that may kill you? Reddit Pocket Flipboard Email Photo by Win McNamee/Getty Images This story is part of a group of stories called Finding the best ways to do good.
Part of “ AI will probably most likely lead to the end of the world, but in the meantime, there’ll be great companies,” OpenAI CEO Sam Altman once said.
 He was joking. Probably. Mostly. It’s a little hard to tell.
Altman’s company, OpenAI, is fundraising unfathomable amounts of money in order to build powerful groundbreaking AI systems. “The risks could be extraordinary,” he wrote in a February blog post.
 “A misaligned superintelligent AGI could cause grievous harm to the world; an autocratic regime with a decisive superintelligence lead could do that too.” His overall conclusion, nonetheless: OpenAI should press forward.
There’s a fundamental oddity on display whenever Altman talks about existential risks from AI, and it was particularly notable in his most recent blog post , “Governance of superintelligence”, which also lists OpenAI president Greg Brockman and chief scientist Ilya Sutskever as co-authors.
It’s kind of weird to think that what you do might kill everyone, but still do it The oddity is this: Altman isn’t wholly persuaded of the case that AI may destroy life on Earth, but he does take it very seriously. Much of his writing and thinking is in conversation with AI safety concerns. His blog posts link to respected AI safety thinkers like Holden Karnofsky, and often dive into fairly in-depth disagreements with safety researchers over questions like how the cost of hardware at the point where powerful systems are first developed will affect “takeoff speed” — the rate at which improvements to powerful AI systems drive development of more powerful AI systems.
At the very least, it is hard to accuse him of ignorance.
But many people, if they thought their work had significant potential to destroy the world, would probably stop doing it. Geoffrey Hinton left his role at Google when he became convinced that dangers from AI were real and potentially imminent. Leading figures in AI have called for a slowdown while we figure out how to evaluate systems for safety and govern their development.
Altman has said OpenAI will slow down or change course if it comes to realize that it’s driving toward catastrophe. But right now he thinks that, even though everyone might die of advanced AI, the best course is full steam ahead, because developing AI sooner makes it safer and because other, worse actors might develop it otherwise.
Altman appears to me to be walking an odd tightrope. Some of the people around him think that AI safety is fundamentally unserious and won’t be a problem. Others think that safety is the highest-stakes problem humanity has ever faced. OpenAI would like to alienate neither of them. (It would also like to make unfathomable sums of money and not destroy the world.) It’s not an easy balancing act.
“Some people in the AI field think the risks of AGI (and successor systems) are fictitious,” the February blog post says.
 “We would be delighted if they turn out to be right, but we are going to operate as if these risks are existential.” And as momentum has grown toward some kind of regulation of AI, fears have grown — especially in techno-optimist, futurist Silicon Valley — that a vague threat of doom will lead to valuable, important technologies that could vastly improve the human condition being nipped in the bud.
There are some genuine trade-offs between ensuring AI is developed safely and building it as fast as possible. Regulatory policy adequate to notice if AI systems are extremely dangerous will probably add to the costs of building powerful AI systems, and will mean we move slower as our systems get more dangerous. I don’t think there’s a way out of this trade-off entirely. But it’s also obviously possible for regulation to be wildly more inefficient than necessary, to crush lots of value with minimal effects on safety.
Trying to keep everyone happy when it comes to regulation The latest OpenAI blog post reads to me as an effort by Altman and the rest of OpenAI’s leadership to once again dance a tightrope: to call for regulation which they think will be adequate to prevent the literal end of life on Earth (and other catastrophes), and to ward off regulation that they think will be blunt, costly, and bad for the world.
That’s why the so-called governance road map for superintelligence contains paragraphs warning: “Today’s systems will create tremendous value in the world and, while they do have risks, the level of those risks feel commensurate with other Internet technologies and society’s likely approaches seem appropriate.
“By contrast, the systems we are concerned about will have power beyond any technology yet created, and we should be careful not to water down the focus on them by applying similar standards to technology far below this bar.” Cynically, this just reads “regulate us at some unspecified future point, not today!” Slightly less cynically, I think that both of the sentiments Altman is trying to convey here are deeply felt in Silicon Valley right now.
People are scared both that AI is something powerful, dangerous, and world-changing, worth approaching differently than your typical consumer software startup — and that many possible regulatory proposals would be strangling human prosperity in its cradle.
But the problem with “regulate the dangerous, powerful future AI systems, not the present-day safe ones” is that, because AI systems that were developed with our current training techniques are poorly understood, it’s not actually clear that it’ll be obvious when the “dangerous, powerful” ones show up — and there’ll always be commercial incentive to say that a system is safe when it’s not.
I’m excited about specific proposals to tie regulation to specific capabilities: to have higher standards for systems that can do large-scale independent actions, systems that are highly manipulative and persuasive, systems that can give instructions for acts of terror, and so on. But to get anywhere, the conversation does have to get specific. What makes a system powerful enough to be important to regulate? How do we know the risks of today’s systems, and how do we know when those risks get too high to tolerate? That’s what a “governance of superintelligence” plan has to answer.
Will you support Vox’s explanatory journalism? Most news outlets make their money through advertising or subscriptions. But when it comes to what we’re trying to do at Vox, there are a couple reasons that we can't rely only on ads and subscriptions to keep the lights on.
First, advertising dollars go up and down with the economy. We often only know a few months out what our advertising revenue will be, which makes it hard to plan ahead.
Second, we’re not in the subscriptions business. Vox is here to help everyone understand the complex issues shaping the world — not just the people who can afford to pay for a subscription. We believe that’s an important part of building a more equal society. We can’t do that if we have a paywall.
That’s why we also turn to you, our readers, to help us keep Vox free.
If you also believe that everyone deserves access to trusted high-quality information, will you make a gift to Vox today? One-Time Monthly Annual $5 /month $10 /month $25 /month $50 /month Other $ /month /month We accept credit card, Apple Pay, and Google Pay. You can also contribute via The rise of artificial intelligence, explained How does AI actually work? 4 What is generative AI, and why is it suddenly everywhere? What happens when ChatGPT starts to feed on its own writing? The exciting new AI transforming search — and maybe everything — explained The tricky truth about how generative AI uses your data How is AI changing society? 19 What the stories we tell about robots tell us about ourselves Silicon Valley’s vision for AI? It’s religion, repackaged.
What will love and death mean in the age of machine intelligence? What if AI treats humans the way we treat animals? Can AI learn to love — and can we learn to love it? Black Mirror’s big AI episode has the wrong villain The ad industry is going all-in on AI The looming threat of AI to Hollywood, and why it should matter to you Can AI kill the greenscreen? What gets lost in the AI debate: It can be really fun How unbelievably realistic fake images could take over the internet Robot priests can bless you, advise you, and even perform your funeral AI art freaks me out. So I tried to make some.
How fake AI images can expand your mind AI art looks way too European An AI artist explains his workflow What will stop AI from flooding the internet with fake images? You’re going to see more AI-written articles whether you like it or not How “windfall profits” from AI companies could fund a universal basic income Show More Is AI coming for your job? 7 AI is flooding the workplace, and workers love it If you’re not using ChatGPT for your writing, you’re probably making a mistake Maybe AI can finally kill the cover letter Americans think AI is someone else’s problem Mark Zuckerberg’s not-so-secret plan to join the AI race The hottest new job is “head of AI” and nobody knows what they do Why Meta is giving away its extremely powerful AI model Should we be worried about AI? 10 Four different ways of understanding AI — and its risks AI experts are increasingly afraid of what they’re creating AI leaders (and Elon Musk) urge all labs to press pause on powerful AI The case for slowing down AI Are we racing toward AI catastrophe? The promise and peril of AI, according to 5 experts An unusual way to figure out if humanity is toast How AI could spark the next pandemic AI is supposedly the new nuclear weapons — but how similar are they, really? Don’t let AI fears of the future overshadow present-day causes Who will regulate AI? 8 The $1 billion gamble to ensure AI doesn’t destroy humanity Finally, a realistic roadmap for getting AI companies in check Biden sure seems serious about not letting AI get out of control Why an Air Force colonel — and many other experts — are so worried about the existential risk of AI Scared tech workers are scrambling to reinvent themselves as AI experts Panic about overhyped AI risk could lead to the wrong kind of regulation AI is a “tragedy of the commons.” We’ve got solutions for that.
The AI rules that US policymakers are considering, explained Most Read The controversy over TikTok and Osama bin Laden’s “Letter to America,” explained Is the green texting bubble about to burst? Formula 1 grew too fast. Now its new fans are tuning out.
The Ballad of Songbirds & Snakes might be the best Hunger Games movie yet Why are so few people getting the latest Covid-19 vaccine? vox-mark Sign up for the newsletter Sentences The day's most important news stories, explained in your inbox.
Thanks for signing up! Check your inbox for a welcome email.
Email (required) Oops. Something went wrong. Please enter a valid email and try again.
Chorus Facebook Twitter YouTube About us Our staff Privacy policy Ethics & Guidelines How we make money Contact us How to pitch Vox Contact Send Us a Tip Vox Media Terms of Use Privacy Notice Cookie Policy Do Not Sell or Share My Personal Info Licensing FAQ Accessibility Platform Status Advertise with us Jobs @ Vox Media
